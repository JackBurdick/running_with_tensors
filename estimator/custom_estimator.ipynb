{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# download data\n",
    "from six.moves.urllib.request import urlopen\n",
    "\n",
    "# set env log level to supress messages, unless an error\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# set tf log level\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Helper to make the output consistent\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "def reset_graph(seed=SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "# helper to create dirs if they don't already exist\n",
    "def maybe_create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print(\"{} created\".format(dir_path))\n",
    "    else:\n",
    "        print(\"{} already exists\".format(dir_path))\n",
    "\n",
    "\n",
    "def maybe_fetch_data(url, file, dir_path):\n",
    "    maybe_create_dir(dir_path)\n",
    "    if not os.path.exists(file):\n",
    "        # download and write data\n",
    "        raw = urlopen(url).read()\n",
    "        with open(file, \"wb\") as f:\n",
    "            f.write(raw)\n",
    "        print(file, \"path written\")\n",
    "    else:\n",
    "        print(\"{} already exists. Please rm to download new data\".format(file))\n",
    "\n",
    "\n",
    "# Version information\n",
    "print(\"Python: {}\".format(sys.version_info[:]))\n",
    "assert \"1.4\" <= tf.__version__, \"TensorFlow r1.4 or later is needed\"\n",
    "print(\"TensorFlow: {}\".format(tf.__version__))\n",
    "\n",
    "# Check if using GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    print(\"No GPU found\")\n",
    "else:\n",
    "    print(\"Default GPU Device: {}\".format(tf.test.gpu_device_name()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download data paths\n",
    "ROOT_DATA = \"../ROOT_DATA/\"\n",
    "DATA_DIR = os.path.join(ROOT_DATA, \"IRIS\")\n",
    "\n",
    "IRIS_TRAINING_PATH = os.path.join(DATA_DIR, \"iris_training.csv\")\n",
    "IRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "\n",
    "IRIS_TEST_PATH = os.path.join(DATA_DIR, \"iris_test.csv\")\n",
    "IRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "maybe_fetch_data(IRIS_TRAINING_URL, IRIS_TRAINING_PATH, DATA_DIR)\n",
    "maybe_fetch_data(IRIS_TEST_URL, IRIS_TEST_PATH, DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyper parameters\n",
    "A dictionary (global, in this case) will be used to define and manage hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hyper_params(start_fresh=False):\n",
    "    global maybe_create_dir\n",
    "    data_params = {}\n",
    "    data_params[\"n_epochs\"] = 500\n",
    "    data_params[\"batch_size\"] = 32\n",
    "    data_params[\"buffer_size\"] = 128  # for shuffling\n",
    "    data_params[\"init_lr\"] = 1e-2\n",
    "\n",
    "    # dataset information, this is known information that will be\n",
    "    # used for the features. Header usually includes this info..\n",
    "    data_params[\"feature_names\"] = [\n",
    "        \"SepalLength\",\n",
    "        \"SepalWidth\",\n",
    "        \"PetalLength\",\n",
    "        \"PetalWidth\",\n",
    "    ]\n",
    "\n",
    "    data_params[\"out_dir\"] = \"./iris_custom_est\"\n",
    "\n",
    "    # by default, the project will NOT start fresh\n",
    "    if start_fresh:\n",
    "        if os.path.exists(data_params[\"out_dir\"]):\n",
    "            shutil.rmtree(data_params[\"out_dir\"], ignore_errors=True)\n",
    "\n",
    "    if not os.path.exists(data_params[\"out_dir\"]):\n",
    "        os.makedirs(data_params[\"out_dir\"])\n",
    "        print(\"{} created\".format(data_params[\"out_dir\"]))\n",
    "    else:\n",
    "        print(\"{} already exists\".format(data_params[\"out_dir\"]))\n",
    "\n",
    "    return data_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input function reading a file using the Dataset API\n",
    "# will return (features),(labels)\n",
    "def my_input_fn(file_path, predict=False):\n",
    "    global hyp_params\n",
    "    if hyp_params[\"feature_names\"]:\n",
    "        feat_types = [[0.]] * len(hyp_params[\"feature_names\"]) + [[0]]\n",
    "        # [[0.0], [0.0], [0.0], [0.0], [0]]\n",
    "        # [feat1, feat2, feat3, feat4, label]\n",
    "    else:\n",
    "        pass\n",
    "        # TODO: this could read the csv header\n",
    "\n",
    "    def decode_csv(line):\n",
    "        parsed_line = tf.decode_csv(line, feat_types)\n",
    "        label = parsed_line[-1]  # Last element is the label\n",
    "        features = parsed_line[:-1]  # all but last element\n",
    "        d = dict(zip(hyp_params[\"feature_names\"], features)), label\n",
    "        return d\n",
    "\n",
    "    dataset = (\n",
    "        tf.data.TextLineDataset(file_path)  # Read text file\n",
    "        .skip(1)  # Skip header row\n",
    "        .map(decode_csv)  # Decode each line using custom decode_csv\n",
    "    )\n",
    "    if predict:\n",
    "        dataset = dataset.shuffle(1).repeat(1)\n",
    "    else:  # (1 == no operation)\n",
    "        dataset = dataset.shuffle(hyp_params[\"buffer_size\"]).repeat(\n",
    "            hyp_params[\"n_epochs\"]\n",
    "        )  # Repeats dataset this # times\n",
    "\n",
    "    dataset = dataset.batch(hyp_params[\"batch_size\"]).prefetch(\n",
    "        1\n",
    "    )  # Make sure 1 batch is ready to serve\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model_fn(\n",
    "    features, # This is batch_features from input_fn\n",
    "    labels,   # This is batch_labels from input_fn\n",
    "    mode # instance of tf.estimator.ModeKeys\n",
    "    ):    # hyper parameters\n",
    "    \n",
    "    global hyp_params\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        tf.logging.info(\"my_model_fn: PREDICT, {}\".format(mode))\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        tf.logging.info(\"my_model_fn: EVAL, {}\".format(mode))\n",
    "    elif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        tf.logging.info(\"my_model_fn: TRAIN, {}\".format(mode))\n",
    "\n",
    "    # TODO: this needs to be customized\n",
    "    if hyp_params['feature_names']:\n",
    "        feature_columns = [\n",
    "            tf.feature_column.numeric_column(hyp_params['feature_names'][0]),\n",
    "            tf.feature_column.numeric_column(hyp_params['feature_names'][1]),\n",
    "            tf.feature_column.numeric_column(hyp_params['feature_names'][2]),\n",
    "            tf.feature_column.numeric_column(hyp_params['feature_names'][3])\n",
    "        ]\n",
    "    else:\n",
    "        pass\n",
    "        # TODO: this logic may have already been performed and\n",
    "        # could read the csv header\n",
    "\n",
    "    # TODO: core model architecture logic\n",
    "    # Create the layer of input\n",
    "    input_layer = tf.feature_column.input_layer(features, feature_columns)\n",
    "    h1 = tf.layers.dense(inputs=input_layer, units=10, activation=tf.nn.elu)\n",
    "    h2 = tf.layers.dense(inputs=h1, units=10, activation=tf.nn.elu)\n",
    "    # TODO: this could be len(features)\n",
    "    logits = tf.layers.dense(inputs=h2, units=3)\n",
    "\n",
    "    # class_ids will be the model prediction for the class (Iris flower type)\n",
    "    # The output node with the highest value is our prediction\n",
    "    predictions = { 'class_ids': tf.argmax(input=logits, axis=1), \n",
    "                    'in_data': tf.identity(input=input_layer)}\n",
    "\n",
    "    # 1. Prediction mode, Return our prediction. no need to add other ops\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    accuracy = tf.metrics.accuracy(labels, predictions['class_ids'])\n",
    "\n",
    "    # 2. Evaluation mode\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            loss=loss,\n",
    "            predictions=predictions,\n",
    "            eval_metric_ops={'my_accuracy': accuracy})\n",
    "\n",
    "    # If mode is not PREDICT nor EVAL, then we must be in TRAIN\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN, \"TRAIN is only ModeKey left\"\n",
    "\n",
    "    # 3. Training mode\n",
    "    # Provide global step counter (used to count gradient updates)\n",
    "    #optimizer = tf.train.AdagradOptimizer(0.05)\n",
    "    optimizer = tf.train.AdamOptimizer(hyp_params['init_lr'])\n",
    "    train_op = optimizer.minimize(\n",
    "        loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "\n",
    "    tf.summary.scalar('my_accuracy', accuracy[1])\n",
    "    \n",
    "    weights = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) \n",
    "               if v.name.endswith('kernel:0')]\n",
    "    bias = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) \n",
    "               if v.name.endswith('bias:0')]\n",
    "    assert len(weights) == len(bias), \"number of weights & bias are not equal\"\n",
    "    \n",
    "    for i,w in enumerate(weights):\n",
    "        name = \"weights_\"+str(i)\n",
    "        b_name = \"bias_\"+str(i)\n",
    "        with tf.variable_scope(str(i)):\n",
    "            w_hist = tf.summary.histogram(name, w)\n",
    "            b_hist = tf.summary.histogram(b_name, bias[i])\n",
    "\n",
    "    # Return training operations: loss and train_op\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode,\n",
    "        loss=loss,\n",
    "        train_op=train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_params = create_hyper_params(start_fresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_config = tf.estimator.RunConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.info(\"START estimator construction\")\n",
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=my_model_fn, model_dir=hyp_params[\"out_dir\"]\n",
    ")  # Path to where checkpoints etc are stored\n",
    "tf.logging.info(\"... END estimator construction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.logging.info(\"START classifier.train\")\n",
    "classifier.train(\n",
    "    input_fn=lambda: my_input_fn(IRIS_TRAINING_PATH),\n",
    "    hooks=None,\n",
    "    steps=None,  # NONE = train to dataset OutOfRange\n",
    "    max_steps=None,  # NONE = train to dataset OutOfRange\n",
    "    saving_listeners=None,\n",
    ")\n",
    "tf.logging.info(\"... END classifier.train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluate** model using the examples contained in `FILE_TEST`\n",
    "The return value will contain evaluation_metrics such as: loss & average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.logging.info(\"START classifier.evaluate\")\n",
    "evaluate_result = classifier.evaluate(\n",
    "    input_fn=lambda: my_input_fn(IRIS_TEST_PATH))\n",
    "tf.logging.info(\"... END classifier.evaluate\")\n",
    "tf.logging.info(\"Evaluation results\")\n",
    "for key in evaluate_result:\n",
    "    tf.logging.info(\"   {}, was: {}\".format(key, evaluate_result[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Predict** model using the examples contained in `FILE_TEST`\n",
    "The return value will contain evaluation_metrics such as: loss & average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from FILE_TEST, repeat only once (predict=True).\n",
    "predict_results = classifier.predict(\n",
    "    input_fn=lambda: my_input_fn(IRIS_TEST_PATH, predict=True),\n",
    "    predict_keys=None,\n",
    "    hooks=None,\n",
    "    checkpoint_path=None,  # If None: latest checkpoint in model_dir is used.\n",
    "    yield_single_examples=True,\n",
    ")\n",
    "tf.logging.info(\"Prediction on test file\")\n",
    "for idx, prediction in enumerate(predict_results):\n",
    "    # Will print the predicted class, i.e: 0, 1, or 2 if the prediction\n",
    "    # is Iris Setosa, Vericolor, Virginica, respectively.\n",
    "    iris_type = prediction[\"class_ids\"]  # Get the predicted class (index)\n",
    "    if iris_type == 0:\n",
    "        tf.logging.info(\"..pred: {}, is Setosa\".format(prediction[\"in_data\"]))\n",
    "    elif iris_type == 1:\n",
    "        tf.logging.info(\"..pred: {}, is Versicolor\".format(prediction[\"in_data\"]))\n",
    "    else:\n",
    "        tf.logging.info(\"..pred: {}, is Virginica\".format(prediction[\"in_data\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Predict** model using examples as python objects\n",
    "The return value will contain evaluation_metrics such as: loss & average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_input = [\n",
    "    [5.9, 3.0, 4.2, 1.5],  # -> 1, Iris Versicolor\n",
    "    [6.9, 3.1, 5.4, 2.1],  # -> 2, Iris Virginica\n",
    "    [5.1, 3.3, 1.7, 0.5],  # -> 0, Iris Setosa\n",
    "    [5.7, 2.5, 6.7, 3.3],  # -> 2, Iris Virginica\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_input_fn():\n",
    "    global hyp_params\n",
    "\n",
    "    def decode(x):\n",
    "        # TODO: this is hardcoded currently\n",
    "        x = tf.split(x, 4)  # Need to split into our 4 features\n",
    "        return dict(zip(hyp_params[\"feature_names\"], x))  # To build a dict of them\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(prediction_input)\n",
    "    dataset = dataset.map(decode)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_feature_batch = iterator.get_next()\n",
    "    return next_feature_batch, None  # In prediction, we have no labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict all our prediction_input\n",
    "predict_results = classifier.predict(\n",
    "    input_fn=new_input_fn,\n",
    "    predict_keys=None,\n",
    "    hooks=None,\n",
    "    checkpoint_path=None,  # If None: latest checkpoint in model_dir is used.\n",
    "    yield_single_examples=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.info(\"Predictions on memory\")\n",
    "for idx, prediction in enumerate(predict_results):\n",
    "    type = prediction[\"class_ids\"]  # Get the predicted class (index)\n",
    "    if type == 0:\n",
    "        tf.logging.info(\"...I think: {}, is Iris Setosa\".format(prediction_input[idx]))\n",
    "    elif type == 1:\n",
    "        tf.logging.info(\n",
    "            \"...I think: {}, is Iris Versicolor\".format(prediction_input[idx])\n",
    "        )\n",
    "    else:\n",
    "        tf.logging.info(\n",
    "            \"...I think: {}, is Iris Virginica\".format(prediction_input[idx])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we want to compare the ground truth to the prediction value for each entry?\n",
    "We have to do a bit more work and manually build/run the graph w/checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild the model\n",
    "features, labels = my_input_fn(IRIS_TEST_PATH, predict=True)\n",
    "predictions = my_model_fn(features, labels, tf.estimator.ModeKeys.EVAL).predictions\n",
    "\n",
    "# Manually load the latest checkpoint\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state(hyp_params[\"out_dir\"])\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    # Loop batches and store predictions and labels\n",
    "    prediction_values = []\n",
    "    label_values = []\n",
    "    while True:\n",
    "        try:\n",
    "            preds, lbls = sess.run([predictions, labels])\n",
    "            prediction_values.append(preds[\"class_ids\"])\n",
    "            label_values.append(lbls)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(prediction_values[0]) == len(label_values[0]), \"wrong size\"\n",
    "for i, pred in enumerate(prediction_values[0]):\n",
    "    gt = label_values[0][i]\n",
    "    print(\"pred: {}, GT: {} == {}\".format(pred, gt, pred == gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#tensorboard --logdir \"./slow\"\n",
    "# bad idea since I would need to stop it also..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be used to wipe logs / TF board\n",
    "#shutil.rmtree(hyp_params['out_dir'], ignore_errors = True) # start fresh each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_edge",
   "language": "python",
   "name": "dl_edge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
