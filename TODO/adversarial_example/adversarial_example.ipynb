{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: (3, 5, 4, 'final', 0)\n",
      "TensorFlow: 1.6.0-dev20180105\n",
      "No GPU\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this is a custom cell that contains the common imports I personally \n",
    "# use these may/may not be necessary for the following examples\n",
    "\n",
    "# DL framework\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# common packages\n",
    "import numpy as np\n",
    "import os # handling file i/o\n",
    "import sys\n",
    "import math\n",
    "import time # timing epochs\n",
    "import random\n",
    "\n",
    "# for ordered dict when building layer components\n",
    "import collections\n",
    "\n",
    "# plotting pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import colors # making colors consistent\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable # colorbar helper\n",
    "\n",
    "\n",
    "# from imageio import imread # read image from disk\n",
    "# + data augmentation\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "\n",
    "\n",
    "import pickle # manually saving best params\n",
    "from sklearn.utils import shuffle # shuffling data batches\n",
    "from tqdm import tqdm # display training progress bar\n",
    "\n",
    "# const\n",
    "SEED = 42\n",
    "\n",
    "# Helper to make the output consistent\n",
    "def reset_graph(seed=SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# helper to create dirs if they don't already exist\n",
    "def maybe_create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print(\"{} created\".format(dir_path))\n",
    "    else:\n",
    "        print(\"{} already exists\".format(dir_path))\n",
    "    \n",
    "def make_standard_dirs(saver=True, best_params=True, tf_logs=True):\n",
    "    # `saver/` will hold tf saver files\n",
    "    maybe_create_dir(\"saver\")\n",
    "    # `best_params/` will hold a serialized version of the best params\n",
    "    # I like to keep this as a backup in case I run into issues with\n",
    "    # the saver files\n",
    "    maybe_create_dir(\"best_params\")\n",
    "    # `tf_logs/` will hold the logs that will be visable in tensorboard\n",
    "    maybe_create_dir(\"tf_logs\")\n",
    "\n",
    "    \n",
    "# set tf log level to supress messages, unless an error\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Important Version information\n",
    "print(\"Python: {}\".format(sys.version_info[:]))\n",
    "print('TensorFlow: {}'.format(tf.__version__))\n",
    "\n",
    "# Check if using GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "    \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saver already exists\n",
      "best_params already exists\n",
      "tf_logs already exists\n"
     ]
    }
   ],
   "source": [
    "make_standard_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these two functions (get_model_params and restore_model_params) are \n",
    "# ad[a|o]pted from; \n",
    "# https://github.com/ageron/handson-ml/blob/master/11_deep_learning.ipynb\n",
    "def get_model_params():\n",
    "    global_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    return {global_vars.op.name: value for global_vars, value in \n",
    "            zip(global_vars, tf.get_default_session().run(global_vars))}\n",
    "\n",
    "def restore_model_params(model_params, g, sess):\n",
    "    gvar_names = list(model_params.keys())\n",
    "    assign_ops = {gvar_name: g.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                  for gvar_name in gvar_names}\n",
    "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "# these two functions are used to manually save the best\n",
    "# model params to disk\n",
    "def save_obj(obj, name):\n",
    "    with open('best_params/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('best_params/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t10k-images-idx3-ubyte.gz\n",
      "t10k-labels-idx1-ubyte.gz\n",
      "train-images-idx3-ubyte.gz\n",
      "train-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "ROOT_DATA = \"../../ROOT_DATA/\"\n",
    "DATA_DIR = \"mnist_data\"\n",
    "\n",
    "MNIST_TRAINING_PATH = os.path.join(ROOT_DATA, DATA_DIR)\n",
    "# ensure we have the correct directory\n",
    "for _, _, files in os.walk(MNIST_TRAINING_PATH):\n",
    "    files = sorted(files)\n",
    "    for filename in files:\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../ROOT_DATA/mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../ROOT_DATA/mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../ROOT_DATA/mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../ROOT_DATA/mnist_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "MNIST = input_data.read_data_sets(MNIST_TRAINING_PATH, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_hyper_params():\n",
    "    data_params = {}\n",
    "    data_params['n_epochs'] = 3\n",
    "    data_params['batch_size'] = 128\n",
    "    data_params['buffer_size'] = 128 # for shuffling\n",
    "\n",
    "    data_params['init_lr'] = 1e-2\n",
    "\n",
    "    \n",
    "    return data_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(data_params):\n",
    "    g = tf.Graph()\n",
    "    n_outputs = 10\n",
    "    IMG_HEIGHT = 28\n",
    "    IMG_WIDTH = 28\n",
    "    CHANNELS = 1\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"inputs\"):\n",
    "            X = tf.placeholder(tf.float32, shape=(None, 784), name=\"data\") # Input\n",
    "#             Xx = tf.Variable(tf.zeros((28, 28, 1)))\n",
    "#             Xxx = tf.expand_dims(Xx, 0)\n",
    "            X_reshaped = tf.reshape(X, shape=[-1, IMG_HEIGHT, IMG_WIDTH, CHANNELS])\n",
    "            y = tf.placeholder(tf.int32, shape=(None, n_outputs), name=\"labels\") # Target\n",
    "\n",
    "        with tf.name_scope(\"cnn\"):\n",
    "            h_1 = tf.layers.conv2d(X_reshaped, filters=32, kernel_size=3, activation=tf.nn.relu,\n",
    "                                   padding='SAME', strides=1, name=\"conv_1\")\n",
    "            h_2 = tf.layers.conv2d(h_1, filters=64, kernel_size=3, activation=tf.nn.relu,\n",
    "                                   padding='SAME', strides=1, name=\"conv_2\")\n",
    "            h_3 = tf.layers.conv2d(h_1, filters=36, kernel_size=3, activation=tf.nn.elu,\n",
    "                                   padding='SAME', strides=2, name=\"conv_3\")\n",
    "            #h_4 = tf.nn.max_pool(h_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\", name=\"max_pool_1\")\n",
    "            h_4 = tf.layers.max_pooling2d(h_3, pool_size=[2,2],\n",
    "                                          strides=2, name=\"max_pool_01\")\n",
    "            last_shape = int(np.prod(h_4.get_shape()[1:]))\n",
    "            h_4_flat = tf.reshape(h_4, shape=[-1, last_shape])\n",
    "            h_5 = tf.layers.dense(h_4_flat, 64, name=\"layer_05\", activation=tf.nn.relu)\n",
    "            logits = tf.layers.dense(h_5, n_outputs, name=\"logits\")\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "            batch_loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "        \n",
    "        with tf.name_scope(\"train\"):\n",
    "            optimizer = tf.train.GradientDescentOptimizer(data_params['init_lr'])\n",
    "            training_op = optimizer.minimize(batch_loss)\n",
    "            \n",
    "        with tf.name_scope(\"save_session\"):\n",
    "            init_global = tf.global_variables_initializer()\n",
    "            init_local = tf.local_variables_initializer()\n",
    "            saver = tf.train.Saver()\n",
    "        \n",
    "#         with tf.name_scope(\"adv\"):\n",
    "#             x = tf.placeholder(tf.float32, (28, 28, 1), name=\"jack\") # Input\n",
    "#             x_hat = Xx\n",
    "#             assign_op = tf.assign(x_hat, x)\n",
    "\n",
    "#             y_hat = tf.placeholder(tf.int32, ())\n",
    "#             labels = tf.one_hot(y_hat, 10)\n",
    "#             loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels, name=\"adv_loss\")\n",
    "#             optim_step = tf.train.GradientDescentOptimizer(1e-1).minimize(loss, var_list=[Xx])\n",
    "            \n",
    "#             epsilon = tf.placeholder(tf.float32, ())\n",
    "#             below = x - epsilon\n",
    "#             above = x + epsilon\n",
    "#             projected = tf.clip_by_value(tf.clip_by_value(x_hat, below, above), 0, 1)\n",
    "\n",
    "#             with tf.control_dependencies([projected]):\n",
    "#                 project_step = tf.assign(x_hat, projected)\n",
    "                \n",
    "#             for node in (assign_op, x, optim_step, loss, y_hat, x, epsilon, x_hat, project_step):\n",
    "#                 g.add_to_collection(\"adv\", node)\n",
    "\n",
    "            \n",
    "        # Ops: training metrics\n",
    "        with tf.name_scope(\"metrics\"):\n",
    "            # ================================== performance\n",
    "            with tf.name_scope(\"common\"):\n",
    "                preds = tf.nn.softmax(logits, name=\"prediction\")\n",
    "                y_true_cls = tf.argmax(y,1)\n",
    "                y_pred_cls = tf.argmax(preds,1)\n",
    "                correct_prediction = tf.equal(y_pred_cls, y_true_cls, name=\"correct_predictions\")\n",
    "                batch_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            with tf.name_scope(\"train_metrics\") as scope:\n",
    "                train_auc, train_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                train_acc, train_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                train_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                train_met_reset_op = tf.variables_initializer(train_acc_vars, name=\"train_met_reset_op\")\n",
    "            with tf.name_scope(\"val_metrics\") as scope:\n",
    "                val_auc, val_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                val_acc, val_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                val_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                val_met_reset_op = tf.variables_initializer(val_acc_vars, name=\"val_met_reset_op\")\n",
    "            with tf.name_scope(\"test_metrics\") as scope:\n",
    "                test_auc, test_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                test_acc, test_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                test_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                test_acc_reset_op = tf.variables_initializer(test_acc_vars, name=\"test_met_reset_op\")\n",
    "\n",
    "            # =============================================== loss \n",
    "            with tf.name_scope(\"train_loss_eval\") as scope:\n",
    "                train_mean_loss, train_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                train_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                train_loss_reset_op = tf.variables_initializer(train_loss_vars, name=\"train_loss_reset_op\")\n",
    "            with tf.name_scope(\"val_loss_eval\") as scope:\n",
    "                val_mean_loss, val_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                val_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                val_loss_reset_op = tf.variables_initializer(val_loss_vars, name=\"val_loss_reset_op\")\n",
    "            with tf.name_scope(\"test_loss_eval\")as scope:\n",
    "                test_mean_loss, test_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                test_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                test_loss_reset_op = tf.variables_initializer(test_loss_vars, name=\"test_loss_rest_op\")\n",
    "\n",
    "        # --- create collections\n",
    "        for node in (saver, init_global, init_local):\n",
    "            g.add_to_collection(\"save_init\", node)\n",
    "        for node in (X, X_reshaped, y, training_op):\n",
    "            g.add_to_collection(\"main_ops\", node)\n",
    "        for node in (preds, y_true_cls, y_pred_cls):\n",
    "            g.add_to_collection(\"preds\", node)\n",
    "        for node in (train_auc, train_auc_update, train_acc, train_acc_update, train_met_reset_op):\n",
    "            g.add_to_collection(\"train_metrics\", node)\n",
    "        for node in (val_auc, val_auc_update, val_acc, val_acc_update, val_met_reset_op):\n",
    "            g.add_to_collection(\"val_metrics\", node)\n",
    "        for node in (test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op):\n",
    "            g.add_to_collection(\"test_metrics\", node)\n",
    "        for node in (train_mean_loss, train_mean_loss_update, train_loss_reset_op):\n",
    "            g.add_to_collection(\"train_loss\", node)\n",
    "        for node in (val_mean_loss, val_mean_loss_update, val_loss_reset_op):\n",
    "            g.add_to_collection(\"val_loss\", node)\n",
    "        for node in (test_mean_loss, test_mean_loss_update, test_loss_reset_op):\n",
    "            g.add_to_collection(\"test_loss\", node)\n",
    "        g.add_to_collection(\"logits\", logits)\n",
    "            \n",
    "        # ===================================== tensorboard\n",
    "        with tf.name_scope(\"tensorboard_writer\") as scope:\n",
    "            epoch_train_loss_scalar = tf.summary.scalar('train_epoch_loss', train_mean_loss)\n",
    "            epoch_train_acc_scalar = tf.summary.scalar('train_epoch_acc', train_acc)\n",
    "            epoch_train_auc_scalar = tf.summary.scalar('train_epoch_auc', train_auc)\n",
    "            epoch_train_write_op = tf.summary.merge([epoch_train_loss_scalar, epoch_train_acc_scalar, epoch_train_auc_scalar], name=\"epoch_train_write_op\")\n",
    "\n",
    "            # ===== epoch, validation\n",
    "            epoch_validation_loss_scalar = tf.summary.scalar('validation_epoch_loss', val_mean_loss)\n",
    "            epoch_validation_acc_scalar = tf.summary.scalar('validation_epoch_acc', val_acc)\n",
    "            epoch_validation_auc_scalar = tf.summary.scalar('validation_epoch_auc', val_auc)\n",
    "            epoch_validation_write_op = tf.summary.merge([epoch_validation_loss_scalar, epoch_validation_acc_scalar, epoch_validation_auc_scalar], name=\"epoch_validation_write_op\")\n",
    "        \n",
    "        for node in (epoch_train_write_op, epoch_validation_write_op):\n",
    "            g.add_to_collection(\"tensorboard\", node)\n",
    "            \n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_graph(g):\n",
    "    saver, init_global, init_local = g.get_collection(\"save_init\")\n",
    "    X, X_reshaped, y, training_op = g.get_collection(\"main_ops\")\n",
    "    preds, y_true_cls, y_pred_cls = g.get_collection(\"preds\")\n",
    "    train_auc, train_auc_update, train_acc, train_acc_update, train_met_reset_op = g.get_collection(\"train_metrics\")\n",
    "    val_auc, val_auc_update, val_acc, val_acc_update, val_met_reset_op = g.get_collection(\"val_metrics\")\n",
    "    train_mean_loss, train_mean_loss_update, train_loss_reset_op = g.get_collection(\"train_loss\")\n",
    "    val_mean_loss, val_mean_loss_update, val_loss_reset_op = g.get_collection(\"val_loss\")\n",
    "    epoch_train_write_op, epoch_validation_write_op = g.get_collection(\"tensorboard\")\n",
    "\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(\"tf_logs\",\"train\"))\n",
    "    val_writer = tf.summary.FileWriter(os.path.join(\"tf_logs\",\"validation\"))\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    with tf.Session(graph=g) as sess:\n",
    "        sess.run([init_global, init_local])\n",
    "        \n",
    "        for e in tqdm(range(1,data_params['n_epochs']+1)):\n",
    "            sess.run([val_met_reset_op,val_loss_reset_op,train_met_reset_op,train_loss_reset_op])\n",
    "            \n",
    "            n_batches = int(MNIST.train.num_examples/data_params['batch_size'])\n",
    "            for i in range(1, n_batches+1):\n",
    "                data, target = MNIST.train.next_batch(data_params['batch_size'])\n",
    "                sess.run([training_op, train_auc_update, train_acc_update, train_mean_loss_update], feed_dict={X:data, y:target})\n",
    "        \n",
    "            # write average for epoch\n",
    "            summary = sess.run(epoch_train_write_op)    \n",
    "            train_writer.add_summary(summary, e)\n",
    "            train_writer.flush()\n",
    "\n",
    "            # run validation\n",
    "            n_batches = int(MNIST.validation.num_examples/data_params['batch_size'])\n",
    "            for i in range(1,n_batches+1):\n",
    "                Xb, yb = MNIST.validation.next_batch(data_params['batch_size'])\n",
    "                sess.run([val_auc_update, val_acc_update, val_mean_loss_update], feed_dict={X:data, y:target})\n",
    "\n",
    "            # check for (and save) best validation params here\n",
    "            cur_loss, cur_acc = sess.run([val_mean_loss, val_acc])\n",
    "            if cur_loss < best_val_loss:\n",
    "                best_val_loss = cur_loss\n",
    "                best_params = get_model_params()\n",
    "                save_obj(best_params, \"new_best_params\")\n",
    "                print(\"best params saved: acc: {:.3f}% loss: {:.4f}\".format(cur_acc*100, cur_loss))\n",
    "\n",
    "\n",
    "            summary = sess.run(epoch_validation_write_op) \n",
    "            val_writer.add_summary(summary, e)\n",
    "            val_writer.flush()\n",
    "        \n",
    "        train_writer.close()\n",
    "        val_writer.close()\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:56<01:52, 56.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 89.844% loss: 0.4224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 2/3 [02:10<01:05, 65.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 95.312% loss: 0.2076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:21<00:00, 67.04s/it]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "data_params = create_hyper_params()\n",
    "g = build_graph(data_params)\n",
    "sess = train_graph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "This is a checkpoint - in that training can be skipped if previous best params are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:04<00:00, 17.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 99.345% acc: 91.556% loss: 0.27961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "data_params = create_hyper_params()\n",
    "g2 = build_graph(data_params)\n",
    "best_params = load_obj(\"new_best_params\")\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    saver, init_global, init_local = g2.get_collection(\"save_init\")\n",
    "    X, X_reshaped, y, training_op = g2.get_collection(\"main_ops\")\n",
    "    preds, y_true_cls, y_pred_cls = g2.get_collection(\"preds\")\n",
    "    test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op = g2.get_collection(\"test_metrics\")\n",
    "    test_mean_loss, test_mean_loss_update, test_loss_reset_op = g2.get_collection(\"test_loss\")\n",
    "    \n",
    "    restore_model_params(model_params=best_params, g=g2, sess=sess)\n",
    "    sess.run([test_acc_reset_op, test_loss_reset_op])\n",
    "    \n",
    "    n_batches = int(MNIST.test.num_examples/data_params['batch_size'])\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        Xb, yb = MNIST.test.next_batch(data_params['batch_size'])\n",
    "        batch_accuracy, batch_loss, batch_auc = sess.run([test_acc_update, test_mean_loss_update, test_auc_update], \n",
    "                                                                  feed_dict={X:Xb,y:yb})\n",
    "    # print\n",
    "    final_test_acc, final_test_loss, final_test_auc = sess.run([test_acc, test_mean_loss, test_auc])\n",
    "    print(\"test auc: {:.3f}% acc: {:.3f}% loss: {:.5f}\".format(final_test_auc*100, \n",
    "                                                              final_test_acc*100,\n",
    "                                                              final_test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Sample Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f46c35ddcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "some_idx = 42\n",
    "some_image = MNIST.test.images[some_idx]\n",
    "some_label_enc = MNIST.test.labels[some_idx]\n",
    "some_label_dec = np.argmax(some_label_enc)\n",
    "some_digit_image = some_image.reshape(28, 28)\n",
    "plt.imshow(some_digit_image, cmap = matplotlib.cm.binary,\n",
    "           interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(some_label_dec)\n",
    "diesel = some_digit_image.reshape(28,28,1)\n",
    "print(diesel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the current architecture classify the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 100.000% acc: 100.000% loss: 0.12512\n",
      "pred: [  3.54572870e-02   7.68108332e-09   3.25736110e-05   8.82392764e-01\n",
      "   5.04017009e-07   8.16918463e-02   2.54823999e-06   1.49284038e-04\n",
      "   1.64060926e-04   1.09149194e-04]\n",
      "true_class: [3]\n",
      "pred_class [3]\n",
      "confidence: 88.2393%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f46c09eda90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reset_graph()\n",
    "data_params = create_hyper_params()\n",
    "g2 = build_graph(data_params)\n",
    "best_params = load_obj(\"new_best_params\")\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    saver, init_global, init_local = g2.get_collection(\"save_init\")\n",
    "    X, X_reshaped, y, training_op = g2.get_collection(\"main_ops\")\n",
    "    preds, y_true_cls, y_pred_cls = g2.get_collection(\"preds\")\n",
    "    test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op = g2.get_collection(\"test_metrics\")\n",
    "    test_mean_loss, test_mean_loss_update, test_loss_reset_op = g2.get_collection(\"test_loss\")\n",
    "    logz = g2.get_collection(\"logits\")[0]\n",
    "\n",
    "    sess.run([init_global, init_local])\n",
    "\n",
    "    restore_model_params(model_params=best_params, g=g2, sess=sess)\n",
    "    sess.run([test_acc_reset_op, test_loss_reset_op])\n",
    "    Xb, yb = np.expand_dims(some_image,0), np.expand_dims(some_label_enc, 0)\n",
    "    batch_accuracy, batch_loss, batch_auc = sess.run([test_acc_update, test_mean_loss_update, test_auc_update], \n",
    "                                                              feed_dict={X:Xb,y:yb})\n",
    "    pred_value, true_cls_value, pred_cls_value = sess.run([preds, y_true_cls, y_pred_cls],\n",
    "                                                          feed_dict={X:Xb,y:yb})\n",
    "    logits_val = sess.run([logz], feed_dict={X:Xb,y:yb})[0]\n",
    "    print\n",
    "    final_test_acc, final_test_loss, final_test_auc = sess.run([test_acc, test_mean_loss, test_auc])\n",
    "    print(\"test auc: {:.3f}% acc: {:.3f}% loss: {:.5f}\".format(final_test_auc*100, \n",
    "                                                               final_test_acc*100,\n",
    "                                                               final_test_loss))\n",
    "    pred_idx = pred_cls_value[0]\n",
    "    print(\"pred: {}\\ntrue_class: {}\\npred_class {}\".format(pred_value[0], true_cls_value, pred_cls_value))\n",
    "    print(\"confidence: {:.4f}%\".format(pred_value[0][pred_idx]*100))\n",
    "    plt.imshow(some_digit_image, cmap = matplotlib.cm.binary,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_adv_graph(data_params):\n",
    "    g = tf.Graph()\n",
    "    n_outputs = 10\n",
    "    IMG_HEIGHT = 28\n",
    "    IMG_WIDTH = 28\n",
    "    CHANNELS = 1\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"inputs\"):\n",
    "            #X = tf.placeholder(tf.float32, shape=(None, 784), name=\"data\") # Input\n",
    "            Xx = tf.Variable(tf.zeros((28, 28, 1)))\n",
    "            Xxx = tf.expand_dims(Xx, 0)\n",
    "            #X_reshaped = tf.reshape(X, shape=[-1, IMG_HEIGHT, IMG_WIDTH, CHANNELS])\n",
    "            y = tf.placeholder(tf.int32, shape=(None, n_outputs), name=\"labels\") # Target\n",
    "\n",
    "        with tf.name_scope(\"cnn\"):\n",
    "            h_1 = tf.layers.conv2d(Xxx, filters=32, kernel_size=3, activation=tf.nn.relu,\n",
    "                                   padding='SAME', strides=1, name=\"conv_1\")\n",
    "            h_2 = tf.layers.conv2d(h_1, filters=64, kernel_size=3, activation=tf.nn.relu,\n",
    "                                   padding='SAME', strides=1, name=\"conv_2\")\n",
    "            h_3 = tf.layers.conv2d(h_1, filters=36, kernel_size=3, activation=tf.nn.elu,\n",
    "                                   padding='SAME', strides=2, name=\"conv_3\")\n",
    "            #h_4 = tf.nn.max_pool(h_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\", name=\"max_pool_1\")\n",
    "            h_4 = tf.layers.max_pooling2d(h_3, pool_size=[2,2],\n",
    "                                          strides=2, name=\"max_pool_01\")\n",
    "            last_shape = int(np.prod(h_4.get_shape()[1:]))\n",
    "            h_4_flat = tf.reshape(h_4, shape=[-1, last_shape])\n",
    "            h_5 = tf.layers.dense(h_4_flat, 64, name=\"layer_05\", activation=tf.nn.relu)\n",
    "            logits = tf.layers.dense(h_5, n_outputs, name=\"logits\")\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "            batch_loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "        \n",
    "        with tf.name_scope(\"train\"):\n",
    "            optimizer = tf.train.GradientDescentOptimizer(data_params['init_lr'])\n",
    "            training_op = optimizer.minimize(batch_loss)\n",
    "            \n",
    "        with tf.name_scope(\"save_session\"):\n",
    "            init_global = tf.global_variables_initializer()\n",
    "            init_local = tf.local_variables_initializer()\n",
    "            saver = tf.train.Saver()\n",
    "        \n",
    "        with tf.name_scope(\"adv\"):\n",
    "            x = tf.placeholder(tf.float32, (28, 28, 1), name=\"jack\") # Input\n",
    "            x_hat = Xx\n",
    "            assign_op = tf.assign(x_hat, x)\n",
    "\n",
    "            y_hat = tf.placeholder(tf.int32, ())\n",
    "            labels = tf.one_hot(y_hat, 10)\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels, name=\"adv_loss\")\n",
    "            optim_step = tf.train.GradientDescentOptimizer(1e-1).minimize(loss, var_list=[Xx])\n",
    "            \n",
    "            epsilon = tf.placeholder(tf.float32, ())\n",
    "            below = x - epsilon\n",
    "            above = x + epsilon\n",
    "            projected = tf.clip_by_value(tf.clip_by_value(x_hat, below, above), 0, 1)\n",
    "\n",
    "            with tf.control_dependencies([projected]):\n",
    "                project_step = tf.assign(x_hat, projected)\n",
    "                \n",
    "            for node in (assign_op, x, optim_step, loss, y_hat, epsilon, x_hat, project_step):\n",
    "                g.add_to_collection(\"adv\", node)\n",
    "\n",
    "            \n",
    "        # Ops: training metrics\n",
    "        with tf.name_scope(\"metrics\"):\n",
    "            # ================================== performance\n",
    "            with tf.name_scope(\"common\"):\n",
    "                preds = tf.nn.softmax(logits, name=\"prediction\")\n",
    "                y_true_cls = tf.argmax(y,1)\n",
    "                y_pred_cls = tf.argmax(preds,1)\n",
    "                correct_prediction = tf.equal(y_pred_cls, y_true_cls, name=\"correct_predictions\")\n",
    "                batch_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            with tf.name_scope(\"train_metrics\") as scope:\n",
    "                train_auc, train_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                train_acc, train_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                train_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                train_met_reset_op = tf.variables_initializer(train_acc_vars, name=\"train_met_reset_op\")\n",
    "            with tf.name_scope(\"val_metrics\") as scope:\n",
    "                val_auc, val_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                val_acc, val_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                val_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                val_met_reset_op = tf.variables_initializer(val_acc_vars, name=\"val_met_reset_op\")\n",
    "            with tf.name_scope(\"test_metrics\") as scope:\n",
    "                test_auc, test_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                test_acc, test_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                test_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                test_acc_reset_op = tf.variables_initializer(test_acc_vars, name=\"test_met_reset_op\")\n",
    "\n",
    "            # =============================================== loss \n",
    "            with tf.name_scope(\"train_loss_eval\") as scope:\n",
    "                train_mean_loss, train_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                train_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                train_loss_reset_op = tf.variables_initializer(train_loss_vars, name=\"train_loss_reset_op\")\n",
    "            with tf.name_scope(\"val_loss_eval\") as scope:\n",
    "                val_mean_loss, val_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                val_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                val_loss_reset_op = tf.variables_initializer(val_loss_vars, name=\"val_loss_reset_op\")\n",
    "            with tf.name_scope(\"test_loss_eval\")as scope:\n",
    "                test_mean_loss, test_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                test_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                test_loss_reset_op = tf.variables_initializer(test_loss_vars, name=\"test_loss_rest_op\")\n",
    "\n",
    "        # --- create collections\n",
    "        for node in (saver, init_global, init_local):\n",
    "            g.add_to_collection(\"save_init\", node)\n",
    "        for node in (X, Xx, y, training_op):\n",
    "            g.add_to_collection(\"main_ops\", node)\n",
    "        for node in (preds, y_true_cls, y_pred_cls):\n",
    "            g.add_to_collection(\"preds\", node)\n",
    "        for node in (train_auc, train_auc_update, train_acc, train_acc_update, train_met_reset_op):\n",
    "            g.add_to_collection(\"train_metrics\", node)\n",
    "        for node in (val_auc, val_auc_update, val_acc, val_acc_update, val_met_reset_op):\n",
    "            g.add_to_collection(\"val_metrics\", node)\n",
    "        for node in (test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op):\n",
    "            g.add_to_collection(\"test_metrics\", node)\n",
    "        for node in (train_mean_loss, train_mean_loss_update, train_loss_reset_op):\n",
    "            g.add_to_collection(\"train_loss\", node)\n",
    "        for node in (val_mean_loss, val_mean_loss_update, val_loss_reset_op):\n",
    "            g.add_to_collection(\"val_loss\", node)\n",
    "        for node in (test_mean_loss, test_mean_loss_update, test_loss_reset_op):\n",
    "            g.add_to_collection(\"test_loss\", node)\n",
    "        g.add_to_collection(\"logits\", logits)\n",
    "            \n",
    "        # ===================================== tensorboard\n",
    "        with tf.name_scope(\"tensorboard_writer\") as scope:\n",
    "            epoch_train_loss_scalar = tf.summary.scalar('train_epoch_loss', train_mean_loss)\n",
    "            epoch_train_acc_scalar = tf.summary.scalar('train_epoch_acc', train_acc)\n",
    "            epoch_train_auc_scalar = tf.summary.scalar('train_epoch_auc', train_auc)\n",
    "            epoch_train_write_op = tf.summary.merge([epoch_train_loss_scalar, epoch_train_acc_scalar, epoch_train_auc_scalar], name=\"epoch_train_write_op\")\n",
    "\n",
    "            # ===== epoch, validation\n",
    "            epoch_validation_loss_scalar = tf.summary.scalar('validation_epoch_loss', val_mean_loss)\n",
    "            epoch_validation_acc_scalar = tf.summary.scalar('validation_epoch_acc', val_acc)\n",
    "            epoch_validation_auc_scalar = tf.summary.scalar('validation_epoch_auc', val_auc)\n",
    "            epoch_validation_write_op = tf.summary.merge([epoch_validation_loss_scalar, epoch_validation_acc_scalar, epoch_validation_auc_scalar], name=\"epoch_validation_write_op\")\n",
    "        \n",
    "        for node in (epoch_train_write_op, epoch_validation_write_op):\n",
    "            g.add_to_collection(\"tensorboard\", node)\n",
    "            \n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10, loss=0.478872\n",
      "step 20, loss=0.405101\n",
      "step 30, loss=0.383487\n",
      "step 40, loss=0.372441\n",
      "step 50, loss=0.365566\n",
      "step 60, loss=0.361177\n",
      "step 70, loss=0.35838\n",
      "step 80, loss=0.356516\n",
      "step 90, loss=0.354725\n",
      "step 100, loss=0.353641\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "data_params = create_hyper_params()\n",
    "g3 = build_adv_graph(data_params)\n",
    "best_params = load_obj(\"new_best_params\")\n",
    "with tf.Session(graph=g3) as sess:\n",
    "    saver, init_global, init_local = g3.get_collection(\"save_init\")\n",
    "    X, Xx, y, training_op = g3.get_collection(\"main_ops\")\n",
    "    preds, y_true_cls, y_pred_cls = g3.get_collection(\"preds\")\n",
    "    test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op = g3.get_collection(\"test_metrics\")\n",
    "    test_mean_loss, test_mean_loss_update, test_loss_reset_op = g3.get_collection(\"test_loss\")\n",
    "    logz = g3.get_collection(\"logits\")[0]\n",
    "\n",
    "    sess.run([init_global, init_local])\n",
    "\n",
    "    restore_model_params(model_params=best_params, g=g3, sess=sess)\n",
    "    sess.run([test_acc_reset_op, test_loss_reset_op])\n",
    "    \n",
    "    # execution\n",
    "    demo_eps = 0.07\n",
    "    demo_lr = 1e-1\n",
    "    demo_steps = 100\n",
    "    demo_target = 5\n",
    "    \n",
    "    assign_op, x, optim_step, loss, y_hat, epsilon, x_hat, project_step = g3.get_collection(\"adv\")\n",
    "    \n",
    "    # initialization step\n",
    "    sess.run(assign_op, feed_dict={x: diesel})\n",
    "\n",
    "    # projected gradient descent\n",
    "    for i in range(demo_steps):\n",
    "        # gradient descent step\n",
    "        _, loss_value = sess.run(\n",
    "            [optim_step, loss],\n",
    "            feed_dict={y_hat: demo_target})\n",
    "        # project step\n",
    "        sess.run(project_step, feed_dict={x: diesel, epsilon: demo_eps})\n",
    "        if (i+1) % 10 == 0:\n",
    "            print('step %d, loss=%g' % (i+1, loss_value))\n",
    "\n",
    "\n",
    "    adv = x_hat.eval() # retrieve the adversarial example\n",
    "    adv_flat = adv.reshape((784))\n",
    "\n",
    "#     Xb, yb = np.expand_dims(some_image,0), np.expand_dims(some_label_enc, 0)\n",
    "#     batch_accuracy, batch_loss, batch_auc = sess.run([test_acc_update, test_mean_loss_update, test_auc_update], \n",
    "#                                                               feed_dict={X:Xb,y:yb})\n",
    "#     pred_value, true_cls_value, pred_cls_value = sess.run([preds, y_true_cls, y_pred_cls],\n",
    "#                                                           feed_dict={X:Xb,y:yb})\n",
    "#     logits_val = sess.run([logz], feed_dict={X:Xb,y:yb})[0]\n",
    "#     print\n",
    "#     final_test_acc, final_test_loss, final_test_auc = sess.run([test_acc, test_mean_loss, test_auc])\n",
    "#     print(\"test auc: {:.3f}% acc: {:.3f}% loss: {:.5f}\".format(final_test_auc*100, \n",
    "#                                                                final_test_acc*100,\n",
    "#                                                                final_test_loss))\n",
    "#     pred_idx = pred_cls_value[0]\n",
    "#     print(\"pred: {}\\ntrue_class: {}\\npred_class {}\".format(pred_value[0], true_cls_value, pred_cls_value))\n",
    "#     print(\"confidence: {:.4f}%\".format(pred_value[0][pred_idx]*100))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 88.889% acc: 0.000% loss: 1.55436\n",
      "pred: [  8.56879652e-02   1.17312995e-08   3.47209098e-05   2.11324826e-01\n",
      "   1.93065193e-06   7.02193618e-01   9.73399256e-06   1.80631265e-04\n",
      "   4.35130205e-04   1.31526758e-04]\n",
      "true_class: [3]\n",
      "pred_class [5]\n",
      "confidence: 70.2194%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f46cd6083c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reset_graph()\n",
    "data_params = create_hyper_params()\n",
    "g2 = build_graph(data_params)\n",
    "best_params = load_obj(\"new_best_params\")\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    saver, init_global, init_local = g2.get_collection(\"save_init\")\n",
    "    X, X_reshaped, y, training_op = g2.get_collection(\"main_ops\")\n",
    "    preds, y_true_cls, y_pred_cls = g2.get_collection(\"preds\")\n",
    "    test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op = g2.get_collection(\"test_metrics\")\n",
    "    test_mean_loss, test_mean_loss_update, test_loss_reset_op = g2.get_collection(\"test_loss\")\n",
    "    logz = g2.get_collection(\"logits\")[0]\n",
    "\n",
    "    sess.run([init_global, init_local])\n",
    "\n",
    "    restore_model_params(model_params=best_params, g=g2, sess=sess)\n",
    "    sess.run([test_acc_reset_op, test_loss_reset_op])\n",
    "    Xb, yb = np.expand_dims(adv_flat,0), np.expand_dims(some_label_enc, 0)\n",
    "    batch_accuracy, batch_loss, batch_auc = sess.run([test_acc_update, test_mean_loss_update, test_auc_update], \n",
    "                                                              feed_dict={X:Xb,y:yb})\n",
    "    pred_value, true_cls_value, pred_cls_value = sess.run([preds, y_true_cls, y_pred_cls],\n",
    "                                                          feed_dict={X:Xb,y:yb})\n",
    "    logits_val = sess.run([logz], feed_dict={X:Xb,y:yb})[0]\n",
    "    print\n",
    "    final_test_acc, final_test_loss, final_test_auc = sess.run([test_acc, test_mean_loss, test_auc])\n",
    "    print(\"test auc: {:.3f}% acc: {:.3f}% loss: {:.5f}\".format(final_test_auc*100, \n",
    "                                                               final_test_acc*100,\n",
    "                                                               final_test_loss))\n",
    "    pred_idx = pred_cls_value[0]\n",
    "    print(\"pred: {}\\ntrue_class: {}\\npred_class {}\".format(pred_value[0], true_cls_value, pred_cls_value))\n",
    "    print(\"confidence: {:.4f}%\".format(pred_value[0][pred_idx]*100))\n",
    "    plt.imshow(adv.reshape(28, 28), cmap = matplotlib.cm.binary,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu_tf",
   "language": "python",
   "name": "cpu_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
