{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: (3, 5, 4, 'final', 0)\n",
      "TensorFlow: 1.6.0-dev20180105\n",
      "No GPU\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this is a custom cell that contains the common imports I personally \n",
    "# use these may/may not be necessary for the following examples\n",
    "\n",
    "# DL framework\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# common packages\n",
    "import numpy as np\n",
    "import os # handling file i/o\n",
    "import sys\n",
    "import math\n",
    "import time # timing epochs\n",
    "import random\n",
    "\n",
    "# for ordered dict when building layer components\n",
    "import collections\n",
    "\n",
    "# plotting pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import colors # making colors consistent\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable # colorbar helper\n",
    "\n",
    "\n",
    "# from imageio import imread # read image from disk\n",
    "# + data augmentation\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "\n",
    "\n",
    "import pickle # manually saving best params\n",
    "from sklearn.utils import shuffle # shuffling data batches\n",
    "from tqdm import tqdm # display training progress bar\n",
    "\n",
    "# const\n",
    "SEED = 42\n",
    "\n",
    "# Helper to make the output consistent\n",
    "def reset_graph(seed=SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# helper to create dirs if they don't already exist\n",
    "def maybe_create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print(\"{} created\".format(dir_path))\n",
    "    else:\n",
    "        print(\"{} already exists\".format(dir_path))\n",
    "    \n",
    "def make_standard_dirs(saver=True, best_params=True, tf_logs=True):\n",
    "    # `saver/` will hold tf saver files\n",
    "    maybe_create_dir(\"saver\")\n",
    "    # `best_params/` will hold a serialized version of the best params\n",
    "    # I like to keep this as a backup in case I run into issues with\n",
    "    # the saver files\n",
    "    maybe_create_dir(\"best_params\")\n",
    "    # `tf_logs/` will hold the logs that will be visable in tensorboard\n",
    "    maybe_create_dir(\"tf_logs\")\n",
    "\n",
    "    \n",
    "# set tf log level to supress messages, unless an error\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Important Version information\n",
    "print(\"Python: {}\".format(sys.version_info[:]))\n",
    "print('TensorFlow: {}'.format(tf.__version__))\n",
    "\n",
    "# Check if using GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "    \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saver already exists\n",
      "best_params already exists\n",
      "tf_logs already exists\n"
     ]
    }
   ],
   "source": [
    "make_standard_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these two functions (get_model_params and restore_model_params) are \n",
    "# ad[a|o]pted from; \n",
    "# https://github.com/ageron/handson-ml/blob/master/11_deep_learning.ipynb\n",
    "def get_model_params():\n",
    "    global_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    return {global_vars.op.name: value for global_vars, value in \n",
    "            zip(global_vars, tf.get_default_session().run(global_vars))}\n",
    "\n",
    "def restore_model_params(model_params, g, sess):\n",
    "    gvar_names = list(model_params.keys())\n",
    "    assign_ops = {gvar_name: g.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                  for gvar_name in gvar_names}\n",
    "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "# these two functions are used to manually save the best\n",
    "# model params to disk\n",
    "def save_obj(obj, name):\n",
    "    with open('best_params/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('best_params/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t10k-images-idx3-ubyte.gz\n",
      "t10k-labels-idx1-ubyte.gz\n",
      "train-images-idx3-ubyte.gz\n",
      "train-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "ROOT_DATA = \"../../ROOT_DATA/\"\n",
    "DATA_DIR = \"mnist_data\"\n",
    "\n",
    "MNIST_TRAINING_PATH = os.path.join(ROOT_DATA, DATA_DIR)\n",
    "# ensure we have the correct directory\n",
    "for _, _, files in os.walk(MNIST_TRAINING_PATH):\n",
    "    files = sorted(files)\n",
    "    for filename in files:\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../ROOT_DATA/mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../ROOT_DATA/mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../ROOT_DATA/mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../ROOT_DATA/mnist_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "MNIST = input_data.read_data_sets(MNIST_TRAINING_PATH, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_hyper_params():\n",
    "    data_params = {}\n",
    "    data_params['n_epochs'] = 150\n",
    "    data_params['batch_size'] = 128\n",
    "    data_params['buffer_size'] = 128 # for shuffling\n",
    "\n",
    "    data_params['init_lr'] = 1e-2\n",
    "\n",
    "    \n",
    "    return data_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(data_params):\n",
    "    g = tf.Graph()\n",
    "    n_outputs = 10\n",
    "    IMG_HEIGHT = 28\n",
    "    IMG_WIDTH = 28\n",
    "    CHANNELS = 1\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"inputs\"):\n",
    "            X = tf.placeholder(tf.float32, shape=(None, 784), name=\"data\") # Input\n",
    "            Xx = tf.Variable(tf.zeros((28, 28, 1)))\n",
    "            Xxx = tf.expand_dims(Xx, 0)\n",
    "            #X_reshaped = tf.reshape(Xxx, shape=[-1, IMG_HEIGHT, IMG_WIDTH, CHANNELS])\n",
    "            y = tf.placeholder(tf.int32, shape=(None, n_outputs), name=\"labels\") # Target\n",
    "\n",
    "        with tf.name_scope(\"cnn\"):\n",
    "            h_1 = tf.layers.conv2d(Xxx, filters=32, kernel_size=3, activation=tf.nn.relu,\n",
    "                                   padding='SAME', strides=1, name=\"conv_1\")\n",
    "            h_2 = tf.layers.conv2d(h_1, filters=64, kernel_size=3, activation=tf.nn.relu,\n",
    "                                   padding='SAME', strides=1, name=\"conv_2\")\n",
    "            h_3 = tf.layers.conv2d(h_1, filters=36, kernel_size=3, activation=tf.nn.elu,\n",
    "                                   padding='SAME', strides=2, name=\"conv_3\")\n",
    "            #h_4 = tf.nn.max_pool(h_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\", name=\"max_pool_1\")\n",
    "            h_4 = tf.layers.max_pooling2d(h_3, pool_size=[2,2],\n",
    "                                          strides=2, name=\"max_pool_01\")\n",
    "            last_shape = int(np.prod(h_4.get_shape()[1:]))\n",
    "            h_4_flat = tf.reshape(h_4, shape=[-1, last_shape])\n",
    "            h_5 = tf.layers.dense(h_4_flat, 64, name=\"layer_05\", activation=tf.nn.relu)\n",
    "            logits = tf.layers.dense(h_5, n_outputs, name=\"logits\")\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "            batch_loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "        \n",
    "        with tf.name_scope(\"train\"):\n",
    "            optimizer = tf.train.GradientDescentOptimizer(data_params['init_lr'])\n",
    "            training_op = optimizer.minimize(batch_loss)\n",
    "            \n",
    "        with tf.name_scope(\"save_session\"):\n",
    "            init_global = tf.global_variables_initializer()\n",
    "            init_local = tf.local_variables_initializer()\n",
    "            saver = tf.train.Saver()\n",
    "        \n",
    "        with tf.name_scope(\"adv\"):\n",
    "            x = tf.placeholder(tf.float32, (28, 28, 1), name=\"jack\") # Input\n",
    "            x_hat = Xx\n",
    "            assign_op = tf.assign(x_hat, x)\n",
    "\n",
    "            y_hat = tf.placeholder(tf.int32, ())\n",
    "            labels = tf.one_hot(y_hat, 10)\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels, name=\"adv_loss\")\n",
    "            optim_step = tf.train.GradientDescentOptimizer(1e-1).minimize(loss, var_list=[Xx])\n",
    "            \n",
    "            epsilon = tf.placeholder(tf.float32, ())\n",
    "            below = x - epsilon\n",
    "            above = x + epsilon\n",
    "            projected = tf.clip_by_value(tf.clip_by_value(x_hat, below, above), 0, 1)\n",
    "\n",
    "            with tf.control_dependencies([projected]):\n",
    "                project_step = tf.assign(x_hat, projected)\n",
    "                \n",
    "            for node in (assign_op, x, optim_step, loss, y_hat, x, epsilon, x_hat, project_step):\n",
    "                g.add_to_collection(\"adv\", node)\n",
    "\n",
    "            \n",
    "        # Ops: training metrics\n",
    "        with tf.name_scope(\"metrics\"):\n",
    "            # ================================== performance\n",
    "            with tf.name_scope(\"common\"):\n",
    "                preds = tf.nn.softmax(logits, name=\"prediction\")\n",
    "                y_true_cls = tf.argmax(y,1)\n",
    "                y_pred_cls = tf.argmax(preds,1)\n",
    "                correct_prediction = tf.equal(y_pred_cls, y_true_cls, name=\"correct_predictions\")\n",
    "                batch_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            with tf.name_scope(\"train_metrics\") as scope:\n",
    "                train_auc, train_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                train_acc, train_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                train_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                train_met_reset_op = tf.variables_initializer(train_acc_vars, name=\"train_met_reset_op\")\n",
    "            with tf.name_scope(\"val_metrics\") as scope:\n",
    "                val_auc, val_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                val_acc, val_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                val_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                val_met_reset_op = tf.variables_initializer(val_acc_vars, name=\"val_met_reset_op\")\n",
    "            with tf.name_scope(\"test_metrics\") as scope:\n",
    "                test_auc, test_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                test_acc, test_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                test_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                test_acc_reset_op = tf.variables_initializer(test_acc_vars, name=\"test_met_reset_op\")\n",
    "\n",
    "            # =============================================== loss \n",
    "            with tf.name_scope(\"train_loss_eval\") as scope:\n",
    "                train_mean_loss, train_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                train_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                train_loss_reset_op = tf.variables_initializer(train_loss_vars, name=\"train_loss_reset_op\")\n",
    "            with tf.name_scope(\"val_loss_eval\") as scope:\n",
    "                val_mean_loss, val_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                val_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                val_loss_reset_op = tf.variables_initializer(val_loss_vars, name=\"val_loss_reset_op\")\n",
    "            with tf.name_scope(\"test_loss_eval\")as scope:\n",
    "                test_mean_loss, test_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                test_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                test_loss_reset_op = tf.variables_initializer(test_loss_vars, name=\"test_loss_rest_op\")\n",
    "\n",
    "        # --- create collections\n",
    "        for node in (saver, init_global, init_local):\n",
    "            g.add_to_collection(\"save_init\", node)\n",
    "        for node in (X, Xx, y, training_op):\n",
    "            g.add_to_collection(\"main_ops\", node)\n",
    "        for node in (preds, y_true_cls, y_pred_cls):\n",
    "            g.add_to_collection(\"preds\", node)\n",
    "        for node in (train_auc, train_auc_update, train_acc, train_acc_update, train_met_reset_op):\n",
    "            g.add_to_collection(\"train_metrics\", node)\n",
    "        for node in (val_auc, val_auc_update, val_acc, val_acc_update, val_met_reset_op):\n",
    "            g.add_to_collection(\"val_metrics\", node)\n",
    "        for node in (test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op):\n",
    "            g.add_to_collection(\"test_metrics\", node)\n",
    "        for node in (train_mean_loss, train_mean_loss_update, train_loss_reset_op):\n",
    "            g.add_to_collection(\"train_loss\", node)\n",
    "        for node in (val_mean_loss, val_mean_loss_update, val_loss_reset_op):\n",
    "            g.add_to_collection(\"val_loss\", node)\n",
    "        for node in (test_mean_loss, test_mean_loss_update, test_loss_reset_op):\n",
    "            g.add_to_collection(\"test_loss\", node)\n",
    "        g.add_to_collection(\"logits\", logits)\n",
    "            \n",
    "        # ===================================== tensorboard\n",
    "        with tf.name_scope(\"tensorboard_writer\") as scope:\n",
    "            epoch_train_loss_scalar = tf.summary.scalar('train_epoch_loss', train_mean_loss)\n",
    "            epoch_train_acc_scalar = tf.summary.scalar('train_epoch_acc', train_acc)\n",
    "            epoch_train_auc_scalar = tf.summary.scalar('train_epoch_auc', train_auc)\n",
    "            epoch_train_write_op = tf.summary.merge([epoch_train_loss_scalar, epoch_train_acc_scalar, epoch_train_auc_scalar], name=\"epoch_train_write_op\")\n",
    "\n",
    "            # ===== epoch, validation\n",
    "            epoch_validation_loss_scalar = tf.summary.scalar('validation_epoch_loss', val_mean_loss)\n",
    "            epoch_validation_acc_scalar = tf.summary.scalar('validation_epoch_acc', val_acc)\n",
    "            epoch_validation_auc_scalar = tf.summary.scalar('validation_epoch_auc', val_auc)\n",
    "            epoch_validation_write_op = tf.summary.merge([epoch_validation_loss_scalar, epoch_validation_acc_scalar, epoch_validation_auc_scalar], name=\"epoch_validation_write_op\")\n",
    "        \n",
    "        for node in (epoch_train_write_op, epoch_validation_write_op):\n",
    "            g.add_to_collection(\"tensorboard\", node)\n",
    "            \n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_graph(g):\n",
    "    saver, init_global, init_local = g.get_collection(\"save_init\")\n",
    "    X, X_reshaped, y, training_op = g.get_collection(\"main_ops\")\n",
    "    preds, y_true_cls, y_pred_cls = g.get_collection(\"preds\")\n",
    "    train_auc, train_auc_update, train_acc, train_acc_update, train_met_reset_op = g.get_collection(\"train_metrics\")\n",
    "    val_auc, val_auc_update, val_acc, val_acc_update, val_met_reset_op = g.get_collection(\"val_metrics\")\n",
    "    train_mean_loss, train_mean_loss_update, train_loss_reset_op = g.get_collection(\"train_loss\")\n",
    "    val_mean_loss, val_mean_loss_update, val_loss_reset_op = g.get_collection(\"val_loss\")\n",
    "    epoch_train_write_op, epoch_validation_write_op = g.get_collection(\"tensorboard\")\n",
    "\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(\"tf_logs\",\"train\"))\n",
    "    val_writer = tf.summary.FileWriter(os.path.join(\"tf_logs\",\"validation\"))\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    with tf.Session(graph=g) as sess:\n",
    "        sess.run([init_global, init_local])\n",
    "        \n",
    "        for e in tqdm(range(1,data_params['n_epochs']+1)):\n",
    "            sess.run([val_met_reset_op,val_loss_reset_op,train_met_reset_op,train_loss_reset_op])\n",
    "            \n",
    "            n_batches = int(MNIST.train.num_examples/data_params['batch_size'])\n",
    "            for i in range(1, n_batches+1):\n",
    "                data, target = MNIST.train.next_batch(data_params['batch_size'])\n",
    "                sess.run([training_op, train_auc_update, train_acc_update, train_mean_loss_update], feed_dict={X:data, y:target})\n",
    "        \n",
    "            # write average for epoch\n",
    "            summary = sess.run(epoch_train_write_op)    \n",
    "            train_writer.add_summary(summary, e)\n",
    "            train_writer.flush()\n",
    "\n",
    "            # run validation\n",
    "            n_batches = int(MNIST.validation.num_examples/data_params['batch_size'])\n",
    "            for i in range(1,n_batches+1):\n",
    "                Xb, yb = MNIST.validation.next_batch(data_params['batch_size'])\n",
    "                sess.run([val_auc_update, val_acc_update, val_mean_loss_update], feed_dict={X:data, y:target})\n",
    "\n",
    "            # check for (and save) best validation params here\n",
    "            cur_loss, cur_acc = sess.run([val_mean_loss, val_acc])\n",
    "            if cur_loss < best_val_loss:\n",
    "                best_val_loss = cur_loss\n",
    "                best_params = get_model_params()\n",
    "                save_obj(best_params, \"best_val_params\")\n",
    "                print(\"best params saved: acc: {:.3f}% loss: {:.4f}\".format(cur_acc*100, cur_loss))\n",
    "\n",
    "\n",
    "            summary = sess.run(epoch_validation_write_op) \n",
    "            val_writer.add_summary(summary, e)\n",
    "            val_writer.flush()\n",
    "        \n",
    "        train_writer.close()\n",
    "        val_writer.close()\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/150 [00:00<?, ?it/s]\u001b[A\n",
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jack/anaconda3/envs/tf_edge/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/jack/anaconda3/envs/tf_edge/lib/python3.5/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/jack/anaconda3/envs/tf_edge/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "  1%|          | 1/150 [00:04<09:58,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 85.938% loss: 0.5085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▏         | 2/150 [00:07<09:28,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 94.531% loss: 0.2730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 3/150 [00:11<09:17,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 93.750% loss: 0.1707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 4/150 [00:14<09:07,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 97.656% loss: 0.1327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 7/150 [00:26<08:54,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 97.656% loss: 0.0856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 8/150 [00:31<09:23,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 98.438% loss: 0.0591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 9/150 [00:36<09:29,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 100.000% loss: 0.0475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 15/150 [00:59<08:51,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 100.000% loss: 0.0303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 18/150 [01:10<08:35,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 100.000% loss: 0.0252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 21/150 [01:21<08:20,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 100.000% loss: 0.0136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 23/150 [01:29<08:11,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 100.000% loss: 0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 36/150 [02:17<07:16,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 100.000% loss: 0.0091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 39/150 [02:29<07:05,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 100.000% loss: 0.0088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 42/150 [02:40<06:52,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 100.000% loss: 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 77/150 [05:01<04:45,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 100.000% loss: 0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 88/150 [05:44<04:03,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 100.000% loss: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 94/150 [06:08<03:39,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 100.000% loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 104/150 [06:50<03:01,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 100.000% loss: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 133/150 [08:39<01:06,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 100.000% loss: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [09:50<00:00,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 100.000% loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# reset_graph()\n",
    "# data_params = create_hyper_params()\n",
    "# g = build_graph(data_params)\n",
    "# sess = train_graph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "This is a checkpoint - in that training can be skipped if previous best params are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:03<00:00, 24.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 99.852% acc: 98.648% loss: 0.05209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "data_params = create_hyper_params()\n",
    "g2 = build_graph(data_params)\n",
    "best_params = load_obj(\"best_val_params\")\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    saver, init_global, init_local = g2.get_collection(\"save_init\")\n",
    "    X, X_reshaped, y, training_op = g2.get_collection(\"main_ops\")\n",
    "    preds, y_true_cls, y_pred_cls = g2.get_collection(\"preds\")\n",
    "    test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op = g2.get_collection(\"test_metrics\")\n",
    "    test_mean_loss, test_mean_loss_update, test_loss_reset_op = g2.get_collection(\"test_loss\")\n",
    "    \n",
    "    restore_model_params(model_params=best_params, g=g2, sess=sess)\n",
    "    sess.run([test_acc_reset_op, test_loss_reset_op])\n",
    "    \n",
    "    n_batches = int(MNIST.test.num_examples/data_params['batch_size'])\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        Xb, yb = MNIST.test.next_batch(data_params['batch_size'])\n",
    "        batch_accuracy, batch_loss, batch_auc = sess.run([test_acc_update, test_mean_loss_update, test_auc_update], \n",
    "                                                                  feed_dict={X:Xb,y:yb})\n",
    "    # print\n",
    "    final_test_acc, final_test_loss, final_test_auc = sess.run([test_acc, test_mean_loss, test_auc])\n",
    "    print(\"test auc: {:.3f}% acc: {:.3f}% loss: {:.5f}\".format(final_test_auc*100, \n",
    "                                                              final_test_acc*100,\n",
    "                                                              final_test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Sample Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5b8fc62a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "some_idx = 42\n",
    "some_image = MNIST.test.images[some_idx]\n",
    "some_label_enc = MNIST.test.labels[some_idx]\n",
    "some_label_dec = np.argmax(some_label_enc)\n",
    "some_digit_image = some_image.reshape(28, 28)\n",
    "plt.imshow(some_digit_image, cmap = matplotlib.cm.binary,\n",
    "           interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(some_label_dec)\n",
    "diesel = some_digit_image.reshape(28,28,1)\n",
    "print(diesel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10, loss=0.176517\n",
      "step 20, loss=0.147356\n",
      "step 30, loss=0.13825\n",
      "step 40, loss=0.132482\n",
      "step 50, loss=0.129917\n",
      "step 60, loss=0.1307\n",
      "step 70, loss=0.128183\n",
      "step 80, loss=0.126931\n",
      "step 90, loss=0.125626\n",
      "step 100, loss=0.130015\n",
      "test auc: 88.889% acc: 0.000% loss: 2.10951\n",
      "pred: [  2.49636121e-16   5.16652054e-09   1.08865248e-13   1.02887761e-08\n",
      "   1.21297523e-01   8.32749614e-13   2.38443922e-13   1.75187651e-08\n",
      "   1.72083858e-08   8.78702402e-01]\n",
      "true_class: [4]\n",
      "pred_class [9]\n",
      "confidence: 87.8702%\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "data_params = create_hyper_params()\n",
    "g2 = build_graph(data_params)\n",
    "best_params = load_obj(\"best_val_params\")\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    saver, init_global, init_local = g2.get_collection(\"save_init\")\n",
    "    X, Xx, y, training_op = g2.get_collection(\"main_ops\")\n",
    "    preds, y_true_cls, y_pred_cls = g2.get_collection(\"preds\")\n",
    "    test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op = g2.get_collection(\"test_metrics\")\n",
    "    test_mean_loss, test_mean_loss_update, test_loss_reset_op = g2.get_collection(\"test_loss\")\n",
    "    logz = g2.get_collection(\"logits\")[0]\n",
    "\n",
    "    sess.run([init_global, init_local])\n",
    "\n",
    "    restore_model_params(model_params=best_params, g=g2, sess=sess)\n",
    "    sess.run([test_acc_reset_op, test_loss_reset_op])\n",
    "    \n",
    "    # execution\n",
    "    demo_eps = 0.07\n",
    "    demo_lr = 1e-1\n",
    "    demo_steps = 100\n",
    "    demo_target = 9\n",
    "    \n",
    "    assign_op, x, optim_step, loss, y_hat, x, epsilon, x_hat, project_step = g2.get_collection(\"adv\")\n",
    "    \n",
    "    # initialization step\n",
    "    sess.run(assign_op, feed_dict={x: diesel})\n",
    "\n",
    "    # projected gradient descent\n",
    "    for i in range(demo_steps):\n",
    "        # gradient descent step\n",
    "        _, loss_value = sess.run(\n",
    "            [optim_step, loss],\n",
    "            feed_dict={y_hat: demo_target})\n",
    "        # project step\n",
    "        sess.run(project_step, feed_dict={x: diesel, epsilon: demo_eps})\n",
    "        if (i+1) % 10 == 0:\n",
    "            print('step %d, loss=%g' % (i+1, loss_value))\n",
    "\n",
    "\n",
    "    adv = x_hat.eval() # retrieve the adversarial example\n",
    "\n",
    "    Xb, yb = np.expand_dims(some_image,0), np.expand_dims(some_label_enc, 0)\n",
    "    batch_accuracy, batch_loss, batch_auc = sess.run([test_acc_update, test_mean_loss_update, test_auc_update], \n",
    "                                                              feed_dict={X:Xb,y:yb})\n",
    "    pred_value, true_cls_value, pred_cls_value = sess.run([preds, y_true_cls, y_pred_cls],\n",
    "                                                          feed_dict={X:Xb,y:yb})\n",
    "    logits_val = sess.run([logz], feed_dict={X:Xb,y:yb})[0]\n",
    "    print\n",
    "    final_test_acc, final_test_loss, final_test_auc = sess.run([test_acc, test_mean_loss, test_auc])\n",
    "    print(\"test auc: {:.3f}% acc: {:.3f}% loss: {:.5f}\".format(final_test_auc*100, \n",
    "                                                               final_test_acc*100,\n",
    "                                                               final_test_loss))\n",
    "    pred_idx = pred_cls_value[0]\n",
    "    print(\"pred: {}\\ntrue_class: {}\\npred_class {}\".format(pred_value[0], true_cls_value, pred_cls_value))\n",
    "    print(\"confidence: {:.4f}%\".format(pred_value[0][pred_idx]*100))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5b970fedd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(adv.reshape(28, 28), cmap = matplotlib.cm.binary,\n",
    "           interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(some_label_dec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu_tf",
   "language": "python",
   "name": "cpu_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
