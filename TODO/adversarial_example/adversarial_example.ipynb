{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: (3, 5, 4, 'final', 0)\n",
      "TensorFlow: 1.6.0-dev20180105\n",
      "No GPU\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this is a custom cell that contains the common imports I personally \n",
    "# use these may/may not be necessary for the following examples\n",
    "\n",
    "# DL framework\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# common packages\n",
    "import numpy as np\n",
    "import os # handling file i/o\n",
    "import sys\n",
    "import math\n",
    "import time # timing epochs\n",
    "import random\n",
    "\n",
    "# for ordered dict when building layer components\n",
    "import collections\n",
    "\n",
    "# plotting pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import colors # making colors consistent\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable # colorbar helper\n",
    "\n",
    "\n",
    "# from imageio import imread # read image from disk\n",
    "# + data augmentation\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "\n",
    "\n",
    "import pickle # manually saving best params\n",
    "from sklearn.utils import shuffle # shuffling data batches\n",
    "from tqdm import tqdm # display training progress bar\n",
    "\n",
    "# const\n",
    "SEED = 42\n",
    "\n",
    "# Helper to make the output consistent\n",
    "def reset_graph(seed=SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# helper to create dirs if they don't already exist\n",
    "def maybe_create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print(\"{} created\".format(dir_path))\n",
    "    else:\n",
    "        print(\"{} already exists\".format(dir_path))\n",
    "    \n",
    "def make_standard_dirs(saver=True, best_params=True, tf_logs=True):\n",
    "    # `saver/` will hold tf saver files\n",
    "    maybe_create_dir(\"saver\")\n",
    "    # `best_params/` will hold a serialized version of the best params\n",
    "    # I like to keep this as a backup in case I run into issues with\n",
    "    # the saver files\n",
    "    maybe_create_dir(\"best_params\")\n",
    "    # `tf_logs/` will hold the logs that will be visable in tensorboard\n",
    "    maybe_create_dir(\"tf_logs\")\n",
    "\n",
    "    \n",
    "# set tf log level to supress messages, unless an error\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Important Version information\n",
    "print(\"Python: {}\".format(sys.version_info[:]))\n",
    "print('TensorFlow: {}'.format(tf.__version__))\n",
    "\n",
    "# Check if using GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "    \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saver already exists\n",
      "best_params already exists\n",
      "tf_logs already exists\n"
     ]
    }
   ],
   "source": [
    "make_standard_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BEST_PARAMS_PATH = \"new_best_params\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these two functions (get_model_params and restore_model_params) are \n",
    "# ad[a|o]pted from; \n",
    "# https://github.com/ageron/handson-ml/blob/master/11_deep_learning.ipynb\n",
    "def get_model_params():\n",
    "    global_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    return {global_vars.op.name: value for global_vars, value in \n",
    "            zip(global_vars, tf.get_default_session().run(global_vars))}\n",
    "\n",
    "def restore_model_params(model_params, g, sess):\n",
    "    gvar_names = list(model_params.keys())\n",
    "    assign_ops = {gvar_name: g.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                  for gvar_name in gvar_names}\n",
    "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "# these two functions are used to manually save the best\n",
    "# model params to disk\n",
    "def save_obj(obj, name):\n",
    "    with open('best_params/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('best_params/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t10k-images-idx3-ubyte.gz\n",
      "t10k-labels-idx1-ubyte.gz\n",
      "train-images-idx3-ubyte.gz\n",
      "train-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "ROOT_DATA = \"../../ROOT_DATA/\"\n",
    "DATA_DIR = \"mnist_data\"\n",
    "\n",
    "MNIST_TRAINING_PATH = os.path.join(ROOT_DATA, DATA_DIR)\n",
    "# ensure we have the correct directory\n",
    "for _, _, files in os.walk(MNIST_TRAINING_PATH):\n",
    "    files = sorted(files)\n",
    "    for filename in files:\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../ROOT_DATA/mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../ROOT_DATA/mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../ROOT_DATA/mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../ROOT_DATA/mnist_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "MNIST = input_data.read_data_sets(MNIST_TRAINING_PATH, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_hyper_params():\n",
    "    data_params = {}\n",
    "    data_params['n_epochs'] = 5\n",
    "    data_params['batch_size'] = 128\n",
    "    data_params['buffer_size'] = 128 # for shuffling\n",
    "\n",
    "    data_params['init_lr'] = 1e-2\n",
    "\n",
    "    return data_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(data_params):\n",
    "    g = tf.Graph()\n",
    "    n_outputs = 10\n",
    "    IMG_HEIGHT = 28\n",
    "    IMG_WIDTH = 28\n",
    "    CHANNELS = 1\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"inputs\"):\n",
    "            X = tf.placeholder(tf.float32, shape=(None, 784), name=\"data\") # Input\n",
    "            X_reshaped = tf.reshape(X, shape=[-1, IMG_HEIGHT, IMG_WIDTH, CHANNELS])\n",
    "            y = tf.placeholder(tf.int32, shape=(None, n_outputs), name=\"labels\") # Target\n",
    "\n",
    "        with tf.name_scope(\"cnn\"):\n",
    "            h_1 = tf.layers.conv2d(X_reshaped, filters=32, kernel_size=3, activation=tf.nn.relu,\n",
    "                                   padding='SAME', strides=1, name=\"conv_1\")\n",
    "            h_2 = tf.layers.conv2d(h_1, filters=64, kernel_size=3, activation=tf.nn.relu,\n",
    "                                   padding='SAME', strides=1, name=\"conv_2\")\n",
    "            h_3 = tf.layers.conv2d(h_1, filters=36, kernel_size=3, activation=tf.nn.elu,\n",
    "                                   padding='SAME', strides=2, name=\"conv_3\")\n",
    "            h_4 = tf.layers.max_pooling2d(h_3, pool_size=[2,2],\n",
    "                                          strides=2, name=\"max_pool_01\")\n",
    "            last_shape = int(np.prod(h_4.get_shape()[1:]))\n",
    "            h_4_flat = tf.reshape(h_4, shape=[-1, last_shape])\n",
    "            h_5 = tf.layers.dense(h_4_flat, 64, name=\"layer_05\", activation=tf.nn.relu)\n",
    "            logits = tf.layers.dense(h_5, n_outputs, name=\"logits\")\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "            batch_loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "        \n",
    "        with tf.name_scope(\"train\"):\n",
    "            optimizer = tf.train.GradientDescentOptimizer(data_params['init_lr'])\n",
    "            training_op = optimizer.minimize(batch_loss)\n",
    "            \n",
    "        with tf.name_scope(\"save_session\"):\n",
    "            init_global = tf.global_variables_initializer()\n",
    "            init_local = tf.local_variables_initializer()\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "        # Ops: training metrics\n",
    "        with tf.name_scope(\"metrics\"):\n",
    "            # ================================== performance\n",
    "            with tf.name_scope(\"common\"):\n",
    "                preds = tf.nn.softmax(logits, name=\"prediction\")\n",
    "                y_true_cls = tf.argmax(y,1)\n",
    "                y_pred_cls = tf.argmax(preds,1)\n",
    "                correct_prediction = tf.equal(y_pred_cls, y_true_cls, name=\"correct_predictions\")\n",
    "                batch_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            with tf.name_scope(\"train_metrics\") as scope:\n",
    "                train_auc, train_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                train_acc, train_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                train_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                train_met_reset_op = tf.variables_initializer(train_acc_vars, name=\"train_met_reset_op\")\n",
    "            with tf.name_scope(\"val_metrics\") as scope:\n",
    "                val_auc, val_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                val_acc, val_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                val_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                val_met_reset_op = tf.variables_initializer(val_acc_vars, name=\"val_met_reset_op\")\n",
    "            with tf.name_scope(\"test_metrics\") as scope:\n",
    "                test_auc, test_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                test_acc, test_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                test_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                test_acc_reset_op = tf.variables_initializer(test_acc_vars, name=\"test_met_reset_op\")\n",
    "\n",
    "            # =============================================== loss \n",
    "            with tf.name_scope(\"train_loss_eval\") as scope:\n",
    "                train_mean_loss, train_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                train_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                train_loss_reset_op = tf.variables_initializer(train_loss_vars, name=\"train_loss_reset_op\")\n",
    "            with tf.name_scope(\"val_loss_eval\") as scope:\n",
    "                val_mean_loss, val_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                val_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                val_loss_reset_op = tf.variables_initializer(val_loss_vars, name=\"val_loss_reset_op\")\n",
    "            with tf.name_scope(\"test_loss_eval\")as scope:\n",
    "                test_mean_loss, test_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                test_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                test_loss_reset_op = tf.variables_initializer(test_loss_vars, name=\"test_loss_rest_op\")\n",
    "\n",
    "        # --- create collections\n",
    "        for node in (saver, init_global, init_local):\n",
    "            g.add_to_collection(\"save_init\", node)\n",
    "        for node in (X, X_reshaped, y, training_op):\n",
    "            g.add_to_collection(\"main_ops\", node)\n",
    "        for node in (preds, y_true_cls, y_pred_cls):\n",
    "            g.add_to_collection(\"preds\", node)\n",
    "        for node in (train_auc, train_auc_update, train_acc, train_acc_update, train_met_reset_op):\n",
    "            g.add_to_collection(\"train_metrics\", node)\n",
    "        for node in (val_auc, val_auc_update, val_acc, val_acc_update, val_met_reset_op):\n",
    "            g.add_to_collection(\"val_metrics\", node)\n",
    "        for node in (test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op):\n",
    "            g.add_to_collection(\"test_metrics\", node)\n",
    "        for node in (train_mean_loss, train_mean_loss_update, train_loss_reset_op):\n",
    "            g.add_to_collection(\"train_loss\", node)\n",
    "        for node in (val_mean_loss, val_mean_loss_update, val_loss_reset_op):\n",
    "            g.add_to_collection(\"val_loss\", node)\n",
    "        for node in (test_mean_loss, test_mean_loss_update, test_loss_reset_op):\n",
    "            g.add_to_collection(\"test_loss\", node)\n",
    "        g.add_to_collection(\"logits\", logits)\n",
    "            \n",
    "        # ===================================== tensorboard\n",
    "        with tf.name_scope(\"tensorboard_writer\") as scope:\n",
    "            epoch_train_loss_scalar = tf.summary.scalar('train_epoch_loss', train_mean_loss)\n",
    "            epoch_train_acc_scalar = tf.summary.scalar('train_epoch_acc', train_acc)\n",
    "            epoch_train_auc_scalar = tf.summary.scalar('train_epoch_auc', train_auc)\n",
    "            epoch_train_write_op = tf.summary.merge([epoch_train_loss_scalar, epoch_train_acc_scalar, epoch_train_auc_scalar], name=\"epoch_train_write_op\")\n",
    "\n",
    "            # ===== epoch, validation\n",
    "            epoch_validation_loss_scalar = tf.summary.scalar('validation_epoch_loss', val_mean_loss)\n",
    "            epoch_validation_acc_scalar = tf.summary.scalar('validation_epoch_acc', val_acc)\n",
    "            epoch_validation_auc_scalar = tf.summary.scalar('validation_epoch_auc', val_auc)\n",
    "            epoch_validation_write_op = tf.summary.merge([epoch_validation_loss_scalar, epoch_validation_acc_scalar, epoch_validation_auc_scalar], name=\"epoch_validation_write_op\")\n",
    "        \n",
    "        for node in (epoch_train_write_op, epoch_validation_write_op):\n",
    "            g.add_to_collection(\"tensorboard\", node)\n",
    "            \n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_graph(g):\n",
    "    global BEST_PARAMS_PATH\n",
    "    saver, init_global, init_local = g.get_collection(\"save_init\")\n",
    "    X, X_reshaped, y, training_op = g.get_collection(\"main_ops\")\n",
    "    preds, y_true_cls, y_pred_cls = g.get_collection(\"preds\")\n",
    "    train_auc, train_auc_update, train_acc, train_acc_update, train_met_reset_op = g.get_collection(\"train_metrics\")\n",
    "    val_auc, val_auc_update, val_acc, val_acc_update, val_met_reset_op = g.get_collection(\"val_metrics\")\n",
    "    train_mean_loss, train_mean_loss_update, train_loss_reset_op = g.get_collection(\"train_loss\")\n",
    "    val_mean_loss, val_mean_loss_update, val_loss_reset_op = g.get_collection(\"val_loss\")\n",
    "    epoch_train_write_op, epoch_validation_write_op = g.get_collection(\"tensorboard\")\n",
    "\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(\"tf_logs\",\"train\"))\n",
    "    val_writer = tf.summary.FileWriter(os.path.join(\"tf_logs\",\"validation\"))\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    with tf.Session(graph=g) as sess:\n",
    "        sess.run([init_global, init_local])\n",
    "        \n",
    "        for e in tqdm(range(1,data_params['n_epochs']+1)):\n",
    "            sess.run([val_met_reset_op,val_loss_reset_op,train_met_reset_op,train_loss_reset_op])\n",
    "            \n",
    "            n_batches = int(MNIST.train.num_examples/data_params['batch_size'])\n",
    "            for i in range(1, n_batches+1):\n",
    "                data, target = MNIST.train.next_batch(data_params['batch_size'])\n",
    "                sess.run([training_op, train_auc_update, train_acc_update, train_mean_loss_update], feed_dict={X:data, y:target})\n",
    "        \n",
    "            # write average for epoch\n",
    "            summary = sess.run(epoch_train_write_op)    \n",
    "            train_writer.add_summary(summary, e)\n",
    "            train_writer.flush()\n",
    "\n",
    "            # run validation\n",
    "            n_batches = int(MNIST.validation.num_examples/data_params['batch_size'])\n",
    "            for i in range(1,n_batches+1):\n",
    "                Xb, yb = MNIST.validation.next_batch(data_params['batch_size'])\n",
    "                sess.run([val_auc_update, val_acc_update, val_mean_loss_update], feed_dict={X:data, y:target})\n",
    "\n",
    "            # check for (and save) best validation params here\n",
    "            cur_loss, cur_acc = sess.run([val_mean_loss, val_acc])\n",
    "            if cur_loss < best_val_loss:\n",
    "                best_val_loss = cur_loss\n",
    "                best_params = get_model_params()\n",
    "                save_obj(best_params, BEST_PARAMS_PATH)\n",
    "                print(\"best params saved: acc: {:.3f}% loss: {:.4f}\".format(cur_acc*100, cur_loss))\n",
    "\n",
    "            summary = sess.run(epoch_validation_write_op) \n",
    "            val_writer.add_summary(summary, e)\n",
    "            val_writer.flush()\n",
    "        \n",
    "        train_writer.close()\n",
    "        val_writer.close()\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-f3a74a302cbd>:28: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:54<03:39, 54.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 88.281% loss: 0.3587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [01:56<02:54, 58.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 90.625% loss: 0.3209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [02:56<01:57, 58.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: acc: 96.875% loss: 0.1216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [05:07<00:00, 61.54s/it]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "data_params = create_hyper_params()\n",
    "g = build_graph(data_params)\n",
    "sess = train_graph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "This is a checkpoint - in that training can be skipped if previous best params are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:03<00:00, 19.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 99.426% acc: 91.967% loss: 0.26584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "data_params = create_hyper_params()\n",
    "g2 = build_graph(data_params)\n",
    "best_params = load_obj(BEST_PARAMS_PATH)\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    saver, init_global, init_local = g2.get_collection(\"save_init\")\n",
    "    X, X_reshaped, y, training_op = g2.get_collection(\"main_ops\")\n",
    "    preds, y_true_cls, y_pred_cls = g2.get_collection(\"preds\")\n",
    "    test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op = g2.get_collection(\"test_metrics\")\n",
    "    test_mean_loss, test_mean_loss_update, test_loss_reset_op = g2.get_collection(\"test_loss\")\n",
    "    \n",
    "    restore_model_params(model_params=best_params, g=g2, sess=sess)\n",
    "    sess.run([test_acc_reset_op, test_loss_reset_op])\n",
    "    \n",
    "    n_batches = int(MNIST.test.num_examples/data_params['batch_size'])\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        Xb, yb = MNIST.test.next_batch(data_params['batch_size'])\n",
    "        batch_accuracy, batch_loss, batch_auc = sess.run([test_acc_update, test_mean_loss_update, test_auc_update], \n",
    "                                                                  feed_dict={X:Xb,y:yb})\n",
    "    # print\n",
    "    final_test_acc, final_test_loss, final_test_auc = sess.run([test_acc, test_mean_loss, test_auc])\n",
    "    print(\"test auc: {:.3f}% acc: {:.3f}% loss: {:.5f}\".format(final_test_auc*100, \n",
    "                                                              final_test_acc*100,\n",
    "                                                              final_test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Sample Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0bb0f7edd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "some_idx = 42\n",
    "some_image = MNIST.test.images[some_idx]\n",
    "some_label_enc = MNIST.test.labels[some_idx]\n",
    "some_label_dec = np.argmax(some_label_enc)\n",
    "some_digit_image = some_image.reshape(28, 28)\n",
    "plt.imshow(some_digit_image, cmap = matplotlib.cm.binary,\n",
    "           interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the current architecture classify the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function needs some work, but it currently serves it's purpose\n",
    "def display_figure_and_prob(img, probs):\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\n",
    "    ax1.imshow(img, cmap = matplotlib.cm.binary,\n",
    "           interpolation=\"nearest\")\n",
    "    y_pos = np.arange(10)\n",
    "    ax2.bar(y_pos, probs, align='center', alpha=0.5)\n",
    "    plt.title('Confidence')\n",
    "    plt.grid('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 100.000% acc: 100.000% loss: 0.36743\n",
      "true_class: [3]\n",
      "pred_class [3]\n",
      "confidence: 69.2512%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0bb06e77b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reset_graph()\n",
    "data_params = create_hyper_params()\n",
    "g2 = build_graph(data_params)\n",
    "best_params = load_obj(BEST_PARAMS_PATH)\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    saver, init_global, init_local = g2.get_collection(\"save_init\")\n",
    "    X, X_reshaped, y, training_op = g2.get_collection(\"main_ops\")\n",
    "    preds, y_true_cls, y_pred_cls = g2.get_collection(\"preds\")\n",
    "    test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op = g2.get_collection(\"test_metrics\")\n",
    "    test_mean_loss, test_mean_loss_update, test_loss_reset_op = g2.get_collection(\"test_loss\")\n",
    "    logz = g2.get_collection(\"logits\")[0]\n",
    "\n",
    "    sess.run([init_global, init_local])\n",
    "\n",
    "    restore_model_params(model_params=best_params, g=g2, sess=sess)\n",
    "    sess.run([test_acc_reset_op, test_loss_reset_op])\n",
    "    Xb, yb = np.expand_dims(some_image,0), np.expand_dims(some_label_enc, 0)\n",
    "    batch_accuracy, batch_loss, batch_auc = sess.run([test_acc_update, test_mean_loss_update, test_auc_update], \n",
    "                                                              feed_dict={X:Xb,y:yb})\n",
    "    pred_value, true_cls_value, pred_cls_value = sess.run([preds, y_true_cls, y_pred_cls],\n",
    "                                                          feed_dict={X:Xb,y:yb})\n",
    "    logits_val = sess.run([logz], feed_dict={X:Xb,y:yb})[0]\n",
    "    print\n",
    "    final_test_acc, final_test_loss, final_test_auc = sess.run([test_acc, test_mean_loss, test_auc])\n",
    "    print(\"test auc: {:.3f}% acc: {:.3f}% loss: {:.5f}\".format(final_test_auc*100, \n",
    "                                                               final_test_acc*100,\n",
    "                                                               final_test_loss))\n",
    "    pred_idx = pred_cls_value[0]\n",
    "    print(\"true_class: {}\\npred_class {}\".format(true_cls_value, pred_cls_value))\n",
    "    print(\"confidence: {:.4f}%\".format(pred_value[0][pred_idx]*100))\n",
    "    \n",
    "    display_figure_and_prob(some_digit_image, pred_value[0])\n",
    "#     plt.imshow(some_digit_image, cmap = matplotlib.cm.binary,\n",
    "#                interpolation=\"nearest\")\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_adv_graph(data_params):\n",
    "    g = tf.Graph()\n",
    "    n_outputs = 10\n",
    "    IMG_HEIGHT = 28\n",
    "    IMG_WIDTH = 28\n",
    "    CHANNELS = 1\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"inputs\"):\n",
    "            #X = tf.placeholder(tf.float32, shape=(None, 784), name=\"data\") # Input\n",
    "            #X_reshaped = tf.reshape(X, shape=[-1, IMG_HEIGHT, IMG_WIDTH, CHANNELS])\n",
    "            Xx = tf.Variable(tf.zeros((28, 28, 1)))\n",
    "            Xxx = tf.expand_dims(Xx, 0)\n",
    "            y = tf.placeholder(tf.int32, shape=(None, n_outputs), name=\"labels\") # Target\n",
    "\n",
    "        with tf.name_scope(\"cnn\"):\n",
    "            h_1 = tf.layers.conv2d(Xxx, filters=32, kernel_size=3, activation=tf.nn.relu,\n",
    "                                   padding='SAME', strides=1, name=\"conv_1\")\n",
    "            h_2 = tf.layers.conv2d(h_1, filters=64, kernel_size=3, activation=tf.nn.relu,\n",
    "                                   padding='SAME', strides=1, name=\"conv_2\")\n",
    "            h_3 = tf.layers.conv2d(h_1, filters=36, kernel_size=3, activation=tf.nn.elu,\n",
    "                                   padding='SAME', strides=2, name=\"conv_3\")\n",
    "            h_4 = tf.layers.max_pooling2d(h_3, pool_size=[2,2],\n",
    "                                          strides=2, name=\"max_pool_01\")\n",
    "            last_shape = int(np.prod(h_4.get_shape()[1:]))\n",
    "            h_4_flat = tf.reshape(h_4, shape=[-1, last_shape])\n",
    "            h_5 = tf.layers.dense(h_4_flat, 64, name=\"layer_05\", activation=tf.nn.relu)\n",
    "            logits = tf.layers.dense(h_5, n_outputs, name=\"logits\")\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "            batch_loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "        \n",
    "        with tf.name_scope(\"train\"):\n",
    "            optimizer = tf.train.GradientDescentOptimizer(data_params['init_lr'])\n",
    "            training_op = optimizer.minimize(batch_loss)\n",
    "            \n",
    "        with tf.name_scope(\"save_session\"):\n",
    "            init_global = tf.global_variables_initializer()\n",
    "            init_local = tf.local_variables_initializer()\n",
    "            saver = tf.train.Saver()\n",
    "        \n",
    "        with tf.name_scope(\"adv\"):\n",
    "            x = tf.placeholder(tf.float32, (28, 28, 1), name=\"jack\") # Input\n",
    "            x_hat = Xx\n",
    "            assign_op = tf.assign(x_hat, x)\n",
    "\n",
    "            y_hat = tf.placeholder(tf.int32, ())\n",
    "            labels = tf.one_hot(y_hat, 10)\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels, name=\"adv_loss\")\n",
    "            optim_step = tf.train.GradientDescentOptimizer(1e-1).minimize(loss, var_list=[Xx])\n",
    "            \n",
    "            epsilon = tf.placeholder(tf.float32, ())\n",
    "            below = x - epsilon\n",
    "            above = x + epsilon\n",
    "            projected = tf.clip_by_value(tf.clip_by_value(x_hat, below, above), 0, 1)\n",
    "\n",
    "            with tf.control_dependencies([projected]):\n",
    "                project_step = tf.assign(x_hat, projected)\n",
    "                \n",
    "            for node in (assign_op, x, optim_step, loss, y_hat, epsilon, x_hat, project_step):\n",
    "                g.add_to_collection(\"adv\", node)\n",
    "\n",
    "        # Ops: training metrics\n",
    "        with tf.name_scope(\"metrics\"):\n",
    "            # ================================== performance\n",
    "            with tf.name_scope(\"common\"):\n",
    "                preds = tf.nn.softmax(logits, name=\"prediction\")\n",
    "                y_true_cls = tf.argmax(y,1)\n",
    "                y_pred_cls = tf.argmax(preds,1)\n",
    "                correct_prediction = tf.equal(y_pred_cls, y_true_cls, name=\"correct_predictions\")\n",
    "                batch_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            with tf.name_scope(\"train_metrics\") as scope:\n",
    "                train_auc, train_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                train_acc, train_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                train_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                train_met_reset_op = tf.variables_initializer(train_acc_vars, name=\"train_met_reset_op\")\n",
    "            with tf.name_scope(\"val_metrics\") as scope:\n",
    "                val_auc, val_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                val_acc, val_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                val_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                val_met_reset_op = tf.variables_initializer(val_acc_vars, name=\"val_met_reset_op\")\n",
    "            with tf.name_scope(\"test_metrics\") as scope:\n",
    "                test_auc, test_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                test_acc, test_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                test_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                test_acc_reset_op = tf.variables_initializer(test_acc_vars, name=\"test_met_reset_op\")\n",
    "\n",
    "            # =============================================== loss \n",
    "            with tf.name_scope(\"train_loss_eval\") as scope:\n",
    "                train_mean_loss, train_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                train_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                train_loss_reset_op = tf.variables_initializer(train_loss_vars, name=\"train_loss_reset_op\")\n",
    "            with tf.name_scope(\"val_loss_eval\") as scope:\n",
    "                val_mean_loss, val_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                val_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                val_loss_reset_op = tf.variables_initializer(val_loss_vars, name=\"val_loss_reset_op\")\n",
    "            with tf.name_scope(\"test_loss_eval\")as scope:\n",
    "                test_mean_loss, test_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                test_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                test_loss_reset_op = tf.variables_initializer(test_loss_vars, name=\"test_loss_rest_op\")\n",
    "\n",
    "        # --- create collections\n",
    "        for node in (saver, init_global, init_local):\n",
    "            g.add_to_collection(\"save_init\", node)\n",
    "        for node in (X, Xx, y, training_op):\n",
    "            g.add_to_collection(\"main_ops\", node)\n",
    "        for node in (preds, y_true_cls, y_pred_cls):\n",
    "            g.add_to_collection(\"preds\", node)\n",
    "        for node in (train_auc, train_auc_update, train_acc, train_acc_update, train_met_reset_op):\n",
    "            g.add_to_collection(\"train_metrics\", node)\n",
    "        for node in (val_auc, val_auc_update, val_acc, val_acc_update, val_met_reset_op):\n",
    "            g.add_to_collection(\"val_metrics\", node)\n",
    "        for node in (test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op):\n",
    "            g.add_to_collection(\"test_metrics\", node)\n",
    "        for node in (train_mean_loss, train_mean_loss_update, train_loss_reset_op):\n",
    "            g.add_to_collection(\"train_loss\", node)\n",
    "        for node in (val_mean_loss, val_mean_loss_update, val_loss_reset_op):\n",
    "            g.add_to_collection(\"val_loss\", node)\n",
    "        for node in (test_mean_loss, test_mean_loss_update, test_loss_reset_op):\n",
    "            g.add_to_collection(\"test_loss\", node)\n",
    "        g.add_to_collection(\"logits\", logits)\n",
    "            \n",
    "        # ===================================== tensorboard\n",
    "        with tf.name_scope(\"tensorboard_writer\") as scope:\n",
    "            epoch_train_loss_scalar = tf.summary.scalar('train_epoch_loss', train_mean_loss)\n",
    "            epoch_train_acc_scalar = tf.summary.scalar('train_epoch_acc', train_acc)\n",
    "            epoch_train_auc_scalar = tf.summary.scalar('train_epoch_auc', train_auc)\n",
    "            epoch_train_write_op = tf.summary.merge([epoch_train_loss_scalar, epoch_train_acc_scalar, epoch_train_auc_scalar], name=\"epoch_train_write_op\")\n",
    "\n",
    "            # ===== epoch, validation\n",
    "            epoch_validation_loss_scalar = tf.summary.scalar('validation_epoch_loss', val_mean_loss)\n",
    "            epoch_validation_acc_scalar = tf.summary.scalar('validation_epoch_acc', val_acc)\n",
    "            epoch_validation_auc_scalar = tf.summary.scalar('validation_epoch_auc', val_auc)\n",
    "            epoch_validation_write_op = tf.summary.merge([epoch_validation_loss_scalar, epoch_validation_acc_scalar, epoch_validation_auc_scalar], name=\"epoch_validation_write_op\")\n",
    "        \n",
    "        for node in (epoch_train_write_op, epoch_validation_write_op):\n",
    "            g.add_to_collection(\"tensorboard\", node)\n",
    "            \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10, loss=0.193227\n",
      "step 20, loss=0.136788\n",
      "step 30, loss=0.118117\n",
      "step 40, loss=0.10917\n",
      "step 50, loss=0.103172\n",
      "step 60, loss=0.0988114\n",
      "step 70, loss=0.0958521\n",
      "step 80, loss=0.0935428\n",
      "step 90, loss=0.0921048\n",
      "step 100, loss=0.0908716\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "data_params = create_hyper_params()\n",
    "g3 = build_adv_graph(data_params)\n",
    "best_params = load_obj(BEST_PARAMS_PATH)\n",
    "with tf.Session(graph=g3) as sess:\n",
    "    saver, init_global, init_local = g3.get_collection(\"save_init\")\n",
    "    X, Xx, y, training_op = g3.get_collection(\"main_ops\")\n",
    "    preds, y_true_cls, y_pred_cls = g3.get_collection(\"preds\")\n",
    "    test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op = g3.get_collection(\"test_metrics\")\n",
    "    test_mean_loss, test_mean_loss_update, test_loss_reset_op = g3.get_collection(\"test_loss\")\n",
    "    logz = g3.get_collection(\"logits\")[0]\n",
    "\n",
    "    sess.run([init_global, init_local])\n",
    "\n",
    "    restore_model_params(model_params=best_params, g=g3, sess=sess)\n",
    "    sess.run([test_acc_reset_op, test_loss_reset_op])\n",
    "    \n",
    "    # execution\n",
    "    demo_eps = 0.07\n",
    "    demo_lr = 1e-1\n",
    "    demo_steps = 100\n",
    "    demo_target = 5\n",
    "    \n",
    "    assign_op, x, optim_step, loss, y_hat, epsilon, x_hat, project_step = g3.get_collection(\"adv\")\n",
    "    \n",
    "    # initialization step\n",
    "    sess.run(assign_op, feed_dict={x: diesel})\n",
    "\n",
    "    # projected gradient descent\n",
    "    for i in range(demo_steps):\n",
    "        # gradient descent step\n",
    "        _, loss_value = sess.run(\n",
    "            [optim_step, loss],\n",
    "            feed_dict={y_hat: demo_target})\n",
    "        # project step\n",
    "        sess.run(project_step, feed_dict={x: diesel, epsilon: demo_eps})\n",
    "        if (i+1) % 10 == 0:\n",
    "            print('step %d, loss=%g' % (i+1, loss_value))\n",
    "\n",
    "    adv = x_hat.eval() # retrieve the adversarial example\n",
    "    adv_flat = adv.reshape((784))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 88.889% acc: 0.000% loss: 2.66948\n",
      "true_class: [3]\n",
      "pred_class [5]\n",
      "confidence: 91.3249%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0ba0c7c6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reset_graph()\n",
    "data_params = create_hyper_params()\n",
    "g2 = build_graph(data_params)\n",
    "best_params = load_obj(BEST_PARAMS_PATH)\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    saver, init_global, init_local = g2.get_collection(\"save_init\")\n",
    "    X, X_reshaped, y, training_op = g2.get_collection(\"main_ops\")\n",
    "    preds, y_true_cls, y_pred_cls = g2.get_collection(\"preds\")\n",
    "    test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op = g2.get_collection(\"test_metrics\")\n",
    "    test_mean_loss, test_mean_loss_update, test_loss_reset_op = g2.get_collection(\"test_loss\")\n",
    "    logz = g2.get_collection(\"logits\")[0]\n",
    "\n",
    "    sess.run([init_global, init_local])\n",
    "\n",
    "    restore_model_params(model_params=best_params, g=g2, sess=sess)\n",
    "    sess.run([test_acc_reset_op, test_loss_reset_op])\n",
    "    Xb, yb = np.expand_dims(adv_flat,0), np.expand_dims(some_label_enc, 0)\n",
    "    batch_accuracy, batch_loss, batch_auc = sess.run([test_acc_update, test_mean_loss_update, test_auc_update], \n",
    "                                                              feed_dict={X:Xb,y:yb})\n",
    "    pred_value, true_cls_value, pred_cls_value = sess.run([preds, y_true_cls, y_pred_cls],\n",
    "                                                          feed_dict={X:Xb,y:yb})\n",
    "    logits_val = sess.run([logz], feed_dict={X:Xb,y:yb})[0]\n",
    "    \n",
    "    final_test_acc, final_test_loss, final_test_auc = sess.run([test_acc, test_mean_loss, test_auc])\n",
    "    print(\"test auc: {:.3f}% acc: {:.3f}% loss: {:.5f}\".format(final_test_auc*100, \n",
    "                                                               final_test_acc*100,\n",
    "                                                               final_test_loss))\n",
    "    pred_idx = pred_cls_value[0]\n",
    "    print(\"true_class: {}\\npred_class {}\".format(true_cls_value, pred_cls_value))\n",
    "    print(\"confidence: {:.4f}%\".format(pred_value[0][pred_idx]*100))\n",
    "    \n",
    "    display_figure_and_prob(adv.reshape(28, 28), pred_value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu_tf",
   "language": "python",
   "name": "cpu_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
