{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: (3, 5, 4, 'final', 0)\n",
      "TensorFlow: 1.4.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this is a custom cell that contains the common imports I personally \n",
    "# use these may/may not be necessary for the following examples\n",
    "\n",
    "# DL framework\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# common packages\n",
    "import numpy as np\n",
    "import os # handling file i/o\n",
    "import sys\n",
    "import math\n",
    "import time # timing epochs\n",
    "import random\n",
    "\n",
    "# for ordered dict when building layer components\n",
    "import collections\n",
    "\n",
    "# plotting pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import colors # making colors consistent\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable # colorbar helper\n",
    "\n",
    "\n",
    "# from imageio import imread # read image from disk\n",
    "# + data augmentation\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "\n",
    "\n",
    "import pickle # manually saving best params\n",
    "from sklearn.utils import shuffle # shuffling data batches\n",
    "from tqdm import tqdm # display training progress bar\n",
    "\n",
    "# const\n",
    "SEED = 42\n",
    "\n",
    "# Helper to make the output consistent\n",
    "def reset_graph(seed=SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# helper to create dirs if they don't already exist\n",
    "def maybe_create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print(\"{} created\".format(dir_path))\n",
    "    else:\n",
    "        print(\"{} already exists\".format(dir_path))\n",
    "    \n",
    "def make_standard_dirs(saver=True, best_params=True, tf_logs=True):\n",
    "    # `saver/` will hold tf saver files\n",
    "    maybe_create_dir(\"saver\")\n",
    "    # `best_params/` will hold a serialized version of the best params\n",
    "    # I like to keep this as a backup in case I run into issues with\n",
    "    # the saver files\n",
    "    maybe_create_dir(\"best_params\")\n",
    "    # `tf_logs/` will hold the logs that will be visable in tensorboard\n",
    "    maybe_create_dir(\"tf_logs\")\n",
    "\n",
    "    \n",
    "# set tf log level to supress messages, unless an error\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Important Version information\n",
    "print(\"Python: {}\".format(sys.version_info[:]))\n",
    "print('TensorFlow: {}'.format(tf.__version__))\n",
    "\n",
    "# Check if using GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "    \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saver already exists\n",
      "best_params already exists\n",
      "tf_logs already exists\n"
     ]
    }
   ],
   "source": [
    "make_standard_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean all logs\n",
    "## WARNING! You likely don't want to do this (but if you do, this is a convenient call)\n",
    "# !rm -r -f ./tf_logs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these two functions (get_model_params and restore_model_params) are \n",
    "# ad[a|o]pted from; \n",
    "# https://github.com/ageron/handson-ml/blob/master/11_deep_learning.ipynb\n",
    "def get_model_params():\n",
    "    global_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    return {global_vars.op.name: value for global_vars, value in \n",
    "            zip(global_vars, tf.get_default_session().run(global_vars))}\n",
    "\n",
    "def restore_model_params(model_params, g, sess):\n",
    "    gvar_names = list(model_params.keys())\n",
    "    assign_ops = {gvar_name: g.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                  for gvar_name in gvar_names}\n",
    "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "# these two functions are used to manually save the best\n",
    "# model params to disk\n",
    "def save_obj(obj, name):\n",
    "    with open('best_params/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('best_params/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t10k-images-idx3-ubyte.gz\n",
      "t10k-labels-idx1-ubyte.gz\n",
      "train-images-idx3-ubyte.gz\n",
      "train-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "ROOT_DATA = \"../../ROOT_DATA/\"\n",
    "DATA_DIR = \"mnist_data\"\n",
    "BEST_PARAMS_PATH = \"best_params\"\n",
    "\n",
    "MNIST_TRAINING_PATH = os.path.join(ROOT_DATA, DATA_DIR)\n",
    "# ensure we have the correct directory\n",
    "for _, _, files in os.walk(MNIST_TRAINING_PATH):\n",
    "    files = sorted(files)\n",
    "    for filename in files:\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../ROOT_DATA/mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../ROOT_DATA/mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../ROOT_DATA/mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../ROOT_DATA/mnist_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "MNIST = input_data.read_data_sets(MNIST_TRAINING_PATH, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hyper_params():\n",
    "    hypms = {}\n",
    "    hypms['n_epochs'] = 60\n",
    "    hypms['batch_size'] = 512\n",
    "    hypms['init_lr'] = 1e-3\n",
    "    hypms['raw_input_size'] = 28*28\n",
    "    hypms['in_dim'] = [28,28,1] # width,height,channels\n",
    "    hypms['fc_dropout'] = 0.5\n",
    "    hypms['n_classes'] = 10\n",
    "    hypms['filter_dim'] = 3 # default will be 3x3\n",
    "    return hypms\n",
    "hypms = create_hyper_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this isn't my favorite way to design the architecture, but defining\n",
    "# the architecture up front like this will save us from manually\n",
    "# inputing layer sizes later on.\n",
    "def create_layer_def():\n",
    "    global hypms\n",
    "    lyrdef = {}\n",
    "    lyrdef['conv_01_depth'] = 64\n",
    "    lyrdef['conv_02_depth'] = 128\n",
    "    lyrdef['conv_03_depth'] = 256\n",
    "    lyrdef['fc_01'] = 1024\n",
    "    lyrdef['fc_02'] = 56\n",
    "    lyrdef['output'] = hypms['n_classes']\n",
    "    return lyrdef\n",
    "lyrdef = create_layer_def()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph():\n",
    "    global hypms # hyper params\n",
    "    global lyrdef # layer definition params\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        with tf.name_scope(\"architecture\"):\n",
    "            with tf.name_scope(\"model\"):\n",
    "                # inputs\n",
    "                with tf.name_scope(\"inputs\"):\n",
    "                    X_raw = tf.placeholder(tf.float32, shape=[None, hypms['raw_input_size']])\n",
    "                    X = tf.reshape(X_raw, \n",
    "                                   shape=[-1, hypms['in_dim'][0],hypms['in_dim'][1],hypms['in_dim'][2]], \n",
    "                                   name=\"data\") # Input\n",
    "                    y = tf.placeholder(tf.float32, shape=[None, hypms['n_classes']], name=\"labels\") # Target\n",
    "\n",
    "                with tf.name_scope(\"conv_layers\"):\n",
    "                    with tf.name_scope(\"01\"):\n",
    "                        with tf.name_scope(\"conv\"):\n",
    "                            w_01 = tf.Variable(tf.random_normal([hypms['filter_dim'],\n",
    "                                                                 hypms['filter_dim'],\n",
    "                                                                 hypms['in_dim'][2],\n",
    "                                                                 lyrdef['conv_01_depth']]))\n",
    "                            b_01 = tf.Variable(tf.random_normal([lyrdef['conv_01_depth']]))\n",
    "                            conv_01 = tf.nn.conv2d(X, w_01, strides=[1,1,1,1], \n",
    "                                                   padding='SAME', name=\"convolution\")\n",
    "                            conv_01 = tf.nn.bias_add(conv_01, b_01, name=\"add_bias\")\n",
    "                            conv_01_out = tf.nn.elu(conv_01, name=\"activation\")\n",
    "                        with tf.name_scope(\"pool\"):\n",
    "                            l_01_out = tf.nn.max_pool(conv_01_out, \n",
    "                                                      ksize=[1,2,2,1], strides=[1,2,2,1], \n",
    "                                                      padding='SAME', name=\"max_pool\")\n",
    "\n",
    "                    with tf.name_scope(\"02\"):\n",
    "                        with tf.name_scope(\"conv\"):\n",
    "                            w_02 = tf.Variable(tf.random_normal([hypms['filter_dim'],\n",
    "                                                                 hypms['filter_dim'],\n",
    "                                                                 lyrdef['conv_01_depth'],\n",
    "                                                                 lyrdef['conv_02_depth']]))\n",
    "                            b_02 = tf.Variable(tf.random_normal([lyrdef['conv_02_depth']]))\n",
    "                            conv_02 = tf.nn.conv2d(l_01_out, w_02, strides=[1,1,1,1],\n",
    "                                                   padding='SAME', name=\"convolution\")\n",
    "                            conv_02 = tf.nn.bias_add(conv_02, b_02, name=\"add_bias\")\n",
    "                            conv_02_out = tf.nn.elu(conv_02, name=\"activation\")\n",
    "                        with tf.name_scope(\"pool\"):\n",
    "                            l_02_out = tf.nn.max_pool(conv_02_out, \n",
    "                                                      ksize=[1,2,2,1], strides=[1,2,2,1], \n",
    "                                                      padding='SAME', name=\"max_pool\")\n",
    "\n",
    "                    with tf.name_scope(\"03\"):\n",
    "                        with tf.name_scope(\"conv\"):\n",
    "                            w_03 = tf.Variable(tf.random_normal([hypms['filter_dim'],\n",
    "                                                                 hypms['filter_dim'],\n",
    "                                                                 lyrdef['conv_02_depth'],\n",
    "                                                                 lyrdef['conv_03_depth']]))\n",
    "                            b_03 = tf.Variable(tf.random_normal([lyrdef['conv_03_depth']]))\n",
    "                            conv_03 = tf.nn.conv2d(l_02_out, w_03, strides=[1,1,1,1],\n",
    "                                                   padding='SAME', name=\"convolution\")\n",
    "                            conv_03 = tf.nn.bias_add(conv_03, b_03, name=\"add_bias\")\n",
    "                            conv_03_out = tf.nn.elu(conv_03, name=\"activation\")\n",
    "                        with tf.name_scope(\"pool\"):\n",
    "                            l_03_out = tf.nn.max_pool(conv_03_out, \n",
    "                                                      ksize=[1,2,2,1], strides=[1,2,2,1], \n",
    "                                                      padding='SAME', name=\"max_pool\")\n",
    "\n",
    "\n",
    "                with tf.name_scope(\"fully_connected\"):\n",
    "                    # reshape to flatten\n",
    "                    last_shape = int(np.prod(l_03_out.get_shape()[1:]))\n",
    "                    flattened = tf.reshape(l_03_out, shape=[-1, last_shape])\n",
    "\n",
    "                    with tf.name_scope(\"fc1\"):\n",
    "                        w_fc1 = tf.Variable(tf.random_normal([last_shape, lyrdef['fc_01']]))\n",
    "                        b_fc1 = tf.Variable(tf.random_normal([lyrdef['fc_01']]))\n",
    "                        fc1 = tf.matmul(flattened, w_fc1, name=\"mult_weights\")\n",
    "                        fc1 = tf.add(fc1, b_fc1, name=\"add_bias\")\n",
    "                        fc1 = tf.nn.elu(fc1, name=\"activation\")\n",
    "\n",
    "                    with tf.name_scope(\"fc2\"):\n",
    "                        w_fc2 = tf.Variable(tf.random_normal([lyrdef['fc_01'], lyrdef['fc_02']]))\n",
    "                        b_fc2 = tf.Variable(tf.random_normal([lyrdef['fc_02']]))\n",
    "                        fc2 = tf.matmul(fc1, w_fc2, name=\"mult_weights\")\n",
    "                        fc2 = tf.add(fc2, b_fc2, name=\"add_bias\")\n",
    "                        fc2 = tf.nn.elu(fc2, name=\"activation\")\n",
    "\n",
    "                    with tf.name_scope(\"output\"):\n",
    "                        w_out = tf.Variable(tf.random_normal([lyrdef['fc_02'], lyrdef['output']]))\n",
    "                        b_out = tf.Variable(tf.random_normal([lyrdef['output']]))\n",
    "                        out = tf.matmul(fc2, w_out, name=\"mult_weights\")\n",
    "                        logits = tf.add(out, b_out, name=\"logits\")\n",
    "                        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "            with tf.name_scope(\"train\"):\n",
    "                xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "                batch_loss = tf.reduce_mean(xentropy)\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate=hypms['init_lr'])\n",
    "                training_op = optimizer.minimize(batch_loss)\n",
    "\n",
    "            with tf.name_scope(\"save_session\"):\n",
    "                init_global = tf.global_variables_initializer()\n",
    "                init_local = tf.local_variables_initializer()\n",
    "                saver = tf.train.Saver()\n",
    "\n",
    "            # Ops: training metrics\n",
    "            with tf.name_scope(\"metrics\"):\n",
    "                # ================================== performance\n",
    "                with tf.name_scope(\"common\"):\n",
    "                    y_true_cls = tf.argmax(y,1)\n",
    "                    y_pred_cls = tf.argmax(Y_proba,1)\n",
    "                    correct_prediction = tf.equal(y_pred_cls, y_true_cls, name=\"correct_predictions\")\n",
    "                    batch_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "                with tf.name_scope(\"train_metrics\") as scope:\n",
    "                    train_auc, train_auc_update = tf.metrics.auc(labels=y, predictions=Y_proba)\n",
    "                    train_acc, train_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                    train_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                    train_met_reset_op = tf.variables_initializer(train_acc_vars, name=\"train_met_reset_op\")\n",
    "                with tf.name_scope(\"val_metrics\") as scope:\n",
    "                    val_auc, val_auc_update = tf.metrics.auc(labels=y, predictions=Y_proba)\n",
    "                    val_acc, val_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                    val_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                    val_met_reset_op = tf.variables_initializer(val_acc_vars, name=\"val_met_reset_op\")\n",
    "                with tf.name_scope(\"test_metrics\") as scope:\n",
    "                    test_auc, test_auc_update = tf.metrics.auc(labels=y, predictions=Y_proba)\n",
    "                    test_acc, test_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                    test_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                    test_acc_reset_op = tf.variables_initializer(test_acc_vars, name=\"test_met_reset_op\")\n",
    "\n",
    "                # =============================================== loss \n",
    "                with tf.name_scope(\"train_loss_eval\") as scope:\n",
    "                    train_mean_loss, train_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                    train_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                    train_loss_reset_op = tf.variables_initializer(train_loss_vars, name=\"train_loss_reset_op\")\n",
    "                with tf.name_scope(\"val_loss_eval\") as scope:\n",
    "                    val_mean_loss, val_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                    val_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                    val_loss_reset_op = tf.variables_initializer(val_loss_vars, name=\"val_loss_reset_op\")\n",
    "                with tf.name_scope(\"test_loss_eval\")as scope:\n",
    "                    test_mean_loss, test_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                    test_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                    test_loss_reset_op = tf.variables_initializer(test_loss_vars, name=\"test_loss_rest_op\")\n",
    "\n",
    "            # --- create collections\n",
    "            for node in (saver, init_global, init_local):\n",
    "                g.add_to_collection(\"save_init\", node)\n",
    "            for node in (X_raw, y, training_op):\n",
    "                g.add_to_collection(\"main_ops\", node)\n",
    "            for node in (Y_proba, y_true_cls, y_pred_cls, correct_prediction):\n",
    "                g.add_to_collection(\"preds\", node)\n",
    "            for node in (train_auc, train_auc_update, train_acc, train_acc_update, train_met_reset_op):\n",
    "                g.add_to_collection(\"train_metrics\", node)\n",
    "            for node in (val_auc, val_auc_update, val_acc, val_acc_update, val_met_reset_op):\n",
    "                g.add_to_collection(\"val_metrics\", node)\n",
    "            for node in (test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op):\n",
    "                g.add_to_collection(\"test_metrics\", node)\n",
    "            for node in (train_mean_loss, train_mean_loss_update, train_loss_reset_op):\n",
    "                g.add_to_collection(\"train_loss\", node)\n",
    "            for node in (val_mean_loss, val_mean_loss_update, val_loss_reset_op):\n",
    "                g.add_to_collection(\"val_loss\", node)\n",
    "            for node in (test_mean_loss, test_mean_loss_update, test_loss_reset_op):\n",
    "                g.add_to_collection(\"test_loss\", node)\n",
    "            g.add_to_collection(\"logits\", logits)\n",
    "\n",
    "            # ===================================== tensorboard\n",
    "            with tf.name_scope(\"tensorboard_writer\") as scope:\n",
    "                epoch_train_loss_scalar = tf.summary.scalar('train_epoch_loss', train_mean_loss)\n",
    "                epoch_train_acc_scalar = tf.summary.scalar('train_epoch_acc', train_acc)\n",
    "                epoch_train_auc_scalar = tf.summary.scalar('train_epoch_auc', train_auc)\n",
    "                epoch_train_write_op = tf.summary.merge([epoch_train_loss_scalar, epoch_train_acc_scalar, epoch_train_auc_scalar], name=\"epoch_train_write_op\")\n",
    "\n",
    "                # ===== epoch, validation\n",
    "                epoch_validation_loss_scalar = tf.summary.scalar('validation_epoch_loss', val_mean_loss)\n",
    "                epoch_validation_acc_scalar = tf.summary.scalar('validation_epoch_acc', val_acc)\n",
    "                epoch_validation_auc_scalar = tf.summary.scalar('validation_epoch_auc', val_auc)\n",
    "                epoch_validation_write_op = tf.summary.merge([epoch_validation_loss_scalar, epoch_validation_acc_scalar, epoch_validation_auc_scalar], name=\"epoch_validation_write_op\")\n",
    "\n",
    "            for node in (epoch_train_write_op, epoch_validation_write_op):\n",
    "                g.add_to_collection(\"tensorboard\", node)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph(g, hypms):\n",
    "    global BEST_PARAMS_PATH\n",
    "    saver, init_global, init_local = g.get_collection(\"save_init\")\n",
    "    X_raw, y, training_op = g.get_collection(\"main_ops\")\n",
    "    preds, y_true_cls, y_pred_cls, _ = g.get_collection(\"preds\")\n",
    "    train_auc, train_auc_update, train_acc, train_acc_update, train_met_reset_op = g.get_collection(\"train_metrics\")\n",
    "    val_auc, val_auc_update, val_acc, val_acc_update, val_met_reset_op = g.get_collection(\"val_metrics\")\n",
    "    train_mean_loss, train_mean_loss_update, train_loss_reset_op = g.get_collection(\"train_loss\")\n",
    "    val_mean_loss, val_mean_loss_update, val_loss_reset_op = g.get_collection(\"val_loss\")\n",
    "    epoch_train_write_op, epoch_validation_write_op = g.get_collection(\"tensorboard\")\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(\"tf_logs\",\"train\"))\n",
    "    val_writer = tf.summary.FileWriter(os.path.join(\"tf_logs\",\"validation\"))\n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    with tf.Session(graph=g) as sess:\n",
    "        sess.run([init_global, init_local])\n",
    "        train_writer.add_summary\n",
    "        for e in tqdm(range(1,hypms['n_epochs']+1)):\n",
    "            sess.run([val_met_reset_op,val_loss_reset_op,train_met_reset_op,train_loss_reset_op])\n",
    "            n_batches = int(MNIST.train.num_examples/hypms['batch_size'])\n",
    "            for i in range(1,n_batches+1):\n",
    "                data, target = MNIST.train.next_batch(hypms['batch_size'])\n",
    "                sess.run([training_op, train_auc_update, train_acc_update, train_mean_loss_update], \n",
    "                         feed_dict={X_raw:data, y:target})\n",
    "\n",
    "            # write average for epoch + graph information\n",
    "            # run options is not working as expected -- my directory link must be broken\n",
    "            #run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            run_metadata = tf.RunMetadata()\n",
    "            summary = sess.run(epoch_train_write_op, run_metadata=run_metadata)\n",
    "            train_writer.add_run_metadata(run_metadata, str(e)) # not sure on this..\n",
    "            train_writer.add_graph(g)\n",
    "            train_writer.add_summary(summary, e)\n",
    "            train_writer.flush()\n",
    "\n",
    "            # run validation\n",
    "            n_batches = int(MNIST.validation.num_examples/hypms['batch_size'])\n",
    "            for i in range(1,n_batches+1):\n",
    "                Xb, yb = MNIST.validation.next_batch(hypms['batch_size'])\n",
    "                sess.run([val_auc_update, val_acc_update, val_mean_loss_update], \n",
    "                                  feed_dict={X_raw:Xb, y:yb})\n",
    "\n",
    "            # check for (and save) best validation params here\n",
    "            cur_loss, cur_acc = sess.run([val_mean_loss, val_acc])\n",
    "            if cur_loss < best_val_loss:\n",
    "                best_val_loss = cur_loss\n",
    "                best_params = get_model_params()\n",
    "                save_obj(best_params, BEST_PARAMS_PATH)\n",
    "                print(\"best params saved: val acc: {:.3f}% val loss: {:.4f}\".format(cur_acc*100, cur_loss))\n",
    "            \n",
    "            summary = sess.run(epoch_validation_write_op) \n",
    "            val_writer.add_summary(summary, e)\n",
    "            val_writer.flush()\n",
    "\n",
    "        # close writers\n",
    "        train_writer.close()\n",
    "        val_writer.close()\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/60 [00:05<05:15,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 88.954% val loss: 93217.7578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 2/60 [00:09<04:41,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 91.688% val loss: 68134.9531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 3/60 [00:14<04:27,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 94.293% val loss: 47134.1367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 4/60 [00:18<04:19,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 94.987% val loss: 35515.1172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 5/60 [00:22<04:09,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 96.332% val loss: 24741.5586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 9/60 [00:39<03:45,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 96.441% val loss: 23092.1953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 12/60 [00:52<03:29,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 97.092% val loss: 19016.3672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 13/60 [00:56<03:24,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 97.374% val loss: 18676.9043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|██▎       | 14/60 [01:01<03:20,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 97.157% val loss: 18137.5508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 16/60 [01:09<03:11,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 97.439% val loss: 17526.5527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 18/60 [01:18<03:02,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 97.352% val loss: 16235.7363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 20/60 [01:26<02:53,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 97.765% val loss: 15813.6162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 22/60 [01:35<02:44,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 97.765% val loss: 15383.5576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 29/60 [02:04<02:12,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 98.090% val loss: 14502.6924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 30/60 [02:08<02:08,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 98.155% val loss: 13306.4648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 32/60 [02:16<01:59,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 98.025% val loss: 12348.7129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 36/60 [02:33<01:42,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 98.177% val loss: 12337.7246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 45/60 [03:09<01:03,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 98.351% val loss: 10849.0820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [04:11<00:00,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 98.698% val loss: 10149.4072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "hypms = create_hyper_params()\n",
    "g = build_graph()\n",
    "sess = train_graph(g, hypms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 99.172% acc: 98.509% loss: 10754.31543\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "hypms = create_hyper_params()\n",
    "g_eval = build_graph()\n",
    "best_params = load_obj(BEST_PARAMS_PATH)\n",
    "with tf.Session(graph=g_eval) as sess:\n",
    "    saver, init_global, init_local = g_eval.get_collection(\"save_init\")\n",
    "    X_raw, y, training_op = g_eval.get_collection(\"main_ops\")\n",
    "    Y_proba, y_true_cls, y_pred_cls, _ = g_eval.get_collection(\"preds\")\n",
    "    test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op = g_eval.get_collection(\"test_metrics\")\n",
    "    test_mean_loss, test_mean_loss_update, test_loss_reset_op = g_eval.get_collection(\"test_loss\")\n",
    "    \n",
    "    restore_model_params(model_params=best_params, g=g_eval, sess=sess)\n",
    "    sess.run([test_acc_reset_op, test_loss_reset_op])\n",
    "\n",
    "    # run test\n",
    "    n_batches = int(MNIST.test.num_examples/hypms['batch_size'])\n",
    "    for i in range(1,n_batches+1):\n",
    "        Xb, yb = MNIST.test.next_batch(hypms['batch_size'])\n",
    "        sess.run([test_auc_update, test_acc_update, test_mean_loss_update], \n",
    "                     feed_dict={X_raw:Xb, y:yb})      \n",
    "            \n",
    "    # print\n",
    "    final_test_acc, final_test_loss, final_test_auc = sess.run([test_acc, test_mean_loss, test_auc])\n",
    "    print(\"test auc: {:.3f}% acc: {:.3f}% loss: {:.5f}\".format(final_test_auc*100, \n",
    "                                                              final_test_acc*100,\n",
    "                                                              final_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add dropout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_edge",
   "language": "python",
   "name": "tf_edge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
