{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Gradients\n",
    "\n",
    "### Resources:\n",
    "Papers\n",
    "> [Decoupled Neural Interfaces using Synthetic Gradients, Max Jaderberg et al., 2016](https://arxiv.org/abs/1608.05343)\n",
    "> [Understanding Synthetic Gradients and Decoupled Neural Interfaces, Wojciech Marian Czarnecki et al., 2017](https://arxiv.org/abs/1703.00522)\n",
    "\n",
    "Youtube\n",
    "> [Synthetic Gradients Tutorial by Aurélien Géron](https://youtu.be/1z_Gv98-mkQ)\n",
    "\n",
    "Github\n",
    "> [github; jupyter notebook by Nitarshan Rajkumar](https://github.com/nitarshan/decoupled-neural-interfaces/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: (3, 5, 4, 'final', 0)\n",
      "TensorFlow: 1.6.0-dev20180105\n",
      "No GPU\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this is a custom cell that contains the common imports I personally \n",
    "# use these may/may not be necessary for the following examples\n",
    "\n",
    "# DL framework\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# common packages\n",
    "import numpy as np\n",
    "import os # handling file i/o\n",
    "import sys\n",
    "import math\n",
    "import time # timing epochs\n",
    "import random\n",
    "\n",
    "# for ordered dict when building layer components\n",
    "import collections\n",
    "\n",
    "# plotting pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import colors # making colors consistent\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable # colorbar helper\n",
    "\n",
    "\n",
    "from imageio import imread # read image from disk\n",
    "# + data augmentation\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "\n",
    "\n",
    "import pickle # manually saving best params\n",
    "from sklearn.utils import shuffle # shuffling data batches\n",
    "from tqdm import tqdm # display training progress bar\n",
    "\n",
    "# const\n",
    "SEED = 42\n",
    "\n",
    "# Helper to make the output consistent\n",
    "def reset_graph(seed=SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# helper to create dirs if they don't already exist\n",
    "def maybe_create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print(\"{} created\".format(dir_path))\n",
    "    else:\n",
    "        print(\"{} already exists\".format(dir_path))\n",
    "    \n",
    "def make_standard_dirs(saver=True, best_params=True, tf_logs=True):\n",
    "    # `saver/` will hold tf saver files\n",
    "    maybe_create_dir(\"saver\")\n",
    "    # `best_params/` will hold a serialized version of the best params\n",
    "    # I like to keep this as a backup in case I run into issues with\n",
    "    # the saver files\n",
    "    maybe_create_dir(\"best_params\")\n",
    "    # `tf_logs/` will hold the logs that will be visable in tensorboard\n",
    "    maybe_create_dir(\"tf_logs\")\n",
    "\n",
    "    \n",
    "# set tf log level to supress messages, unless an error\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Important Version information\n",
    "print(\"Python: {}\".format(sys.version_info[:]))\n",
    "print('TensorFlow: {}'.format(tf.__version__))\n",
    "\n",
    "# Check if using GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "    \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saver already exists\n",
      "best_params already exists\n",
      "tf_logs already exists\n"
     ]
    }
   ],
   "source": [
    "make_standard_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these two functions (get_model_params and restore_model_params) are \n",
    "# ad[a|o]pted from; \n",
    "# https://github.com/ageron/handson-ml/blob/master/11_deep_learning.ipynb\n",
    "def get_model_params():\n",
    "    global_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    return {global_vars.op.name: value for global_vars, value in \n",
    "            zip(global_vars, tf.get_default_session().run(global_vars))}\n",
    "\n",
    "def restore_model_params(model_params, g, sess):\n",
    "    gvar_names = list(model_params.keys())\n",
    "    assign_ops = {gvar_name: g.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                  for gvar_name in gvar_names}\n",
    "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "# these two functions are used to manually save the best\n",
    "# model params to disk\n",
    "def save_obj(obj, name):\n",
    "    with open('best_params/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('best_params/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# `!conda install -c conda-forge tqdm`\n",
    "# from tqdm import tqdm # Used to display training progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t10k-images-idx3-ubyte.gz\n",
      "t10k-labels-idx1-ubyte.gz\n",
      "train-images-idx3-ubyte.gz\n",
      "train-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "ROOT_DATA = \"../../ROOT_DATA/\"\n",
    "DATA_DIR = \"mnist_data\"\n",
    "\n",
    "MNIST_TRAINING_PATH = os.path.join(ROOT_DATA, DATA_DIR)\n",
    "# ensure we have the correct directory\n",
    "for _, _, files in os.walk(MNIST_TRAINING_PATH):\n",
    "    files = sorted(files)\n",
    "    for filename in files:\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../ROOT_DATA/mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../ROOT_DATA/mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../ROOT_DATA/mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../ROOT_DATA/mnist_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "MNIST = input_data.read_data_sets(MNIST_TRAINING_PATH, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image dimensions (GLOBAL) - [MG_WIDTH x IMG_HEIGHT, CHANNELS]\n",
    "# SQUARE_DIM = 299\n",
    "# if SQUARE_DIM:\n",
    "#     IMG_WIDTH = SQUARE_DIM\n",
    "#     IMG_HEIGHT = SQUARE_DIM\n",
    "# CHANNELS = 3\n",
    "    \n",
    "# ROOT_DIR = \"../../dataset/record_holder\"\n",
    "# # ensure we have the correct directory\n",
    "# for _, _, files in os.walk(ROOT_DIR):\n",
    "#     files = sorted(files)\n",
    "#     for filename in files:\n",
    "#         print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read tf records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GLOBAL_SET_TYPE = None\n",
    "\n",
    "# def _parse_function(example_proto):\n",
    "#     global GLOBAL_SET_TYPE\n",
    "#     labelName = str(GLOBAL_SET_TYPE) + '/label'\n",
    "#     featureName = str(GLOBAL_SET_TYPE) + '/image'\n",
    "#     feature = {featureName: tf.FixedLenFeature([], tf.string),\n",
    "#                labelName: tf.FixedLenFeature([], tf.int64)}\n",
    "    \n",
    "#     # decode\n",
    "#     parsed_features = tf.parse_single_example(example_proto, features=feature)\n",
    "    \n",
    "#     # convert image data from string to number\n",
    "#     image = tf.decode_raw(parsed_features[featureName], tf.float32)\n",
    "#     image = tf.reshape(image, [IMG_WIDTH, IMG_HEIGHT, CHANNELS])\n",
    "#     label = tf.cast(parsed_features[labelName], tf.int64)\n",
    "    \n",
    "#     # [do any preprocessing here]\n",
    "    \n",
    "#     return image, label\n",
    "\n",
    "# def return_batched_iter(setType, data_params, sess):\n",
    "#     global GLOBAL_SET_TYPE\n",
    "#     GLOBAL_SET_TYPE = setType\n",
    "    \n",
    "#     filenames_ph = tf.placeholder(tf.string, shape=[None])\n",
    "\n",
    "#     dataset = tf.data.TFRecordDataset(filenames_ph)\n",
    "#     dataset = dataset.map(_parse_function)  # Parse the record into tensors.\n",
    "#     dataset = dataset.shuffle(buffer_size=data_params['buffer_size'])\n",
    "#     dataset = dataset.batch(data_params['batch_size'])\n",
    "#     dataset = dataset.repeat(data_params['n_epochs'])\n",
    "    \n",
    "#     iterator = dataset.make_initializable_iterator()\n",
    "    \n",
    "#     tfrecords_file_name = str(GLOBAL_SET_TYPE) + '.tfrecords'\n",
    "#     tfrecord_file_path = os.path.join(FINAL_DIR, tfrecords_file_name)\n",
    "    \n",
    "#     # initialize\n",
    "#     sess.run(iterator.initializer, feed_dict={filenames_ph: [tfrecord_file_path]})\n",
    "    \n",
    "#     return iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_hyper_params():\n",
    "    data_params = {}\n",
    "    data_params['n_epochs'] = 5\n",
    "    data_params['batch_size'] = 16\n",
    "    data_params['buffer_size'] = 128 # for shuffling\n",
    "\n",
    "    data_params['init_lr'] = 1e-5\n",
    "    #data_params['lr_div'] = 10\n",
    "    #lr_low = int(data_params['n_epochs'] * 0.6)\n",
    "    #lr_high = int(data_params['n_epochs'] * 0.8)\n",
    "    #data_params['lr_div_steps'] = set([lr_low, lr_high])\n",
    "\n",
    "    data_params['update_prob'] = 0.2 # Probability of updating a decoupled layer\n",
    "    \n",
    "    return data_params\n",
    "\n",
    "validation_checkpoint = 1 # How often (epochs) to validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_params = create_hyper_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helpers for creating layers\n",
    "def dense_layer(inputs, units, name, output=False):\n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.layers.dense(inputs, units, name=\"fc\")\n",
    "        if not output:\n",
    "            x = tf.layers.batch_normalization(x, name=\"bn\")\n",
    "            x = tf.nn.relu(x, name=\"activation_relu\")\n",
    "    return x\n",
    "\n",
    "def sg_module(inputs, units, name, label):\n",
    "    with tf.variable_scope(name):\n",
    "        inputs_c = tf.concat([inputs, label], 1)\n",
    "        x = tf.layers.dense(inputs_c, units, name=\"fc\", kernel_initializer=tf.zeros_initializer())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "syntgrad_sess = tf.Session()\n",
    "backprop_sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network architecture\n",
    "n_outputs = 10\n",
    "with tf.variable_scope(\"architecture\"):\n",
    "    # inputs\n",
    "    with tf.variable_scope(\"inputs\"):\n",
    "        #X = tf.placeholder(tf.float32, shape=[None, IMG_HEIGHT, IMG_WIDTH, CHANNELS], name=\"X\")\n",
    "        # labels\n",
    "        #y_raw = tf.placeholder(tf.int64, shape=[None, n_outputs], name=\"y_input\")\n",
    "        #y_ = tf.cast(y_raw, tf.float32)\n",
    "        X = tf.placeholder(tf.float32, shape=(None, 784), name=\"data\") # Input\n",
    "        y = tf.placeholder(tf.float32, shape=(None, n_outputs), name=\"labels\") # Target\n",
    "        \n",
    "    # Inference Layers\n",
    "    h_1 = dense_layer(X, 256, \"layer_01\")\n",
    "    h_2 = dense_layer(h_1, 256, \"layer_02\")\n",
    "    h_3 = dense_layer(h_2, 256, \"layer_03\")\n",
    "    logits = dense_layer(h_3, n_outputs, name=\"layer_04\", output=True)\n",
    "    \n",
    "    # Synthetic Gradient Layers\n",
    "    sg_1 = sg_module(h_1, 256, \"sg_02\", y)\n",
    "    sg_2 = sg_module(h_2, 256, \"sg_03\", y)\n",
    "    sg_3 = sg_module(h_3, 256, \"sg_04\", y)\n",
    "    \n",
    "# collections of trainable variables in each block\n",
    "layer_vars = [tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer_01/\"),\n",
    "              tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer_02/\"),\n",
    "              tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer_03/\"),\n",
    "              tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer_04/\")]\n",
    "sg_vars = [None,\n",
    "           tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/sg_02/\"),\n",
    "           tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/sg_03/\"),\n",
    "           tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/sg_04/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize layer and the sythetic gradient module\n",
    "def train_layer_n(n, h_m, h_n, sg_m, class_loss, d_n=None):\n",
    "    with tf.variable_scope(\"layer\"+str(n)):\n",
    "        layer_grads = tf.gradients(h_n, [h_m]+layer_vars[n-1], d_n)\n",
    "        layer_gv = list(zip(layer_grads[1:], layer_vars[n-1]))\n",
    "        layer_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).apply_gradients(layer_gv)\n",
    "    with tf.variable_scope(\"sg\"+str(n)):\n",
    "        d_m = layer_grads[0]\n",
    "        sg_loss = tf.divide(tf.losses.mean_squared_error(sg_m, d_m), class_loss)\n",
    "        sg_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(sg_loss, var_list=sg_vars[n-1])\n",
    "    return layer_opt, sg_opt\n",
    "\n",
    "\n",
    "# Ops: training\n",
    "with tf.variable_scope(\"train\"):\n",
    "    with tf.variable_scope(\"learning_rate\"):\n",
    "        learning_rate = tf.Variable(data_params['init_lr'], dtype=tf.float32, name=\"lr\")\n",
    "        #reduce_lr = tf.assign(learning_Rate, learning_rate/lr_div, name=\"lr_decrease\")\n",
    "        \n",
    "    pred_loss = tf.losses.softmax_cross_entropy(onehot_labels=y, logits=logits, scope=\"prediction_loss\")\n",
    "    \n",
    "    # Optimizers when using synthetic gradients\n",
    "    with tf.variable_scope(\"synthetic\"):\n",
    "        layer4_opt, sg4_opt = train_layer_n(4, h_3, pred_loss, sg_3, pred_loss)\n",
    "        layer3_opt, sg3_opt = train_layer_n(3, h_2, h_3, sg_2, pred_loss, sg_3)\n",
    "        layer2_opt, sg2_opt = train_layer_n(2, h_1, h_2, sg_1, pred_loss, sg_2)\n",
    "        with tf.variable_scope(\"layer1\"):\n",
    "            layer1_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(h_1, var_list=layer_vars[0], grad_loss=sg_1)\n",
    "        \n",
    "    with tf.variable_scope(\"backprop\"):\n",
    "        backprop_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(pred_loss)\n",
    "        \n",
    "init_global = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ops: validation+testing\n",
    "with tf.variable_scope(\"test\"):\n",
    "    preds = tf.nn.softmax(logits, name=\"predictions\")\n",
    "    correct_preds = tf.equal(tf.argmax(preds,1), tf.argmax(y,1), name=\"correct_predictions\")\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32), name=\"correct_prediction_count\") / data_params['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ops: tensorboard\n",
    "with tf.variable_scope(\"summary\"):\n",
    "    cost_summary_opt = tf.summary.scalar(\"loss\", pred_loss)\n",
    "    accuracy_summary_opt = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop (locked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 53.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# backprop\n",
    "with backprop_sess.as_default():\n",
    "    backprop_train_path = os.path.join(\"tf_logs\",\"backprop\",\"train\")\n",
    "    backprop_train_writer = tf.summary.FileWriter(backprop_train_path)\n",
    "    backprop_validation_path = os.path.join(\"tf_logs\",\"backprop\",\"validation\")\n",
    "    backprop_validation_writer = tf.summary.FileWriter(backprop_validation_path)\n",
    "    \n",
    "    backprop_sess.run(init_global)\n",
    "    for i in tqdm(range(1,data_params['n_epochs']+1)):\n",
    "        data, target = MNIST.train.next_batch(data_params['batch_size'])\n",
    "        _, summary = backprop_sess.run([backprop_opt, summary_op], feed_dict={X:data, y:target})\n",
    "        backprop_train_writer.add_summary(summary, i)\n",
    "        \n",
    "        # run validation\n",
    "        # TODO: update this to validation\n",
    "        Xb, yb = MNIST.test.next_batch(data_params['batch_size'])\n",
    "        summary = backprop_sess.run([summary_op], feed_dict={X:Xb, y:yb})[0]\n",
    "        backprop_validation_writer.add_summary(summary, i)\n",
    "        \n",
    "    # close writers\n",
    "    backprop_train_writer.close()\n",
    "    backprop_validation_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 26.72it/s]\n"
     ]
    }
   ],
   "source": [
    "with syntgrad_sess.as_default():\n",
    "    sg_train_path = os.path.join(\"tf_logs\",\"sg\",\"train\")\n",
    "    sg_train_writer = tf.summary.FileWriter(sg_train_path, syntgrad_sess.graph)\n",
    "    sg_validation_path = os.path.join(\"tf_logs\",\"sg\",\"train\")\n",
    "    sg_validation_writer = tf.summary.FileWriter(sg_validation_path)\n",
    "    \n",
    "    syntgrad_sess.run(init_global)\n",
    "    for i in tqdm(range(1,data_params['n_epochs']+1)):\n",
    "        data, target = MNIST.train.next_batch(data_params['batch_size'])\n",
    "        \n",
    "        # The layers here could be independently updated (data parallism) - device placement\n",
    "        # > stochastic updates are possible\n",
    "        if random.random() <= data_params['update_prob']:\n",
    "            syntgrad_sess.run([layer1_opt], feed_dict={X:data, y:target})\n",
    "        if random.random() <= data_params['update_prob']:\n",
    "            syntgrad_sess.run([layer2_opt, sg2_opt], feed_dict={X:data, y:target})\n",
    "        if random.random() <= data_params['update_prob']:\n",
    "            syntgrad_sess.run([layer3_opt, sg3_opt], feed_dict={X:data, y:target})\n",
    "        if random.random() <= data_params['update_prob']:\n",
    "            _, _, summary = syntgrad_sess.run([layer4_opt, sg4_opt, summary_op], feed_dict={X:data, y:target})\n",
    "            sg_train_writer.add_summary(summary, i)\n",
    "            \n",
    "        # validation\n",
    "        # TODO: convert to validation\n",
    "        Xb, yb = MNIST.test.next_batch(data_params['batch_size'])\n",
    "        summary = syntgrad_sess.run([summary_op], feed_dict={X:Xb, y:yb})[0]\n",
    "        sg_validation_writer.add_summary(summary, i)\n",
    "        \n",
    "    sg_train_writer.close()\n",
    "    sg_validation_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.3280\n",
      "acc: 0.0928\n"
     ]
    }
   ],
   "source": [
    "# Test using backprop\n",
    "with backprop_sess.as_default():\n",
    "    n_batches = int(MNIST.test.num_examples/data_params['batch_size'])\n",
    "    test_accuracy = 0\n",
    "    test_loss = 0\n",
    "    for _ in range(n_batches):\n",
    "        Xb, yb = MNIST.test.next_batch(data_params['batch_size'])\n",
    "        batch_accuracy, batch_loss = backprop_sess.run([accuracy, pred_loss], feed_dict={X:Xb,y:yb})\n",
    "        test_accuracy += batch_accuracy\n",
    "        test_loss += batch_loss\n",
    "    print(\"loss: {:.4f}\".format(test_loss/n_batches))\n",
    "    print(\"acc: {:.4f}\".format(test_accuracy/n_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.3293\n",
      "acc: 0.0914\n"
     ]
    }
   ],
   "source": [
    "# Test using backprop\n",
    "with syntgrad_sess.as_default():\n",
    "    n_batches = int(MNIST.test.num_examples/data_params['batch_size'])\n",
    "    test_accuracy = 0\n",
    "    test_loss = 0\n",
    "    for _ in range(n_batches):\n",
    "        Xb, yb = MNIST.test.next_batch(data_params['batch_size'])\n",
    "        batch_accuracy, batch_loss = backprop_sess.run([accuracy, pred_loss], feed_dict={X:Xb,y:yb})\n",
    "        test_accuracy += batch_accuracy\n",
    "        test_loss += batch_loss\n",
    "    print(\"loss: {:.4f}\".format(test_loss/n_batches))\n",
    "    print(\"acc: {:.4f}\".format(test_accuracy/n_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "backprop_sess.close()\n",
    "syntgrad_sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu_tf",
   "language": "python",
   "name": "cpu_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
