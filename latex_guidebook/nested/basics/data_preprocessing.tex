\section{Feature Engineering: Data Pre-processing}

\textcolor{blue}{Data is rarely obtained in a form that is necessary for optimal performance of a learning algorithm. Data can be missing, can contain a mix of categorical and quantitative, can contain values on vastly different scales, etc.}

\textcolor{blue}{Building a good representation of your data -- feature extraction/engineering}

\textcolor{blue}{It is important to note that any parameters related to data pre-processing, such as feature scaling and dimensionality reduction, are obtained solely from observing the training set. The parameters for these methods obtained on the training set are then later applied to the test set. This is important since if these preprocessing parameters were obtained on the entire dataset and included the test set, the the model performance may be overoptimistic since then when applying the methods to the unseen data.}

\textcolor{blue}{make sure to use data that is relevant to the objective/problem.}

% TODO: case study example

\textcolor{blue}{make sure the data can be known at prediction time. e.g. will there be a delay between activity and being able to access that data at the data repository?}

% TODO: case study example

% TODO: expand
\textcolor{blue}{features should be numeric and have a meaningful magnitue (see below)}

% TODO: case study example

\textcolor{blue}{make sure ``enough'' examples exist for the particular feature}

\textcolor{blue}{Don't mix magic values with values you already have. For example, if we have ratings 0-10, don't include a new column for rated vs not rated.}

\subsection{Handling Missing Data}

\subsubsection{Filtering Out}

\textcolor{blue}{Simply removing any entries that are missing data. This is convenient and easy but may not be practical -- any time data is being removed, potentially useful information is lost and too much data may be removed.}

\textcolor{green}{TODO: Code in jupyter on how to do this with pandas and dropna -- key params - how, thresh, subset}

\subsubsection{Filling In}

\textcolor{blue}{Estimating the missing data}

\subsection{Handling Categorical Data}

\subsubsection{Encoding}

\textcolor{blue}{It may seem intuitive to represent categorical data with an integer value. However, this may not always be best. Let's pretend we're representing United States with arbitrary integer values and we assign values alphabetically; Alabama: 0, Alaska: 1, Arizona: 2, Arkansas: 3, California: 4, ..., Wisconsin: 48, Wyoming: 49. So far so good. \textcolor{red}{However, these values indirectly imply a relationship (that may not actually exist)} -- and imply that some states are more similar to one another than others (for instance, this encoding seemingly indicates that Alabama as most related to Alaska.) }

\textcolor{blue}{{one-hot encoding}\index{one-hot encoding} or one-of-k encoding\index{one-of-k encoding} is a method of encoding which represents each explanatory variable as a binary feature}

\textcolor{blue}{one-hot encoding reduces the relationship}

\textcolor{green}{TODO: show example of one hot encoding}

\textcolor{blue}{will lead to {sparse vectors}\index{sparse vectors} -- high dimensional vectors. This is memory intensive (some libraries have methods to address this --- SciPy and Pandas).}

\subsection{Feature Scaling, Normalization}

% TODO: this section needs to be restructured to present ideas in a logical flow/building on one another and the cost(time)/benefit

\textcolor{blue}{Ensuring variables exist on a similar scale and variance is important. If one variable is orders of magnitude larger than others, the variable may dominate the learning algorithm and prevent influence from the other variables.}

\textcolor{blue}{Generally would like to get each feature into the $-1 \le X_i \le 1$ or the $-0.5 \le X_i le 0.5$ range.}

\textcolor{blue}{"OK" if not exact, different people have different rules of thumb for when it is necessary to scale a feature into this range.}

\textcolor{blue}{Additionally, some learning algorithms converge more slowly.}

% hard clipping/capping

% log scalling -- good for when the data has a huge range

% TODO: Give an example


\subsubsection{Mean Normalization}

\textcolor{blue}{feature $x$ is replaced by $x - \mu$ to create a zero mean}

% TODO: 2D figure showing a circle vs oval contour plot and how gradient descent may take longer on the "oval"

\subsubsection{Min-Max scaling (Normalization)}

\textcolor{blue}{values are shifted and rescaled so they end up on a [0,1] range}

\subsubsection{Standardization}

\textcolor{blue}{(Eq.~\ref{eq:preprocess_standardization}) first, subtract the sample mean, then divide by standard deviation variance}

\textcolor{blue}{pros: unlike min-max, not bound to specific range}

\textcolor{blue}{standardized values always have a zero mean and unit variance, a standard deviation of 1.}

\textcolor{blue}{gives our data the property of a standard normal distribution}

\begin{equation}
{X' = \frac{X - \mu}{\sigma}}
\label{eq:preprocess_standardization}
\end{equation}

\textcolor{green}{TODO: create code sample - numpy, and sklearn methods}

\paragraph{Robust Scaler}

\textcolor{blue}{In order to reduce the effect of large outliers, a \textcolor{blue}{RobustScalar} may be used. Rather than subtract the mean and divide by the standard deviation, the median is subtracted and then the data is divided by the {interquartile range}\index{interquartile range} -- \textcolor{red}{see local ref?}}


\subsection{Others}

\subsubsection{Removing Duplicates}

\subsubsection{Outliers}

\textcolor{blue}{rather than remove outliers, the values may be capped.}
% TODO: example, in the California housing data, there are datapoints with 50 rooms per house

\subsubsection{Discretization and Binning}

\textcolor{blue}{An example for this may be housing prices and latitude.}

% ? tf.feature_column.bucketized_column


\textcolor{blue}{rather than store a floating point, bins could be created. would like to keep the information (latitude may be useful, but the magnitude here is not useful (even if normalized) and so binning may be a great option)}

% TODO: figure for latitude (housing prices) and values pre/post binning

\subsection{Where to do preprocessing}

\textcolor{blue}{1. at execution time/with the TF graph}
\textcolor{blue}{2. apache beam ``in front'' of the graph (can use time windows)}
\textcolor{blue}{3. }



\textcolor{blue}{Batch vs Streaming}

% calculating rolling average