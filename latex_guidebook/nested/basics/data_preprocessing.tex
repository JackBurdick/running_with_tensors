\section{Data Pre-processing}

\textcolor{blue}{Data is rarely obtained in a form that is necessary for optimal performance of a learning algorithm. Data can be missing, can contain a mix of categorical and quantitative, can contain values on vastly different scales, etc.}

\textcolor{blue}{It is important to note that any parameters related to data pre-processing, such as feature scaling and dimensionality reduction, are obtained solely from observing the training set. The parameters for these methods obtained on the training set are then later applied to the test set. This is important since if these preprocessing parameters were obtained on the entire dataset and included the test set, the the model performance may be overoptimistic since then when applying the methods to the unseen data the parameters will not be related to the data as it was before and thus performance could suffer.}

\subsection{Handling Missing Data}

\subsubsection{Filtering Out}

\subsubsection{Filling In}

\subsection{Handling Categorical Data}

\subsubsection{Encoding}

\subsection{Feature Scaling, Normalization}

\subsubsection{Min-Max scaling (Normalization)}

\textcolor{blue}{values are shifted and rescaled so they end up on a [0,1] range}

\subsubsection{Standardization}

\textcolor{blue}{(Eq.~\ref{eq:preprocess_standardization}) first, subtract the sample mean, then divide by standard deviation variance}

\textcolor{blue}{pros: unlike min-max, not bound to specific range}

\textcolor{blue}{standardized values always have a zero mean and a standard deviation of 1.}

\textcolor{blue}{gives our data the property of a standard normal distribution}

\begin{equation}
{X' = \frac{X - \mu}{\sigma}}
\label{eq:preprocess_standardization}
\end{equation}

\textcolor{green}{TODO: create code sample - numpy, and sklearn methods}


\subsection{Others}

\subsubsection{Removing Duplicates}

\subsubsection{Outliers}

\subsubsection{Discretization and Binning}
