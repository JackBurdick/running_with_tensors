\section{Hyper-Parameters}

\subsection{Parameters: "tuning knobs"}

\subsubsection{Learning Rate}

\textcolor{green}{TODO: Learning rate overview}

% TODO: Learning rate practical advice

% TODO: figure showing cost vs iteration for a LR that is too small, just right, and too large

% TODO: Learning rate figure showing how if the learning rate is too high, you'll likely see the cost diverage when plotted vs iterations

\r{In general, if the LR is too small, convergence (with something like gradient descent) may be slow.  If LR is too large, then convergence may not occur and the reduction in error may oscillate wildly or may even diverge.}

\paragraph{research}

% TODO: this section may not belong here - may belong in an "advanced section"

%%%% learning rates
\textcolor{blue}{cyclic learning rate~\cite{smith2017cyclical}}

\textcolor{blue}{sgdr: stochastic gradient descent with restarts~\cite{loshchilov2016sgdr}. The learning rate is decreased from the max value along a curve (cosine, shown in Eq.\ref{eq:sgdr_def}, where $n_{max}^i$ and $n_{min}^i$ are ranges for the learning rate, $T_i$ represents epochs, $T_{cur}$ is how many epochs have been performed since the last restart). The authors also suggest making each next cycle longer than the previous cycle by a constant $T_mul$ may be beneficial.}

\begin{equation}
{n_t = n_{min}^i + 1/2(n_{max}^i - n_{min}^i)(1 + cos(\frac{T_{cur}}{T_i}\pi))}
\label{eq:sgdr_def}
\end{equation}

\subsubsection{Batch size}

\textcolor{green}{TODO: batch size overview}

\textcolor{blue}{optimal batch size is problem dependent}

\textcolor{blue}{TODO: notebook and plots showing how the smoothness is affected when comparing batch sizes of 1 vs 10 vs 20 etc.}

% related to shuffling - the gradients are computed on a batch and so a batch should be representative of the data

%%%%% small batch size
\textcolor{blue}{small minibatch sizes (between 2 and 32) may be better than large batch sizes~\cite{masters2018revisiting}.}

\textcolor{blue}{``generalization gap'' may not be due to large mini-batches, rather, due to the number of updates made to the system~\cite{hoffer2017train}}

%%%% minibatch
\textcolor{blue}{Batch training is almost always slower to converge than on-line/mini-batch training, which is likely due to the fact that on-line/mini-batches learning will follow the error surface, allowing for larger learning rates, and thus faster convergence~\cite{wilson2003general}.}

% incrementing batchsize over time
\textcolor{blue}{Increasing the batch size may achieve similar benefits to decaying the learning rate ~\cite{smith2017don} -- which could lead to use of larger batch sizes, reducing the number of parameter updates and therefore reducing training time.}

\subsection{Hyper-Parameter Optimization}

\subsubsection{Grid Search}

\textcolor{blue}{{Grid search}\index{Grid search} Exhaustive search that trains+evaluates a model for each combination of specified hyperparameter configurations and combinations defined by a Cartesian product of the sets of possible values for each hyperparameter.}

\subsubsection{Randomized Search}

\textcolor{blue}{{Randomized search}\index{Randomized search} }

\textcolor{green}{TODO: figure demonstrating difference between grid and randomized search}

\subsubsection{Other Methods}

\textcolor{blue}{See \textcolor{red}{local ref? --- advanced methods and research}}