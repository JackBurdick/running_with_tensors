\section{Optimization}
\label{subsec:optimization}

\TD{This may need it's own chapter!}

\r{Estimate the values of the model's parameters that minimize the value of the cost function based on the data it observes}

\r{"turning a loss function into a search strategy"}

% this may belong elsewhere
% alternatives to gradient descent 
% conjugate gradient
% BFGS
% L-BFGS
% pro: faster, don't need to pick the LR con: more complex
% line search algorithm

\TD{Error surface definition} --- \r{the error surface may include flat region which, in high-dimensional spaces is considered a saddle point.}

\r{Simply stepping in the direction of the steepest descent for a given location (or batch) is not always the best strategy for convergence. \TD{a figure of this would be nice}}

\subsection{First-order}

\subsubsection{Gradient Descent}

\r{Gradient Descent --- overview --- optimization algorithm that can be used to estimate the local minimum of a function}

\r{Iteratively updates the model parameters by calculating the partial derivatives of the cost function at each step during training}

\r{Gradient descent is only guaranteed to find the local minimum of the cost function.}

\r{simultaneous update.}

\TD{First Order (\ALR), Second Order (\ALR) optimizers. Second-order approximations are based on the Hessian (\ALR) or the objective function and are capable of informing not only the direction to step in, but also the step size.}



\paragraph{Batch Gradient Descent}

\r{batch gradient descent --- taking a step (update the weights) opposite (down) the gradient calculated from the entire training set}

\r{Batch gradient descent is deterministic --- will produce the same paramter values if the same dataset is used multiple times.}

\r{single static error surface}


\paragraph{Stochastic Gradient Descent}

\r{Stochastic Gradient Descent (sometimes called iterative or on-line gradient descent) --- rather than update the weights based on the sum of the accumulated errors, the weights are updated for each training sample}

\r{Stochastic gradient descent is deterministic --- may produce the different parameter values if the same dataset is used multiple times. May not minimize the cost function as well as gradient descent but the approximation is often ``close enough''. One potential downside is that if the approximation of the error surface is not ``good enough'' minimization could take a, relatively speaking, long time.}

\r{rather than the single static error surface, the error surface is now dynamic as it is being estimated during every iteration with respect to only one training example.}


\paragraph{Mini-batch Gradient Descent}

\r{mini-batch gradient descent --- compromise between batch and stochastic gradient descent where the gradient is calculated over a subset of training data (minibatch). The minibatch size then acts as another hyper-parameter.}

\r{Since the gradient is calculated on a single example, the error surface will appear noisier than if it was calculated over a batch or the entire training set.}

\r{When using stochastic gradient descent, it is important to shuffle the data after each epoch.}


% when looking at specific optimizers, http://ruder.io/optimizing-gradient-descent/ was a useful resource

\subsection{second-order}

\subsection{direct methods}

\subsection{stochastic methods}

\subsection{population methods}

\subsection{Further optimization information}


\section{Common Optimizers}

\r{Some of the common optimizers are listed below. Additional optimizers are discussed in \textcolor{red}{local ref?}}

\subsection{Momentum}

\textcolor{blue}{Momentum~\cite{qian1999momentum}, will reduce the learning rate when the gradient is small}

\subsection{RMSProp}

\subsection{Nesterov}

\textcolor{blue}{Nesterov accelerated gradient (NAG)}

\subsection{Adam}

\textcolor{blue}{Adaptive Moment Estimation (Adam)~\cite{kingma2014adam}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% TODO: Cut off -- these optimizers will be moved to another location (research?)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Nadam}

\textcolor{blue}{Nadam (Nesterov-accelerated Adaptive Moment Estimation)~\cite{dozat2016incorporating}}

\subsection{AdaGrad}

\textcolor{blue}{Adagrad~\cite{duchi2011adaptive}, will assign frequently occurring features low learning rates}

\subsection{AdaDelta}

\textcolor{blue}{Adadelta~\cite{zeiler2012adadelta}, expands on AdaGrad by avoiding reducing the learning rate to zero.}

\subsection{AdaMax}

\subsection{Ftrl}

\textcolor{blue}{``follow the regularized leader'', \textcolor{red}{CITE}, \textcolor{red}{works well on wide modes?}}





