\subsection{Optimizers}

\r{Estimate the values of the model's parameters that minimize the value of the cost function based on the data it observes}

\r{"turning a loss function into a search strategy"}

% this may belong elsewhere
% alternatives to gradient descent 
% conjugate gradient
% BFGS
% L-BFGS
% pro: faster, don't need to pick the LR con: more complex
% line search algorithm

\TD{Error surface definition} --- \r{the error surface may include flat region which, in high-dimensional spaces is considered a saddle point.}

\subsubsection{Gradient Descent}

\r{Gradient Descent --- overview --- optimization algorithm that can be used to estimate the local minimum of a function}

\r{Iteratively updates the model parameters by calculating the partial derivatives of the cost function at each step during training}

\r{Gradient descent is only guaranteed to find the local minimum of the cost function.}

\r{simultaneous update.}


\paragraph{Batch Gradient Descent}

\r{batch gradient descent --- taking a step (update the weights) opposite (down) the gradient calculated from the entire training set}

\r{Batch gradient descent is deterministic --- will produce the same paramter values if the same dataset is used multiple times.}

\r{single static error surface}


\paragraph{Stochastic Gradient Descent}

\r{Stochastic Gradient Descent (sometimes called iterative or on-line gradient descent) --- rather than update the weights based on the sum of the accumulated errors, the weights are updated for each training sample}

\r{Stochastic gradient descent is deterministic --- may produce the different parameter values if the same dataset is used multiple times. May not minimize the cost function as well as gradient descent but the approximation is often ``close enough''. One potential downside is that if the approximation of the error surface is not ``good enough'' minimization could take a, relatively speaking, long time.}

\r{rather than the single static error surface, the error surface is now dynamic as it is being estimated during every iteration with respect to only one training example.}


\paragraph{Mini-batch Gradient Descent}

\r{mini-batch gradient descent --- compromise between batch and stochastic gradient descent where the gradient is calculated over a subset of training data (minibatch). The minibatch size then acts as another hyper-parameter.}

\r{Since the gradient is calculated on a single example, the error surface will appear noisier than if it was calculated over a batch or the entire training set.}

\r{When using stochastic gradient descent, it is important to shuffle the data after each epoch.}


% when looking at specific optimizers, http://ruder.io/optimizing-gradient-descent/ was a useful resource

\subsection{Improved Optimizers}

\r{Some of the common optimizers are listed below. Additional optimizers are discussed in \textcolor{red}{local ref?}}

\subsubsection{Momentum}

\textcolor{blue}{Momentum~\cite{qian1999momentum}, will reduce the learning rate when the gradient is small}

\subsubsection{RMSProp}

\subsubsection{Nesterov}

\textcolor{blue}{Nesterov accelerated gradient (NAG)}

\subsubsection{Adam}

\textcolor{blue}{Adaptive Moment Estimation (Adam)~\cite{kingma2014adam}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% TODO: Cut off -- these optimizers will be moved to another location (research?)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Nadam}

\textcolor{blue}{Nadam (Nesterov-accelerated Adaptive Moment Estimation)~\cite{dozat2016incorporating}}

\subsubsection{AdaGrad}

\textcolor{blue}{Adagrad~\cite{duchi2011adaptive}, will assign frequently occurring features low learning rates}

\subsubsection{AdaDelta}

\textcolor{blue}{Adadelta~\cite{zeiler2012adadelta}, expands on AdaGrad by avoiding reducing the learning rate to zero.}

\subsubsection{AdaMax}

\subsubsection{Ftrl}

\textcolor{blue}{``follow the regularized leader'', \textcolor{red}{CITE}, \textcolor{red}{works well on wide modes?}}





