\subsection{Qualitative Evalutation: Performance Metrics}

% NOTE: the term for this section should be ``performance metrics'' that are metrics related to model performance

% performance metrics are typcially directly related to business goals

% TODO: this para needs to be merged with the prev section and moved to where it is decided it best fits
\emph{Cost} is frequently used interchangeably with loss. Technically, loss refers to the error on a single example and cost is the average of the loss across the entire training set.

\subsubsection{Confusion Matrix}
\textcolor{blue}{A confusion matrix (sometimes referred to as a table of confusion, or contingency table) XXXXXXXX}

%% Confusion matrix
\begin{table}
	\centering
	\begin{tabular}{l|l|c|c|}
		\multicolumn{2}{c}{}&\multicolumn{2}{c}{Ground Truth}\\ 
		\cline{3-4}
		\multicolumn{2}{c|}{}&Positive&Negative\\ 
		\cline{2-4}
		\multirow{2}{*}{\rotatebox{90}{Pred}}& Positive & $TP$ & $FP$ \\ 
		\cline{2-4}
		& Negative & $FN$ & $TN$ \\ 
		\cline{2-4}
	\end{tabular}
	\caption{Example confusion matrix}
	\label{tab:sample_conf_matrix}
\end{table}

\textcolor{blue}{From the confusion matrix:}

\begin{itemize}
	
\item \textit{TP (True Positive)}: ``hit'', correct positive prediction. The ground truth is positive and the prediction is positive.

\item \textit{TN (True Negative)}: correct rejection. The ground truth is negative and the prediction is negative.

\item \textit{FP (False Positive)}: False alarm or Type I error\index{Type I error}. The ground truth is negative, but the prediction is positive.

\item \textit{FN (False Negative)}: Miss or Type II error\index{Type II error}. The ground truth is positive, but the prediction is negative.
	
\end{itemize}

\subsubsection{Classification Metrics}

\textcolor{blue}{The below measures of performance are calculated with the indicated equation with values obtained from the confusion matrix XXXXXXXX}

\begin{itemize}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Accuracy (ACC)}, (Eq.~\ref{eq:accuracy}): the ratio of correct predictions to the total number of predictions. \textcolor{blue}{this is typically the ``go to metric'', however, accuracy may give a false sense of XXXXX and is particularly not very informative if dealing with skewed (unbalanced data) --- see example in \textcolor{red}{local ref?}}

\begin{equation}
{\frac{TP+TN}{TP+TN+FP+FN}}
\label{eq:accuracy}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Misclassification rate}, (Eq.~\ref{eq:misclassification_def}): \textcolor{blue}{the ``opposite'' of accuracy}.

\begin{equation}
{\frac{FP+FN}{TP+TN+FP+FN}}
\label{eq:misclassification_def}
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Sensitivity (recall, hit rate, true positive rate (TPR))}, (Eq.~\ref{eq:sensitivity}): the ratio of true positives that are correctly identified.

\begin{equation}
{\frac{TP}{TP+FN}}
\label{eq:sensitivity}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Specificity (true negative rate (TNR))}, (Eq.~\ref{eq:specificity}): \textcolor{blue}{XXXXXXXXXX}.

\begin{equation}
{\frac{TN}{TN+FP}}
\label{eq:specificity}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Precision (positive predictive value (PPV))}, (Eq.~\ref{eq:precision}): the ratio of positives that are, in fact, positive. If the classifier predicts positive, how often is is correct?

\begin{equation}
{\frac{TP}{TP+FP}}
\label{eq:precision}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Negative Predictive Value (NPV)}, (Eq.~\ref{eq:npv}): \textcolor{blue}{XXXXXXXXXX}.

\begin{equation}
{\frac{TN}{TN+FN}}
\label{eq:npv}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Miss Rate (False Negative Rate (FNR))}, (Eq.~\ref{eq:miss_rate}): \textcolor{blue}{XXXXXXXXXX}.

\begin{equation}
{\frac{FN}{FN+TP}}
\label{eq:miss_rate}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Fall-Out (False Positive Rate (FPR))}, (Eq.~\ref{eq:fall_out}): \textcolor{blue}{XXXXXXXXXX}.

\begin{equation}
{\frac{FP}{FP+TN}}
\label{eq:fall_out}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{False Discovery Rate (FDR)}, (Eq.~\ref{eq:false_discovery}): \textcolor{blue}{XXXXXXXXXX}.

\begin{equation}
{\frac{FP}{FP+TP}}
\label{eq:false_discovery}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{False Omission Rate (FOR)}, (Eq.~\ref{eq:false_omission}): \textcolor{blue}{XXXXXXXXXX}.

\begin{equation}
{\frac{FN}{FN+TN}}
\label{eq:false_omission}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TODO: define harmonic mean somewhere
\item \textit{F-1 Score}, (Eq.~\ref{eq:f1_metric}): \textcolor{blue}{F1 is the \textcolor{red}{harmonic mean} of precision and sensitivity XXXXXXXXXX. The F1 score will penalize classifiers more as the difference between the precision and sensitivity increases.}.

\begin{equation}
{\frac{2TP}{2TP+FP+FN}}
\label{eq:f1_metric}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Matthews Correlation Coefficient (MCC)}, (Eq.~\ref{eq:mcc_metric}): \textcolor{blue}{MCC is  an alternative to the F1 score for evaluating binary classifiers. MCC is useful even when the ratio of class in the data is severely imbalanced. the XXXXXXXXXX}.

\begin{equation}
{\frac{TP \times TN - FP \times FN}{\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}}
\label{eq:mcc_metric}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Informedness (Bookmaker Informedness (BM))}, (Eq.~\ref{eq:informed_metric}): \textcolor{blue}{Informedness is the XXXXXXXXXX}.

\begin{equation}
{\frac{TP}{TP+FN}+\frac{TN}{TN+FP}-1}
\label{eq:informed_metric}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{Markedness (MK)}, (Eq.~\ref{eq:markedness_metric}): \textcolor{blue}{Markedness is the XXXXXXXXXX}.

\begin{equation}
{\frac{TP}{TP+FP}+\frac{TN}{TN+FN}-1}
\label{eq:markedness_metric}
\end{equation}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item \textit{AUC (Area Under the Curve)}, is a single value representing the area under an ROC curve. Though generally referred to as the AUC, the term is correctly abbreviated AUROC, specifying that the curve is an ROC curve. The larger the auROC, the better.
\end{itemize}

\subsubsection{Precision-Recall curve}

\TD{Diagram}

\r{choice of the threshold to use moving forward}

\r{auROC \ALR used}

\subsubsection{Regression Metrics}

\textcolor{blue}{It is important to note that regression performance metrics must ignore the direction of the error, otherwise the positive and negative errors would cancel each other out and the overall score would appear artificially optimistic \textcolor{blue}{see local figure}. This is typically corrected for by either taking the absolute value or the square of the value. An important consideration will be how severely outliers should be penalized, as a squared component will result in a larger penalization than an absolute value.}

\textcolor{green}{todo: Figure showing $\pm$errors and how direction is important}



\begin{itemize}
\item \textit{Mean Absolute Error (MAE)}, (Eq.~\ref{eq:mae_def}): \textcolor{blue}{the average of the absolute values of the errors of the predictions}.
	
\begin{equation}
{MAE = \frac{1}{n}\sum_{i=0}^{n-1}|y_i - \hat{y}_i| }
\label{eq:mae_def}
\end{equation}
	
\item \textit{Mean Squared Error (MSE, or Mean Squared Deviation (MSD))}, (Eq.~\ref{eq:mse_def}): \textcolor{blue}{the average of the squared values of the errors of the predictions}.
	
\begin{equation}
{MSE = \frac{1}{n}\sum_{i=0}^{n-1}(y_i - \hat{y}_i)^2}
\label{eq:mse_def}
\end{equation}
	
\end{itemize}


\subsubsection{Additional Metrics}


\paragraph{Linear Evaluation}

\textcolor{green}{TODO: overview of linear evaluation metrics}

\textcolor{blue}{Coefficient of Determination (R-squared ($R^2$)) quantifies how close the data is to a \textcolor{red}{hyperplane} -- a line in a 2-Dimensional space.}

\textcolor{blue}{Several methods exist to calculate R-squared}

% p42(30) of Mastering ML w/scikit
\textcolor{blue}{Pearson product-moment correlation coefficient (PPMCC), or {Pearson's R}\index{Pearson's R} results in a positive number between 0 and 1.}

\textcolor{blue}{NOTE: R-squared is particularly sensitive to outliers.}

\textcolor{blue}{R-squared can spuriously increase when features are added}

\paragraph{Distance Metrics}

\textcolor{blue}{There are four basic requirements for the distance metric:}

\begin{itemize}
	\item Non-negativity: the value must be greater or equal to 0
	\item Identity: if the distance metric between $a$ and $b$ is zero, the two values must be at the same location
	\item Symmetry: the distance metric from $a$ to $b$ must be the same as the distance metric from $b$ to $a$
	\item Triangular inequality: metric($a$,$b$) $\le$ metric($a$,$c$) $+$ metric($b$,$c$)
\end{itemize}

\textcolor{blue}{When calculating the nearest neighbors the terms \textit{distance} and \textit{similarity} may be used interchangeably -- it is important to keep in mind that though they are the ``same'', they are different terms in that the lowest value for distance is ``best'' and the highest value for similarity is ``best''.}

\textcolor{blue}{The default distance metric is the Euclidean distance}

\textcolor{blue}{both the Euclidean and Manhattan distances are special cases of the Minkowski distance}

% see p184 of FofMLforpred data analytics
\textcolor{green}{TODO: more about the Minkowski distance def here}

\textcolor{blue}{Minkowski-based Euclidean distance -- a straight line between two points (Eq~\ref{eq:euclidean_distance_def})}

\begin{equation}
{\sqrt{\sum_{i=1}^{m}{{(a[i] - b[i])}^2}}}
\label{eq:euclidean_distance_def}
\end{equation}


\textcolor{blue}{Manhattan distance (Eq.~\ref{eq:manhattan_distance_def}) -- may also be called the taxi-cab distance, since it is similar to how a driver would have to drive from one point to another on a grid based road system like Manhattan.}

\begin{equation}
{\sum_{i=1}^{m}{abs(a[i] - b[i])}}
\label{eq:manhattan_distance_def}
\end{equation}

\textcolor{blue}{When implementing a nearest neighbor using Euclidean distance, the feature space is partitioned into {Voronoi tessellation}\index{Voronoi tessellation}. New points are assigned to a {Voronoi region}\index{Voronoi region}.}

% see p214 of FofMLforpred data analytics
\textcolor{green}{TODO: More about other similarity measures}




\paragraph{Multi-label Classification}

\textcolor{blue}{Images.}

\textcolor{blue}{TODO: include additional metrics like JI, DC, others}

\textcolor{blue}{Jaccard Similarity}

% p125[113] of Mastering ML with SKL
\textcolor{blue}{Hamming Loss}





\subsubsection{Choosing the "right" metrics}

\textcolor{blue}{TODO: paras on choosing the right metrics -- need to consider balance, others}



