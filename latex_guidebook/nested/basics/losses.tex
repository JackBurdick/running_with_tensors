
\section{losses}

% TODO: not sure where this section belongs
% TODO: It would maybe make sense to have an appendix section that covers the common losses

% potentially ``irrelvant''/~uninterpretible in value to the researcher

\subsection{fit somewhere}

\subsubsection{Contrastive Losses}

%% contrastive losses

% TODO: this paper likely belongs elsewhere
\TD{Siamese networks \cite{bromley1994signature}}

\TD{The triplet loss: (FaceNet: A Unified Embedding for Face Recognition and Clustering) \cite{DBLP:journals/corr/SchroffKP15}}

\TD{Beyond triplet loss: a deep quadruplet network for person re-identification \cite{DBLP:journals/corr/ChenCZH17}}

\TD{Debiased Contrastive Learning \cite{Chuang2020DebiasedCL}}

\TD{Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning \cite{Grill2020BootstrapYO}}


\subsection{Overview}

\r{training objective -- continuous}

\subsection{Losses}

\TD{ELBO (Evidence Lower BOund)}

% TODO: index
\TD{Squared logarithmic error (SLE) and Mean SLE (MSLE)}
\TD{Root Mean Squared logarithmic error (RMSLE) and Mean RMSLE (RMSLE)}
\TD{Mean Absolute Percentage Error (MAPE)}


\subsubsection{scalar}

\subsubsection{distribution}


% label smoothing
\subsection{label smoothing}
\TD{Label smoothing}
\TD{When Does Label Smoothing Help? \cite{DBLP:journals/corr/abs-1906-02629}}
\TD{Regularizing Neural Networks by Penalizing Confident Output Distributions \cite{DBLP:journals/corr/PereyraTCKH17}}

