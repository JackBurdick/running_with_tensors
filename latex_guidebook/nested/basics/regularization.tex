\chapter{Improving Generalizability}

\r{The methods shown in the upcoming sections aim to reduce overfitting, but at the cost of potentially limiting the ability of the model. That is, it is worth while avoiding these methods until you overfitting is certain \TD{ref section about determining overfitting}.}

\subsection{Parameter Regularization}

\r{Collection of techniques used to help generalize a model -- which may help prevent overfitting. Typically regularization penalizes complexity of a model.}



% TODO: figure of loss plot showing a steep training and shallow+divergent val/test loss

\r{Helps prevent the model from memorizing noise in the training data.}


\subsubsection{Why Regularization}

% TODO: index overfitting
\r{overfitting: a practical definition may include observing the training loss to improve while the validation loss degrades. \TD{possibly mention \\cite{Nakkiran2020DeepDD}}}

\r{Overfitting --- too complex --- Occam's razor --- hypothesis with the fewest assumptions is best}

\r{modification made to improve generalization.}

\begin{itemize}[noitemsep,topsep=0pt]
	\item Data --- Increase amount of data (even potentially artificially e.g. augmentation)
	\item Architecture --- Reduce complexity of model e.g. applying parameter constraints, and/or reduce overall number of parameters
\end{itemize}

% TODO: note about regularization --- the smaller the value, the stronger the regularization.

\subsubsection{Types of Regularization}

\textcolor{blue}{Regularization is an active area of research.}

% more information on L1/L2 http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/

\begin{itemize}[noitemsep,topsep=0pt]
	\item Early Stopping (implementation: \textcolor{red}{local ref})
	\item Parameter Norm Penalties (implementation: \textcolor{red}{local ref})
	\begin{itemize}[noitemsep,topsep=0pt]
		\item L1 (Lasso) Regularization
		\item L2 (Ridge) Regularization
		\item Elastic Nets
	\end{itemize}
	\item Dataset Augmentation (implementation: \textcolor{red}{local ref})
	\item Noise Robustness
	\item Sparse Representations
	\item Dropout (implementation: \textcolor{red}{local ref})
	\item Ensemble methods (implementation: \textcolor{red}{local ref})
	\item Adversarial Training
\end{itemize}


\subsubsection{Early Stopping}

\r{see p.243 of DL, papers Bishop 1995 and Sjoberg and Ljung 1995}


\subsubsection{Parameter Norm Penalties}

\r{key difference is the penalty term}

\TD{TODO: DIGRAM OF L2 + L1 + elastic nets}

\paragraph{L2 Regularization}

\TD{TODO: DIAGRAM OF L2}

\r{L2, ({Ridge regression}\index{Ridge regression}) may also be known as {Tikhonov regularization}\index{Tikhonov regularization}}

\r{penalizes model parameters that become too large. Will force most of the parameters to be small, but still non-zero}

\r{square of the absolute value of the coefficient}

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.3\textwidth]{example-image-a}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-b}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-c}\hfil\\
	\medskip
	\includegraphics[width=0.3\textwidth]{example-image-a}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-b}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-c}\hfil
	\caption{\TD{Top: NN output decision boundary on 2D dataset Bottom: weight params distribution from tensorboard... from LtoR = same arch with varying degrees of L2 regularization (0.01, 0.1 and 1.0)}}
	\label{fig:basics_regularization_l2_example}
\end{figure}


% p91(71) of mastering ML w SKL says "when lambda is equal to zero, ridge regression is equal to linear regression"

\paragraph{L1 Regularization}

\TD{TODO: DIAGRAM OF L1}

\r{LASSO (\textbf{L}east \textbf{A}bsolute \textbf{S}hrinkage and \textbf{S}election \textbf{O}perator) --- produces sparse parameters. This will force coefficients to zero and cause the model to depend on a small subset of the features.}

\r{absolute value of the weight coefficient}

\r{use only a small subset of the input features and can become resistant to noisy inputs.}

\r{It could be argued that using L1 regularization may help to make a model more interpretable, by using less (presumably more important/relevant) features when making predictions.}

\r{The use of L1 regularization for feature selection}


\paragraph{Elastic Net Regularization}

\r{Linearly combines the $L^1$ (feature selection) and $L^2$ (generalizability) penalties used by both LASSO and ridge regression. The cost is having two parameters (as opposed to just one when using either L1 or L2).}

\TD{TODO: figure}.


\subsection{Dataset Augmentation}

% NOTE: maybe I need to nest this?
\r{regularization technique}

\r{Arguably the best way to increase generalizability of a model is to train the model on more data. However, as readers may already be aware, this is not always easy. Collecting more data may not be time/cost effective, or even possible.}

\r{``free'' data in that the ``cost'' is minor computation}

\r{Please note, augmentation must be done responsibly. For example, if performing digit recognition, it would not be wise to perform rotational or flip transformations on the data since, depending on the specific data, a 6, rotated 180 or flipped vertically may now appear as a 9.}


\r{invariances in the data}

%TODO: here!

\r{Dataset augmentation is \textcolor{green}{TODO}}

% TODO: this might be the wrong heading.. Originally I was thinking ``spatial`` but I think that's worse

\TD{Augmenting the test set. A simple augmentation (horizontal filliping) was performed on the test set in \cite{simonyan2014very} -- where the prediction of the original and augmented images are averaged to obtain the final output score.}


\subsubsection{Two Dimensional Data}

\r{Including Imagery}

\r{flip, rotate. color/channel manipulation}

\r{mixup\cite{zhang2017mixup}}

\r{cutout\cite{devries2017improved}}

\r{cutmix\cite{yun2019cutmix}}

\TD{language -- back translation\cite{sennrich2015improving}}

% TODO: blog about this: https://ai.googleblog.com/2019/07/advancing-semi-supervised-learning-with.html
\TD{unsupervised augmentation\cite{xie2019unsupervised}}


\subsubsection{Learning Augmentation}

% TODO: not sure this belongs here
\r{Sample Pairing\cite{inoue2018data}}

\r{Smart Augmentation\cite{lemley2017smart}}

\r{GAN\cite{shrivastava2017learning}}

\r{population based augmentation (PBA)\cite{ho2019population}}

\r{Bayesian data augmentation\cite{tran2017bayesian}}


\paragraph{AutoAugment}

\r{AutoAugment\cite{cubuk2018autoaugment}}

\r{Comment that AutoAugment can be applied directly to a dataset as well as transfer the learned policies to new datasets.}

\TD{figure of loop. controller, strategy, child network, update controller}

\r{Fast AutoAugment\cite{lim2019fast} improves upon the original search strategy in the original AutoAugment paper.}

\r{Unsupervised Augmentation}

\TD{autoaugment for object detection \cite{zoph2019learning}}



\subsection{Dropout}

% TODO: explain dropout

\r{Dropout -- ref original paper (Hinton? -- intuitive, inspired by bank -- that defrauding the bank would require cooperation between employees to defraud the bank \TD{cite})}.

\r{Dropout (proposed in ``Improving Neural Networks by Preventing Co-Adaption of Feature Dectors''~\cite{DBLP:journals/corr/abs-1207-0580}, and popularized by Nitish et.al in ``Dropout: a Simple Way to Prevent Nerual Networks from Overfitting''~\cite{JMLR:v15:srivastava14a}}

\r{It is important to note that dropout is only present during training. i.e. dropout does not occur during test/evaluation if using dropout in the ``standard way''. However dropout is occassionally used for evaluation in attempt to quantify model uncertainty \TD{CITATION}}

\r{keeps a neuron active by a hyperparameterized probability.}

\r{used in any/all neurons in the network (other than the output neruons).}

\r{think about where dropout is used. That is when you use dropout at any given nueron the upstream paths transversing that particular neuron are also affected (in this case, ``turned off''), as well downstream connections (but often only modified, not entirely turned off since they often still have other inputs) }

\r{Forces the network to learn mappings even in the absence of all the information, that is the network is forced to consider the values of other values and can't rely on a smaller number of values or groups of values. Said another way, the network is prevented from becoming too dependent on certain inputs or features.}

\r{In this way, dropout can be thought of as sort of an ensembling method. When dropout is in use during training, each loop technically produces a different network that is then trained for the given task. During the next loop, a different network is used. As Aurélien Géron~\cite{geron2019hands} describes, if you train for 10,000 training steps (where dropout is used), you will have likely (almost certainly) trained 10,000 different neural networks. It's true that each network is not indpendant (they share weights), but they are different. More generally, a network with $N$ activations with dropout present, there exist $2^N$ possible networks ($2$ since each activation/neuron/value can have either an `on` or `off` state.) and thus, the use of all of these networks at once can be considered an ensembling of sorts.}

\TD{create figure of this ensemble of many networks.}


% TODO: find recent paper I saw mentioned on twitter.... (4July) it may be in my pocket

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.3\textwidth]{example-image-a}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-b}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-c}\hfil
	\caption{\TD{Graph of an example function including dropout. three separate training iterations and how the network changes}}
	\label{fig:regularization_dropout_overview_training}
\end{figure}

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.3\textwidth]{example-image-a}\hfil
	\caption{\TD{Same graph during test --- no dropout applied}}
	\label{fig:regularization_dropout_overview_test}
\end{figure}

\r{It is worth pointing out that since dropout is only applied at training time, comparing the loss curve of training and inference (validation splits) will be a bit misleading since the full ensemble network is used for calculating the validation loss/metrics and only the component \TD{is there a better word than this?} networks are used for the training set.}

\r{Additionally, if you run the training set through multiple times, you may find slightly different results. Again, this is because while dropout is on, you'll find that a slightly different network is used. \TD{This idea can be exploited at inference time to get uncertainty estimates.}}

\r{some important notes about the implementation. The outputs at test time should be equivalent to their expected outputs at training time (which is altered due to the application of dropout).}

\r{One potential solution to this problem is to scale the outputs during inference in a way that compensates for the dropout probability.  For example, if the dropout rate was set to $0.5$, then it would become necessary to halve the neurons outputs at test time in order to keep the expected output the neurons have learned during training.  However, this may not be ideal in practice since it would require scaling all the neuron outputs at test time (where performance is often critical and more important).}

\r{at test time, multiply the values by the expectation, not the on/off mask}


\r{Another, perhaps more desirable solution, would be to use \IDI{inverted dropout}. This applies the same principal as outlined above, only the scaling occurs at training time rather that at test time. That is, during training, any neuron whose activation was not turned off, has the output divided by the dropout rate before being propagation to the next layer.  This way, at test time, no scaling is required.}



% `drop block''?
\TD{investigate more structured dropout.}

% helps learn ``multiple paths''/simulates ensembles


\TD{alpha dropout}

\subsection{Ensemble Methods}

\r{see \textcolor{red}{local ref} for more information on ensemble basics and see \textcolor{red}{local ref} for implementation details.}

% TODO: find Breiman 1994 paper referenced in p249 of Deep Learning
\r{As described in \textcolor{red}{local ref} ensemble methods act as a form of regularization by combining several different models \TD{Breiman 1994}. This often improves generalizability since the included models will often make independent, different, errors on the data.}

\subsection{Adversarial Training}



\subsection{Transfer Learning}

% TODO: is this talked about anywhere else? this is probably the best place for it.

\TD{TODO: transfer learning, using -- explanation}

\TD{tool that may sometimes be efficient way of getting to potentially more accurate approximations, faster. \TD{citations}}

% TODO: index
\r{using parameters or pre-trained components from a model/task for a new model/task.  Process of adapting these components to a new model/task is called fine-tuning}


\begin{figure}[htp]
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{Figure example layer hierarchy and where/when to transfer/freeze params -- this will be 1-2 figures and include many sub-figures \textcolor{green}{TODO}}
	\label{fig:transfer_learning_subfigs_a}
\end{figure}

\textcolor{green}{{freezing}\index{freezing} parameters or a layer means preventing the parameters from being updated during training. This is often controlled by a parameter called ``trainable'' see \textcolor{red}{local TF ref} for the TensorFlow implementation and \textcolor{red}{local YF ref} for the YamlFlow implementation. In relation to transfer learning and freezing, mention the difficulty of propagating updates though a large network}

% TODO: this likely does not belong here...
\subsubsection{Normalization}

\TD{TODO: overview para + importance}

\TD{TODO: figure showing differences}

\paragraph{Instance normalization}

\r{see section in preprocessing \textcolor{red}{local ref?}}

\paragraph{Layer normalization}

\paragraph{Batch normalization}

\TD{Show / explain}

\TD{Batch Normalization: Accelerating Deep Network Training by Reducing	Internal Covariate Shift \cite{DBLP:journals/corr/IoffeS15}}

\r{similar to dropout \ALR, the behavior of batch norm is different at training time and inference time.}

\r{Standard implementation is to calculate the population values using an exponential moving average (EMA).}

%TODO: here!

\TD{{Rethinking "Batch" in BatchNorm}~\cite{Wu2021RethinkingI} concludes that using EMA as the method for calculating the population statistics is not ideal. They show that during the early epochs, the xxxxxxx.}

\r{An adaptive re-parameterization.}

\r{reduce sensitivity to hyperparameterization.}

\TD{TODO: transfer learning considerations --- will likely have to unfreeze these params}

% HUGO talk
\r{``making the optimization easier''. batch norm is not effective in RNNs -- more so layer norm}

\r{seems to help when both under and over fitting.}

\r{order: pre-act, then batch norm, then activate.}

\r{$\gamma$ and $\beta$ parameters that are learned parameters. These params could effectively undo the normalization caused (if ``learned'' to do so.)}

\begin{enumerate}[noitemsep,topsep=0pt]
	\item batch statistics
	\begin{itemize}[noitemsep,topsep=0pt]
		\item mean
		\item variance
	\end{itemize}
	\item normalize the pre-activation
	\item $\gamma$ and $\beta$ --- learned rescalling
\end{enumerate}


% Graham Taylor talk
\begin{itemize}[noitemsep,topsep=0pt]
	\item turn down other regularization
	\item fixes first and second moments which may suppress information in these moments.
\end{itemize}

\TD{work related to adversarial spheres. --- with batch norm, the result was more reflective of the batch, not the entire dataset (which makes sense, right?)}


\paragraph{Group normalization}


\section{Output regularization}

\r{confidence penalty on predictions that are extrememly confident\cite{pereyra2017regularizing}. Originally an RL idea to promote expoloration. In SL, we would prefer fast convergence i) anneal confidence penalty ii) only penalize at a certain confidence threshold (lower entropy threshold). Intuitive (or not), can improve generalization.}

%TODO:
\r{label smoothing\cite{szegedy2016rethinking}}

\r{Adding label noise\cite{xie2016disturblabel}}

\r{smooth labels -- either via a ``teacher model''\cite{hinton2015distilling} or using it's own distribution\cite{reed2014training}}


\r{virtual adversarial training\cite{miyato2018virtual}}