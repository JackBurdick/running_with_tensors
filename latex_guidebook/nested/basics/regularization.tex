\section{Improving Generalizability}

\subsection{Regularization}

\r{Collection of techniques used to help generalize a model -- which may help prevent overfitting. Typically regularization penalizes complexity of a model.}

% TODO: figure of loss plot showing a steep training and shallow+divergent val/test loss

\r{Helps prevent the model from memorizing noise in the training data.}


\subsubsection{Why Regularization}

\r{Overfitting --- too complex --- Occam's razor --- hypothesis with the fewest assumptions is best}

\r{modification made to improve generalization.}

\begin{itemize}
	\item Data --- Increase amount of data (even potentially artificially e.g. augmentation)
	\item Architecture --- Reduce complexity of model e.g. applying parameter constraints, and/or reduce overall number of parameters
\end{itemize}


\subsubsection{Types of Regularization}

\textcolor{blue}{Regularization is an active area of research.}

\begin{itemize}
	\item Early Stopping (implementation: \textcolor{red}{local ref})
	\item Parameter Norm Penalties (implementation: \textcolor{red}{local ref})
	\begin{itemize}
		\item L1 (Lasso) Regularization
		\item L2 (Ridge) Regularization
		\item Elastic Nets
	\end{itemize}
	\item Dataset Augmentation (implementation: \textcolor{red}{local ref})
	\item Noise Robustness
	\item Sparse Representations
	\item Dropout (implementation: \textcolor{red}{local ref})
	\item Ensemble methods (implementation: \textcolor{red}{local ref})
	\item Adversarial Training
\end{itemize}


\subsubsection{Early Stopping}

\r{see p.243 of DL, papers Bishop 1995 and Sjoberg and Ljung 1995}


\subsubsection{Parameter Norm Penalties}

\r{key difference is the penalty term}

\TD{TODO: DIGRAM OF L2 + L1 + elastic nets}

\paragraph{L2 Regularization}

\TD{TODO: DIAGRAM OF L2}

\r{L2, ({Ridge regression}\index{Ridge regression}) may also be known as {Tikhonov regularization}\index{Tikhonov regularization}}

\r{penalizes model parameters that become too large. Will force most of the parameters to be small, but still non-zero}

\r{square of the absolute value of the coefficient}

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.3\textwidth]{example-image-a}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-b}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-c}\hfil\\
	\medskip
	\includegraphics[width=0.3\textwidth]{example-image-a}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-b}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-c}\hfil
	\caption{\TD{Top: NN output decision boundary on 2D dataset Bottom: weight params distribution from tensorboard... from LtoR = same arch with varying degrees of L2 regularization (0.01, 0.1 and 1.0)}}
	\label{fig:basics_regularization_l2_example}
\end{figure}


% p91(71) of mastering ML w SKL says "when lambda is equal to zero, ridge regression is equal to linear regression"

\paragraph{L1 Regularization}

\TD{TODO: DIAGRAM OF L1}

\r{LASSO (Least Absolute Shrinkage and Selection Operator) --- produces sparse parameters. This will force coefficients to zero and cause the model to depend on a small subset of the features.}

\r{absolute value of the weight coefficient}

\r{use only a small subset of the input features and can become resistant to noisy inputs.}

\r{It could be argued that using L1 regularization may help to make a model more interpretable, by using less (presumably more important/relevant) features when making predictions.}

\r{The use of L1 regularization for feature selection}


\paragraph{Elastic Net Regularization}

\r{Linearly combines the $L^1$ (feature selection) and $L^2$ (generalizability) penalties used by both LASSO and ridge regression. The cost is having two parameters (as opposed to just one when using either L1 or L2).}

\TD{TODO: figure}.

\subsubsection{Dataset Augmentation}

\r{Arguably the best way to increase generalizability of a model is to train the model on more data. However, as readers may already be aware, this is not always easy. Collecting more data may not be time/cost effective, or even possible.}

\r{Please note, augmentation must be done responsibly. For example, if performing digit recognition, it would not be wise to perform rotational or flip transformations on the data since, depending on the specific data, a 6, rotated 180 or flipped vertically may now appear as a 9.}

\r{Dataset augmentation is \textcolor{green}{TODO}}

\paragraph{Image Augmentation}



\subsubsection{Dropout}

% TODO: explain dropout

\r{It is important to note that dropout is only present during training. i.e. dropout does not occur during test/evaluation.}

\r{keeps a neuron active by a hyperparameterized probability.}

\r{Forces the network to learn mappings even in the absence of all the information.}

\r{prevents the network from becoming too dependent on certain inputs or groups of inputs.}

\r{combines \textcolor{red}{similarly to ensembling} many different nerual network architectures}

% TODO: include Srivastava et al 2014 (ref on p251 of DL)
\r{Dropout -- ref original paper (Hinton? -- inspired by bank -- that defrauding hte bank would require cooperation between employees to defraud the bank)}

% TODO: find recent paper I saw mentioned on twitter.... (4July) it may be in my pocket

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.3\textwidth]{example-image-a}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-b}\hfil
	\includegraphics[width=0.3\textwidth]{example-image-c}\hfil
	\caption{\TD{Graph of an example function including dropout. three seperate training iterations and how the network changes}}
	\label{fig:regularization_dropout_overview_training}
\end{figure}

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.3\textwidth]{example-image-a}\hfil
	\caption{\TD{Same graph during test --- no dropout applied}}
	\label{fig:regularization_dropout_overview_test}
\end{figure}

\r{some important notes about the implementation. The outputs at test time should be equivalent to their expected outputs at training time (which is altered due to the application of dropout).}

\r{One potential solution to this problem is to scale the outputs during inference in a way that compensates for the dropout probability.  For example, if the dropout rate was set to $0.5$, then it would become necessary to halve the neurons outputs at test time in order to keep the expected output the neurons have learned during training.  However, this may not be ideal in practice since it would require scaling all the neuron outputs at test time (where performance is often critical and more important).}

\r{Another, perhaps more desirable solution, would be to use \IDI{inverted dropout}. This applies the same principal as outlined above, only the scaling occurs at training time rather that at test time. That is, during training, any neuron whose activation was not turned off, has the output divided by the dropout rate before being propagation to the next layer.  This way, at test time, no scaling is required.}

% helps learn ``multiple paths''/simulates ensembles

\subsubsection{Ensemble Methods}

\r{see \textcolor{red}{local ref} for more information on ensemble basics and see \textcolor{red}{local ref} for implementation details.}

% TODO: find Breiman 1994 paper referenced in p249 of Deep Learning
\r{As described in \textcolor{red}{local ref} ensemble methods act as a form of regularization by combining several different models \TD{Breiman 1994}. This often improves generalizability since the included models will often make independent, different, errors on the data.}

\subsubsection{Adversarial Training}

% TODO: this may not belong here...
\subsubsection{Normalization}

\TD{TODO: overview para + importance}

\TD{TODO: figure showing differences}

\paragraph{Instance normalization}

\r{see section in preprocessing \textcolor{red}{local ref?}}

\paragraph{Layer normalization}

\paragraph{Batch normalization}

\paragraph{Group normalization}