\section{Partitioning Data}

\textcolor{blue}{Ideally, a model will generalize well i.e. a model will perform well on data that it has never seen. When evaluating a model, the performance is reported on a data that has never been seen by the model. When a dataset is obtained, one of the first steps performed is to partition the dataset -- a portion of the dataset is removed and placed aside as the ``test'' set that will later be used to measure the performance of an indicated model.}

\subsection{Types of Splits}

\textcolor{blue}{Training, validation, test}

\textcolor{blue}{The validation set is typcially used to help avoid overfitting to the training data}

\textcolor{green}{TODO: include figure showing when to save the best parameters}

\textcolor{blue}{There are no hard rules that clearly define how a dataset should be divided into training, validation, and test sets and usually the ratio of data in each split depends on the overall size of the dataset.}

\textcolor{blue}{Some common dataset splits (training:validation:test) are 50:20:30 and 40:20:40}

\textcolor{blue}{{lucky split}\index{lucky split} is used to describe an instance in which, by chance, the test set contains easily predicted instances and the training set includes difficult to predict instances.}

\subsection{k-Fold Cross Validation}



\subsection{Sampling}

\textcolor{blue}{Random vs \textcolor{red}{stratified}}
