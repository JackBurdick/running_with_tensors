\section{Partitioning Data}

\textcolor{blue}{Ideally, a model will generalize well i.e. a model will perform well on data that it has never seen. When evaluating a model, the performance is reported on a data that has never been seen by the model. When a dataset is obtained, one of the first steps performed is to partition the dataset -- a portion of the dataset is removed and placed aside as the ``test'' set that will later be used to measure the performance of an indicated model.}

\subsection{Types of Splits}

\textcolor{blue}{A learning method is trained on a collection of examples called the ``training set''. The performance of the learning method is then evaluated on another collection of examples known as the ``test set''. It is important to ensure that none of the instances in the test set are included in the training set -- a point that will be mentioned many times throughout this test. If the test set were to include examples from the training set, evaluating the whether the learning method has memorized the training set or generalizes well will be difficult.}

\textcolor{blue}{An additional set, known as the validation is also used. The validation set is used to tune the hyperparameters of the learning algorithm and help prevent overfitting to the training data.}

\textcolor{green}{TODO: include figure showing when to save the best parameters}

\textcolor{blue}{There are no hard rules that clearly define how a dataset should be divided into training, validation, and test sets and usually the ratio of data in each split depends on the overall size of the dataset.}

\textcolor{blue}{Some common dataset splits (training:validation:test) are 50:20:30 and 40:20:40}

\textcolor{blue}{{lucky split}\index{lucky split} is used to describe an instance in which, by chance, the test set contains easily predicted instances and the training set includes difficult to predict instances.}

\textcolor{blue}{Typically the performance of a machine learning algorithm improves with the number of training instances. However, quality is superior to quantity, in that a lot of ``bad'' data is worse than a smaller amount of ``good'' data -- in this sense, machine learning algorithms follow the ``garbage in, garbage out''.}

\subsection{k-Fold Cross Validation}

\textcolor{blue}{cross validation is a method used to train across the entire training dataset without holding out an explicit validation set. The training data is partitioned, into $k$ ``folds'', and the algorithm is trained on all but one of these partitions and evaluated on the remaining partition. The partitions are then rotated such that each fold/partition is included in the training and evaluation of the algorithm. After sufficient training, (as defined by the individual), the model is then evaluated on the test set.}

\subsection{Sampling}

\textcolor{blue}{Random vs \textcolor{red}{stratified}}
