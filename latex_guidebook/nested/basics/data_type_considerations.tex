\section{Feature Extraction from Various Datatypes}

\textcolor{green}{TODO: Feature Extraction}


\subsection{Images}

\textcolor{green}{TODO: Images}


\subsubsection{Video}

\textcolor{green}{TODO: Video}


\subsection{Natural Language}

\textcolor{green}{TODO: Natural Language}

\subsubsection{Terminology}

\textcolor{blue}{A {corpus}\index{corpus} is a collection of documents. {vocabulary}\index{vocabulary} is a corpus's unique words}

\subsubsection{Pre-processing}

\textcolor{green}{TODO: Pre-processing}

\paragraph{Tokenization}

\textcolor{blue}{Tokenization is the process of splitting and grouping characters together into meaningful sequences. \textcolor{red}{If a document is tokenized, the result is a set of tokens (words).} Tokens are not limited to words however, and may also be shorter sequences like punctuation characters and affixes.}

\textcolor{green}{TODO: Tokenization example}

\paragraph{Lemmatization}

\textcolor{green}{TODO: Lemmatization}

\paragraph{Stemming}

\textcolor{green}{TODO: Stemming}

\subsubsection{Encoding}

\paragraph{Bag-of-Words}

\textcolor{blue}{{bag-of-words}\index{bag-of-words} similar to one-hot-encoding, it encodes words that appear in text as one feature for each word of interest. Does not encode any other information like syntax, grammar, or order of the words.}

\textcolor{blue}{Bag-of-Words encodes the corpus's vocabulary as a feature vector to represent each document. The intuition for using bag-of-words is that documents that contain similar words are likely to be similar to one another.}


\paragraph{glove}

\textcolor{green}{TODO: glove}

\paragraph{word2vec}

\textcolor{green}{TODO: word2vec}


\subsection{Audio}

\textcolor{green}{TODO: Audio}
