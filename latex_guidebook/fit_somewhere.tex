

\textcolor{blue}{tensorflow's XLA compiler}

\textcolor{blue}{checkpoints -- allow: save/stop/resume training, resume on failure, predict from point}


%%%% may talk about in production
% Google Cloud MLE


%%%
% stats vs ML -- in ML you may keep outliers and build models for them. in ML outliers may be collapsed (capped) and in statistics they may be removed
% ML is used to learn the ``long tail'', make fine grained predictions, not just gobal averages

%% why try to stay in linear (like with feature crosses)
% NN with many layers are non-convex
% optimizing linear models is a convex problem (much easier)

% how have I not talked about transfer learning yet?

% optimizing is an NP-hard, non-convex optimization problem (coursera.need to double check)
\textcolor{blue}{L0-norm (the count of non-zero weights).}


%%%%


%%%
\textcolor{blue}{Each layer in a DNN is a composition of the previous layer. i.e. if layer 1 = f(x), then layer 1 = g(f(x)), layer three is h(g(f(x)) \textcolor{green}{TODO: create diagram}}



%%%% rough...
%\textcolor{red}{Multi-headed inference. Useful when using the same model for different tasks. e.g. using a model for one task, then later deciding to perform a similar task on the same data -- rather than train an entirely new model, the original model may be performing may of the same computations. }



%%%  preprocessing, this wasn't already somewhere? % plus index
\textcolor{blue}{whitening}

%%% unsupervised method
\textcolor{blue}{NFM (Non-Negative Matrix Factorization)}

%%%
\textcolor{blue}{TODO: parameter calculation -- use VGG example (conv + dense)}
\begin{table}
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\multicolumn{5}{|c|}{\textbf{Number of VGG-16 Parameters}}     \\ \hline
		Layer & Out Shape & Weights & Bias & Total  \\ \hline
		\emph{Convolution}        & & $(in)\times(h\times w)\times(out)$ & $(out)$ & $weights+bias$    \\ \hline
		Conv3-64          & $224\times224\times64$ & $3\times(3\times3)\times64$ & $64$ & 1792    \\ \hline
		Conv3-64 (p)      & $112\times112\times64$ & $64\times(3\times3)\times64$ & $64$ & 36928    \\ \hline
		Conv3-128         & $112\times112\times128$ & $64\times(3\times3)\times128$ & $128$ & 73856     \\ \hline
		Conv3-128 (p)     & $56\times 56\times 128$ & $128\times(3\times3)\times128$ & $128$ & 147584   \\ \hline
		Conv3-256         & $56\times 56\times 256$ & $128\times(3\times3)\times256$ & $256$ & 295168   \\ \hline
		Conv3-256         & $56\times 56\times 256$ & $256\times(3\times3)\times256$ & $256$ & 590080   \\ \hline
		Conv3-256 (p)     & $28\times 28\times 256$ & $256\times(3\times3)\times256$ & $256$ & 590080   \\ \hline
		Conv3-512         & $28\times 28\times 512$ & $256\times(3\times3)\times512$ & $512$ & 1180160  \\ \hline
		Conv3-512         & $28\times 28\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808  \\ \hline
		Conv3-512 (p)     & $14\times 14\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808  \\ \hline
		Conv3-512         & $14\times 14\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808  \\ \hline
		Conv3-512         & $14\times 14\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808  \\ \hline
		Conv3-512 (p)     & $7\times 7\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808    \\ \hline
		\emph{dense}         & & $(in)\times(num)$ & $(out)$ & $weights+bias$    \\ \hline
		fc1 (4096)        & 4096 &$(512\times7\times7)\times4096$ & $4096$ & $102764544$    \\ \hline
		fc2 (4096)        & 4096 &$(4096)\times4096$ & $4096$ & $16781312$    \\ \hline
		fc3 (1000)        & 1000 &$(4096)\times1000$ & $1000$ & $4097000$    \\ \hline
		\emph{Total} & & & & 138,357,544 \\ \hline
	\end{tabular}
	\caption{Calculation of VGG parameters. (p) denotes that the layer is followed by a pooling layer (which does not affect the parameter count)}
	\label{tab:vgg_parameter_count}
\end{table}




%%%
\textcolor{green}{TODO: NN from scratch in appendix}

%%%
\textcolor{green}{TODO: CNN without layers API -- in github}

%%% interactive
\textcolor{blue}{The TensorFlow playground~\cite{tf_playground} (\textcolor{green}{TODO: screen shot}).  There are two other notable projects --- deeplearn.js~\cite{deeplearnjs} and ConvNetJS~\cite{convnet_js}.}


%%% distributed
\textcolor{blue}{Distributed TensorFlow Section}


%%% CUDA
\textcolor{blue}{CUDA (Compute Unified Device Architecture) library --- computation}
\textcolor{blue}{cuDNN (CUDA Deep Neural Network) library --- library of GPU-accelerated DNN primitives (activations, normalization, convolutions, pooling)}


%% model quantization
\textcolor{blue}{drop the parameter float precision from 32 bits (\code{tf.float32}) to 16 bits (\code{tf.bfloat16})}

%%%%
\textcolor{blue}{performance metrics --- Bayes error}

\textcolor{blue}{performance metrics --- Ability to perform on new, previously unseen data, generalization error (test error)}


\textcolor{green}{TODO: figure similar to DL pipeline in my thesis and similar to figure 1.5 in DL book. DL automates feature engineering}


\textcolor{green}{MNIST: described by Geoffrey Hinton as ``the drosophilia'' of machine learning. --- fruitflies are often used in biology as controlled laboratory experiments. Also considered the ``hello world'' of deep learning. The dataset is created from grayscale images ($28 \times 28 \times 1$) and has 10 labels 0-9.  There exist 60,000 training images and 10,000 test images. Created by the National Institute of Standards and Technology.}


\textcolor{green}{Two camps of statistics: frequentist estimators and bayesian inference.}


% TODO: figure -- [tensorflow ] -> cudnn -> cuda -> GPU | -> cuda -> GPU | -> [lib] -> CPU


%%
\textcolor{blue}{nonstationary problems --- unsolvable --- need data that makes sense for the problem. Predicting rentals of snowmobiles doesn't make as much sense if you only have data from summer. predicting cloth purchases during summer doesn't make sense if you only have data from winter.}

%% not sure how this wasn't mentioned yet
\textcolor{green}{TODO: transfer learning -- explanation, implementation in TensorFlow, and in YamlFlow \r{if general enough, the lower features may be reusable}}

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{Figure example layer hierarchy and where/when to transfer/freeze params -- this will be 1-2 figures and include many sub-figures \textcolor{green}{TODO}}
	\label{fig:transfer_learning_subfigs_a}
\end{figure}

\textcolor{green}{{freezing}\index{freezing} parameters or a layer means preventing the parameters from being updated during training. This is often controlled by a parameter called ``trainable'' see \textcolor{red}{local TF ref} for the TensorFlow implementation and \textcolor{red}{local YF ref} for the YamlFlow implementation. In relation to transfer learning and freezing, mention the difficulty of propagating updates though a large network}

%%%
\textcolor{green}{Visualizing the output of a CNN}

\begin{enumerate}
	\item Filters: \r{help understand what visual patter a filter is receptive to \textcolor{green}{TODO: lots of examples and refs to implementations}}
	\item Intermediate outputs: \r{help understand the hierarchy of what is important to the classification task \textcolor{green}{TODO: lots of examples and refs to implementations}}
	\item Heatmaps of activation: \r{Help understand what parts (and by relatively how much) of an image were in its classification. \textcolor{green}{TODO: lots of examples and refs to implementations}}
\end{enumerate}


%%
\r{NOTE: somewhere about terms 'higher' and 'lower' levels of an architecture and how they're meaningless/interchangable and depend on context e.g. a diagram vs concept of lower=being closer to raw input.}

%%
\r{dataset == a ``sample'' in statistics}

%%
\r{decision boundary -- separation of classes}

%%

\r{prior knowledge --- additional knowledge about the desired form of a solution that is not obvious in the training data.  Inclusion of prior knowledge may influence the design of a solution, usually though preprocessing.}

\rr{translational invariance --- a property that relates to how a systems decisions are insensitive to the location of a features within an input.}

\r{scale invariance --- a property that relates to how a systems decisions are insensitive to a uniform resizing of a feature within an input }

%%
\r{Intrisic dimensionality --- }


%%
\r{predictions on new data --- good ``generalization'' -- error/performance metrics}

%%
\r{controlling the complexity of a model = controlling the number of parameters. controlling the \textit{effective complexity} is optimizing the generalization performance of the model (using a penalty term (regularization)).}


\r{radial basis function --- }


% page 16 or neural networks (p31 on tablet)
\r{\textcolor{green}{(Barron 1993)} -- \textcolor{red}{``neural networks offer a dramatic advantage for function approximation in spaces of many dimensions''} --- (+) efficient scaling with dimensionality --- (-) now a non-linear optimiation problem = computationally intensive and may include many minimal in the error function}


%%
\r{\textit{joint probability} --- $x_a$ and $x_b$. \textit{conditional probability} --- P of $x_b$, given $x_a$}


%%
% p.20 of Neural Networks 
\r{``the outputs of neural networks can be interpreted as (approximations to) posterior probabilities''} \r{two stages in a classification process 1) \textit{inference} --- data is used to determine the values for the posterior probablities 2) \textit{decision making} --- the probabilities are used to make new decisions}


%%
\textcolor{green}{TODO: Bayesian vs frequentist --- see page 21 of neural networks (p36 on tablet)}

%%
\r{sigmoid == `S-shaped' == the logistic form of sigmoid maps $(-\infty, \infty)$ to $(0, 1)$}

%%
\r{linearly seperable --- where a dataset can be correctly seperated by a linear (hyperplanar) decision boundry}
\r{non linearly separable --- two-dimensional excllusive OR problem. --- generalized to d-dimensions when it is knowns as the d-bit parity problem.}

%%
\r{small changes in inputs, ideally, should not generally lead to dramatic changes in the outputs --- leading to a mapping that is represented relatively smoothly.}

%%
\r{colinear --- \textcolor{green}{TODO}}

%%
\TD{robbins-monro procedure}

%%
\r{\TD{perceptron convergence theorem} --- ``for any linearly separable data set, the learning rule (see fig 3.68 in NN - p100), is guaranteed to find a solution in a finite number of steps.'')}

\TD{the ``pocket algorithm'' --- finding solutions to problems which are not linearly separable.}

%%
\r{``projection pursuit regression'' --- similar to a two-layer feed-forward neural network --- typcially the parameters, rather than being all updated simultaneously such as in a nerual network ,are optimized cyclically in groups. Training takes place for one hidden unit at a time}
% another framework for non-linear regression
\r{``generalized additive models \TD{Hastie and Tibshirani, 1990} --- restrictive class of models since it does not allow for interactions between the input variables. === a generalization that does allow for interactions is the ``multi-variate adaptive regression splies (MARS) \TD{(Friedman, 1991)}}


%%
% this is pretty interesting... checkout p136 of NN_bishop, p152 tablet
\TD{Kolmogorov's Theorem}

%%
% p148(163) of NN Bishop
\TD{Jacobian Matrix}
% p150(165) of NN Bishop
\TD{Hessian Matrix}
% diagonal approximation
% outer product approximation
% inverse hessian
% finite differences
% exact evaluation of the hessian
\TD{Radial Basis Functions}


%% Look into
\TD{Max norm constraints --- enforce an absolute upper bound.}

%% related to backprop - used by tensorflow
\TD{automatic differentiation}


%% Unsupervised pretraining - 2006, Hinton
\TD{greedy layer-wise unsupervised pretraining. greedy since each portion of the network is trained independently. Typically, today, layers are trained jointly using backpropagation}

\subsection{Restricted Boltzmann Machines}
\r{Restricted Boltzmann machines (RBMs) -- no output layer}

\TD{Deep Belief Networks (DBN) --- RBMs linked together to form multistage neural netowrk. Each RBM generates a representation of the data that the subsequent layer builds upon.  --- may be used as feature detectors}



%%
\r{code resuse and \code{tf.reuse}}