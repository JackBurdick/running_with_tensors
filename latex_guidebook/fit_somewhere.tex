

\textcolor{blue}{tensorflow's XLA compiler}

\textcolor{blue}{checkpoints -- allow: save/stop/resume training, resume on failure, predict from point}


%%%% may talk about in production
% Google Cloud MLE


%%%
% stats vs ML -- in ML you may keep outliers and build models for them. in ML outliers may be collapsed (capped) and in statistics they may be removed
% ML is used to learn the ``long tail'', make fine grained predictions, not just gobal averages

%% why try to stay in linear (like with feature crosses)
% NN with many layers are non-convex
% optimizing linear models is a convex problem (much easier)

% how have I not talked about transfer learning yet?

% optimizing is an NP-hard, non-convex optimization problem (coursera.need to double check)
\textcolor{blue}{L0-norm (the count of non-zero weights).}


%%%%


%%%
\textcolor{blue}{Each layer in a DNN is a composition of the previous layer. i.e. if layer 1 = f(x), then layer 1 = g(f(x)), layer three is h(g(f(x)) \textcolor{green}{TODO: create diagram}}



%%%% rough...
%\textcolor{red}{Multi-headed inference. Useful when using the same model for different tasks. e.g. using a model for one task, then later deciding to perform a similar task on the same data -- rather than train an entirely new model, the original model may be performing may of the same computations. }


%%%%%% research
\textcolor{blue}{Pooling may not fully determine learned deformation stability -- possibly filter smoothness\cite{ruderman2018learned}}

%%%% % plus index - unsupervised method
\textcolor{blue}{manifold learning}

%%%  preprocessing, this wasn't already somewhere? % plus index
\textcolor{blue}{whitening}

%%% unsupervised method
\textcolor{blue}{NFM (Non-Negative Matrix Factorization)}

%%%
\textcolor{blue}{TODO: parameter calculation -- use VGG example (conv + dense)}
\begin{table}
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\multicolumn{5}{|c|}{\textbf{Number of VGG-16 Parameters}}     \\ \hline
		Layer & Out Shape & Weights & Bias & Total  \\ \hline
		\emph{Convolution}        & & $(in)\times(h\times w)\times(out)$ & $(out)$ & $weights+bias$    \\ \hline
		Conv3-64          & $224\times224\times64$ & $3\times(3\times3)\times64$ & $64$ & 1792    \\ \hline
		Conv3-64 (p)      & $112\times112\times64$ & $64\times(3\times3)\times64$ & $64$ & 36928    \\ \hline
		Conv3-128         & $112\times112\times128$ & $64\times(3\times3)\times128$ & $128$ & 73856     \\ \hline
		Conv3-128 (p)     & $56\times 56\times 128$ & $128\times(3\times3)\times128$ & $128$ & 147584   \\ \hline
		Conv3-256         & $56\times 56\times 256$ & $128\times(3\times3)\times256$ & $256$ & 295168   \\ \hline
		Conv3-256         & $56\times 56\times 256$ & $256\times(3\times3)\times256$ & $256$ & 590080   \\ \hline
		Conv3-256 (p)     & $28\times 28\times 256$ & $256\times(3\times3)\times256$ & $256$ & 590080   \\ \hline
		Conv3-512         & $28\times 28\times 512$ & $256\times(3\times3)\times512$ & $512$ & 1180160  \\ \hline
		Conv3-512         & $28\times 28\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808  \\ \hline
		Conv3-512 (p)     & $14\times 14\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808  \\ \hline
		Conv3-512         & $14\times 14\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808  \\ \hline
		Conv3-512         & $14\times 14\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808  \\ \hline
		Conv3-512 (p)     & $7\times 7\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808    \\ \hline
		\emph{dense}         & & $(in)\times(num)$ & $(out)$ & $weights+bias$    \\ \hline
		fc1 (4096)        & 4096 &$(512\times7\times7)\times4096$ & $4096$ & $102764544$    \\ \hline
		fc2 (4096)        & 4096 &$(4096)\times4096$ & $4096$ & $16781312$    \\ \hline
		fc3 (1000)        & 1000 &$(4096)\times1000$ & $1000$ & $4097000$    \\ \hline
		\emph{Total} & & & & 138,357,544 \\ \hline
	\end{tabular}
	\caption{Calculation of VGG parameters. (p) denotes that the layer is followed by a pooling layer (which does not affect the parameter count)}
	\label{tab:vgg_parameter_count}
\end{table}




%%%
\textcolor{green}{TODO: NN from scratch in appendix}

%%%
\textcolor{green}{TODO: CNN without layers API -- in github}

%%% interactive
\textcolor{blue}{The TensorFlow playground~\cite{tf_playground} (\textcolor{green}{TODO: screen shot}).  There are two other notable projects --- deeplearn.js~\cite{deeplearnjs} and ConvNetJS~\cite{convnet_js}.}


%%% distributed
\textcolor{blue}{Distributed TensorFlow Section}


%%% CUDA
\textcolor{blue}{CUDA (Compute Unified Device Architecture) library --- computation}
\textcolor{blue}{cuDNN (CUDA Deep Neural Network) library --- library of GPU-accelerated DNN primitives (activations, normalization, convolutions, pooling)}


%% model quantization
\textcolor{blue}{drop the parameter float precision from 32 bits (\code{tf.float32}) to 16 bits (\code{tf.bfloat16})}

%%%%
\textcolor{blue}{performance metrics --- Bayes error}

\textcolor{blue}{performance metrics --- Ability to perform on new, previously unseen data, generalization error (test error)}

\textcolor{blue}{Visible and Hidden Layers. Visible layers are called visible since they contain variables that are ``visible'', where as the hidden layers extract increasingly abstract features -- hidden since their values are not given in the data}

\textcolor{green}{TODO: figure similar to DL pipeline in my thesis and similar to figure 1.5 in DL book. DL automates feature engineering}


\textcolor{green}{MNIST described by Geoffrey Hinton as ``the drosophilia'' of machine learning. --- fruitflies are often used in biology as controlled laboratory experiments.}


\textcolor{green}{Two camps of statistics: frequentist estimators and bayesian inference.}


% TODO: figure -- [tensorflow ] -> cudnn -> cuda -> GPU | -> cuda -> GPU | -> [lib] -> CPU

