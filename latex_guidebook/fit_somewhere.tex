

\textcolor{blue}{tensorflow's XLA compiler}

\textcolor{blue}{checkpoints -- allow: save/stop/resume training, resume on failure, predict from point}

%%%%%%%%%%
\textcolor{blue}{apache beam}


%%%% may talk about in production
% Google Cloud MLE


%%%
% stats vs ML -- in ML you may keep outliers and build models for them. in ML outliers may be collapsed (capped) and in statistics they may be removed
% ML is used to learn the ``long tail'', make fine grained predictions, not just gobal averages

%% why try to stay in linear (like with feature crosses)
% NN with many layers are non-convex
% optimizing linear models is a convex problem (much easier)

% how have I not talked about transfer learning yet?

% optimizing is an NP-hard, non-convex optimization problem (coursera.need to double check)
\textcolor{blue}{L0-norm (the count of non-zero weights).}


%%%%
\textcolor{blue}{elastic nets --- combine the feature selection of L1 regularization with the generalizability of L2 regularization \textcolor{red}{CITE} \textcolor{green}{TODO: figure}. The trade off is that there are now two (instead of one for L1 or L2), hyper parameters to tune.}


%%%
\textcolor{blue}{Each layer in a DNN is a composition of the previous layer. i.e. if layer 1 = f(x), then layer 1 = g(f(x)), layer three is h(g(f(x)) \textcolor{green}{TODO: create diagram}}