

\textcolor{blue}{tensorflow's XLA compiler}

\textcolor{blue}{checkpoints -- allow: save/stop/resume training, resume on failure, predict from point}

%%%%%%%%%%
\textcolor{blue}{apache beam}


%%%% may talk about in production
% Google Cloud MLE


%%%
% stats vs ML -- in ML you may keep outliers and build models for them. in ML outliers may be collapsed (capped) and in statistics they may be removed
% ML is used to learn the ``long tail'', make fine grained predictions, not just gobal averages

%% why try to stay in linear (like with feature crosses)
% NN with many layers are non-convex
% optimizing linear models is a convex problem (much easier)

% how have I not talked about transfer learning yet?

% optimizing is an NP-hard, non-convex optimization problem (coursera.need to double check)
\textcolor{blue}{L0-norm (the count of non-zero weights).}


%%%%
\textcolor{blue}{elastic nets --- combine the feature selection of L1 regularization with the generalizability of L2 regularization \textcolor{red}{CITE} \textcolor{green}{TODO: figure}. The trade off is that there are now two (instead of one for L1 or L2), hyper parameters to tune.}


%%%
\textcolor{blue}{Each layer in a DNN is a composition of the previous layer. i.e. if layer 1 = f(x), then layer 1 = g(f(x)), layer three is h(g(f(x)) \textcolor{green}{TODO: create diagram}}



%%%%
\textcolor{blue}{SavedModel is the universal serializion format for TensorFlow models. SavedModel has support for multiple metagraphs -- this is important in serving where the model is slightly different than in training (removing dropout layers), and allows storage/access of models with tags. Supports SignatureDefs -- allows the specification of nodes as input/output (also supports multiple signatureDefs for multi-headed inference (see \textcolor{red}{local ref}))}


%%%% rough...
%\textcolor{red}{Multi-headed inference. Useful when using the same model for different tasks. e.g. using a model for one task, then later deciding to perform a similar task on the same data -- rather than train an entirely new model, the original model may be performing may of the same computations. }


%%%%%%
\textcolor{blue}{Pooling may not fully determine learned deformation stability -- possibly filter smoothness\cite{ruderman2018learned}}