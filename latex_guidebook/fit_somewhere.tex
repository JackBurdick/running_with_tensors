

\textcolor{blue}{tensorflow's XLA compiler}

\textcolor{blue}{checkpoints -- allow: save/stop/resume training, resume on failure, predict from point}


%%%% may talk about in production
% Google Cloud MLE


%%%
% stats vs ML -- in ML you may keep outliers and build models for them. in ML outliers may be collapsed (capped) and in statistics they may be removed
% ML is used to learn the ``long tail'', make fine grained predictions, not just gobal averages

%% why try to stay in linear (like with feature crosses)
% NN with many layers are non-convex
% optimizing linear models is a convex problem (much easier)

% how have I not talked about transfer learning yet?

% optimizing is an NP-hard, non-convex optimization problem (coursera.need to double check)
\textcolor{blue}{L0-norm (the count of non-zero weights).}


%%%%


%%%
\textcolor{blue}{Each layer in a DNN is a composition of the previous layer. i.e. if layer 1 = f(x), then layer 1 = g(f(x)), layer three is h(g(f(x)) \textcolor{green}{TODO: create diagram}}



%%%% rough...
%\textcolor{red}{Multi-headed inference. Useful when using the same model for different tasks. e.g. using a model for one task, then later deciding to perform a similar task on the same data -- rather than train an entirely new model, the original model may be performing may of the same computations. }


%%%% % plus index - unsupervised method
% see p156 of DL for more
\textcolor{blue}{manifold learning --- {manifold}\index{manifold}, though having a more formal mathematical meaning, will be considered a conected region for our machine learning purposes.}


%%%  preprocessing, this wasn't already somewhere? % plus index
\textcolor{blue}{whitening}

%%% unsupervised method
\textcolor{blue}{NFM (Non-Negative Matrix Factorization)}

%%%
\textcolor{blue}{TODO: parameter calculation -- use VGG example (conv + dense)}
\begin{table}
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\multicolumn{5}{|c|}{\textbf{Number of VGG-16 Parameters}}     \\ \hline
		Layer & Out Shape & Weights & Bias & Total  \\ \hline
		\emph{Convolution}        & & $(in)\times(h\times w)\times(out)$ & $(out)$ & $weights+bias$    \\ \hline
		Conv3-64          & $224\times224\times64$ & $3\times(3\times3)\times64$ & $64$ & 1792    \\ \hline
		Conv3-64 (p)      & $112\times112\times64$ & $64\times(3\times3)\times64$ & $64$ & 36928    \\ \hline
		Conv3-128         & $112\times112\times128$ & $64\times(3\times3)\times128$ & $128$ & 73856     \\ \hline
		Conv3-128 (p)     & $56\times 56\times 128$ & $128\times(3\times3)\times128$ & $128$ & 147584   \\ \hline
		Conv3-256         & $56\times 56\times 256$ & $128\times(3\times3)\times256$ & $256$ & 295168   \\ \hline
		Conv3-256         & $56\times 56\times 256$ & $256\times(3\times3)\times256$ & $256$ & 590080   \\ \hline
		Conv3-256 (p)     & $28\times 28\times 256$ & $256\times(3\times3)\times256$ & $256$ & 590080   \\ \hline
		Conv3-512         & $28\times 28\times 512$ & $256\times(3\times3)\times512$ & $512$ & 1180160  \\ \hline
		Conv3-512         & $28\times 28\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808  \\ \hline
		Conv3-512 (p)     & $14\times 14\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808  \\ \hline
		Conv3-512         & $14\times 14\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808  \\ \hline
		Conv3-512         & $14\times 14\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808  \\ \hline
		Conv3-512 (p)     & $7\times 7\times 512$ & $512\times(3\times3)\times512$ & $512$ & 2359808    \\ \hline
		\emph{dense}         & & $(in)\times(num)$ & $(out)$ & $weights+bias$    \\ \hline
		fc1 (4096)        & 4096 &$(512\times7\times7)\times4096$ & $4096$ & $102764544$    \\ \hline
		fc2 (4096)        & 4096 &$(4096)\times4096$ & $4096$ & $16781312$    \\ \hline
		fc3 (1000)        & 1000 &$(4096)\times1000$ & $1000$ & $4097000$    \\ \hline
		\emph{Total} & & & & 138,357,544 \\ \hline
	\end{tabular}
	\caption{Calculation of VGG parameters. (p) denotes that the layer is followed by a pooling layer (which does not affect the parameter count)}
	\label{tab:vgg_parameter_count}
\end{table}




%%%
\textcolor{green}{TODO: NN from scratch in appendix}

%%%
\textcolor{green}{TODO: CNN without layers API -- in github}

%%% interactive
\textcolor{blue}{The TensorFlow playground~\cite{tf_playground} (\textcolor{green}{TODO: screen shot}).  There are two other notable projects --- deeplearn.js~\cite{deeplearnjs} and ConvNetJS~\cite{convnet_js}.}


%%% distributed
\textcolor{blue}{Distributed TensorFlow Section}


%%% CUDA
\textcolor{blue}{CUDA (Compute Unified Device Architecture) library --- computation}
\textcolor{blue}{cuDNN (CUDA Deep Neural Network) library --- library of GPU-accelerated DNN primitives (activations, normalization, convolutions, pooling)}


%% model quantization
\textcolor{blue}{drop the parameter float precision from 32 bits (\code{tf.float32}) to 16 bits (\code{tf.bfloat16})}

%%%%
\textcolor{blue}{performance metrics --- Bayes error}

\textcolor{blue}{performance metrics --- Ability to perform on new, previously unseen data, generalization error (test error)}


\textcolor{green}{TODO: figure similar to DL pipeline in my thesis and similar to figure 1.5 in DL book. DL automates feature engineering}


\textcolor{green}{MNIST: described by Geoffrey Hinton as ``the drosophilia'' of machine learning. --- fruitflies are often used in biology as controlled laboratory experiments. Also considered the ``hello world'' of deep learning. The dataset is created from grayscale images ($28 \times 28 \times 1$) and has 10 labels 0-9.  There exist 60,000 training images and 10,000 test images. Created by the National Institute of Standards and Technology.}


\textcolor{green}{Two camps of statistics: frequentist estimators and bayesian inference.}


% TODO: figure -- [tensorflow ] -> cudnn -> cuda -> GPU | -> cuda -> GPU | -> [lib] -> CPU


%%
\textcolor{blue}{nonstationary problems --- unsolvable --- need data that makes sense for the problem. Predicting rentals of snowmobiles doesn't make as much sense if you only have data from summer. predicting cloth purchases during summer doesn't make sense if you only have data from winter.}

%% not sure how this wasn't mentioned yet
\textcolor{green}{TODO: transfer learning -- explanation, implementation in TensorFlow, and in YamlFlow \r{if general enough, the lower features may be reusable}}

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{Figure example layer hierarchy and where/when to transfer/freeze params -- this will be 1-2 figures and include many sub-figures \textcolor{green}{TODO}}
	\label{fig:transfer_learning_subfigs_a}
\end{figure}

\textcolor{green}{{freezing}\index{freezing} parameters or a layer means preventing the parameters from being updated during training. This is often controlled by a parameter called ``trainable'' see \textcolor{red}{local TF ref} for the TensorFlow implementation and \textcolor{red}{local YF ref} for the YamlFlow implementation. In relation to transfer learning and freezing, mention the difficulty of propagating updates though a large network}

%%%
\textcolor{green}{Visualizing the output of a CNN}

\begin{enumerate}
	\item Filters: \r{help understand what visual patter a filter is receptive to \textcolor{green}{TODO: lots of examples and refs to implementations}}
	\item Intermediate outputs: \r{help understand the hierarchy of what is important to the classification task \textcolor{green}{TODO: lots of examples and refs to implementations}}
	\item Heatmaps of activation: \r{Help understand what parts (and by relatively how much) of an image were in its classification. \textcolor{green}{TODO: lots of examples and refs to implementations}}
\end{enumerate}


%%
\r{NOTE: somewhere about terms 'higher' and 'lower' levels of an architecture and how they're meaningless/interchangable and depend on context e.g. a diagram vs concept of lower=being closer to raw input.}


