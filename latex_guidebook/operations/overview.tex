% TODO: I'm still not sure how/where to structure this

\section{Dense}

\TD{TODO}

\section{Convolutions}

\TD{TODO}

\section{Pooling}

\TD{TODO}

\section{Recurrent Cells}

\TD{TODO}

\section{Capsule Networks}

\TD{TODO}

\section{Attention}

\r{overview can be found here\cite{weng2018attention}}

\TD{The original attention mechanism is introduced\cite{Bahdanau2015NeuralMT}.}

% TF attention implementation (https://www.tensorflow.org/tutorials/text/nmt_with_attention)

\TD{Effective Approaches to Attention-based Neural Machine Translation \cite{DBLP:journals/corr/LuongPM15}}

\TD{Massive Exploration of Neural Machine Translation Architectures \cite{DBLP:journals/corr/BritzGLL17}}

% TODO: index for transformer
\TD{Attention Is All You Need -- Transformer network --- multi-head self-attention mechanism, key-value pairs \cite{DBLP:journals/corr/VaswaniSPUJGKP17}}

% self-attention \TD{Self-attention, less commonly intra-attention}
\TD{Long Short-Term Memory-Networks for Machine Reading \cite{DBLP:journals/corr/ChengDL16}}


%\TD{Nice table comparing mechanisms https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html}

\TD{in above post\cite{weng2018attention}: soft vs hard attention and global vs local attention}

\subsection{transformers}


% Factorized Attention to self-attention
\TD{Generating Long Sequences with Sparse Transformers \cite{DBLP:journals/corr/abs-1904-10509}}

% include reccurence:  "enables learning dependency beyond a fixed length" + "relative position encodings"
\TD{Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context \cite{DBLP:journals/corr/abs-1901-02860}}

% extends DBLP:journals/corr/abs-1901-02860 -- 
\TD{Compressive Transformers for Long-Range Sequence Modeling \cite{Rae2020CompressiveTF}}

% linear attention
\TD{Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention \cite{Katharopoulos2020TransformersAR}}

% TODO: top-down attention
% related to self-attention
% https://twitter.com/thomaskipf/status/1277570203665170432
\TD{Object-Centric Learning with Slot Attention \cite{Locatello2020ObjectCentricLW}}

\TD{Recurrent Independent Mechanisms \cite{Goyal2019RecurrentIM}}

% DETR
\TD{End-to-End Object Detection with Transformers \cite{Carion2020EndtoEndOD}}


% TODO: read https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html


\section{NTM (Neural Turing Machines)}
% nerual network + external memory storage
% controller + memory
\TD{Neural Turing Machines \cite{DBLP:journals/corr/GravesWD14}}
