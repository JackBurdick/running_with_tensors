% TODO: I'm still not sure how/where to structure this

\section{Dense}

\TD{TODO}

\section{Convolutions}

\TD{TODO}

\section{Pooling}

\TD{TODO}

\section{Recurrent Cells}

\TD{TODO}

\section{Capsule Networks}

\TD{TODO}

\section{Attention}

\r{overview can be found here\cite{weng2018attention}}

\TD{The original attention mechanism is introduced\cite{Bahdanau2015NeuralMT}.}

% TF attention implementation (https://www.tensorflow.org/tutorials/text/nmt_with_attention)

\TD{Effective Approaches to Attention-based Neural Machine Translation \cite{DBLP:journals/corr/LuongPM15}}

\TD{Massive Exploration of Neural Machine Translation Architectures \cite{DBLP:journals/corr/BritzGLL17}}

% TODO: index for transformer
\TD{Attention Is All You Need -- Transformer network --- multi-head self-attention mechanism, key-value pairs \cite{DBLP:journals/corr/VaswaniSPUJGKP17}}

% self-attention \TD{Self-attention, less commonly intra-attention}
\TD{Long Short-Term Memory-Networks for Machine Reading \cite{DBLP:journals/corr/ChengDL16}}


%\TD{Nice table comparing mechanisms https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html}

\TD{in above post\cite{weng2018attention}: soft vs hard attention and global vs local attention}


% TODO: read https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html


\section{NTM (Neural Turing Machines)}
% nerual network + external memory storage
% controller + memory
\TD{Neural Turing Machines \cite{DBLP:journals/corr/GravesWD14}}
