\section{Optimizers}

\textcolor{blue}{An optimizer is shorthand for an optimization algorithm that is used to minimize the objective function (cost function).}
	
\textcolor{blue}{One advantage of using a \textcolor{red}{continuous} activation function \textcolor{red}{see more at local ref?} is that the function is differentiable at all points. }

\subsection{Overview}

\textcolor{blue}{Gradient Descent -- overview}

\textcolor{blue}{batch gradient descent -- taking a step (update the weights) opposite (down) the gradient calculated from the entire training set}

\textcolor{blue}{Stochastic Gradient Descent (sometimes called iterative or on-line gradient descent) -- rather than update the weights based on the sum of the accumulated errors, the weights are updated for each training sample}

\textcolor{blue}{mini-batch learning -- compromise between batch and stochastic gradient descent where the gradient is calculated over a batch of training data}

\textcolor{blue}{Since the gradient is calculated on a single example, the error surface will appear noisier than if it was calculated over a batch or the entire training set.}

\textcolor{blue}{When using stochastic gradient descent, it is important to shuffle the data after each epoch.}

\subsection{What are optimizers}

\subsection{Types of optimizers}

\subsection{TF optimizers}

tf.train.GradientDescentOptimizer

tf.train.MomentumOptimizer

tf.train.RMSPropOptimizer

tf.train.AdadeltaOptimizer

tf.train.AdagradOptimizer

tf.train.AdagradDAOptimizer

tf.train.AdamOptimizer

tf.train.FtrlOptimizer

tf.train.ProximalGradientDescentOptimizer

tf.train.ProximalAdagradOptimizer

\subsection{Advanced}

\subsubsection{Gradient Computation}

\subsubsection{Gradient Clipping}