\section{Optimizers}

\textcolor{blue}{An optimizer is shorthand for an optimization algorithm that is used to minimize the objective function (cost function).}
	
\textcolor{blue}{One advantage of using a \textcolor{red}{continuous} activation function \textcolor{red}{see more at local ref?} is that the function is differentiable at all points. }

\subsection{What are optimizers}

\subsection{Types of optimizers}

\subsection{TF optimizers}

tf.train.GradientDescentOptimizer

tf.train.MomentumOptimizer

tf.train.RMSPropOptimizer

tf.train.AdadeltaOptimizer

tf.train.AdagradOptimizer

tf.train.AdagradDAOptimizer

tf.train.AdamOptimizer

tf.train.FtrlOptimizer

tf.train.ProximalGradientDescentOptimizer

tf.train.ProximalAdagradOptimizer

\subsection{Advanced}

\subsubsection{Gradient Computation}

\subsubsection{Gradient Clipping}