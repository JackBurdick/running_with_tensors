\subsection{Optimizers}

\textcolor{blue}{An optimizer is shorthand for an optimization algorithm that is used to minimize the objective function (cost function).}
	
\textcolor{blue}{One advantage of using a \textcolor{red}{continuous} activation function \textcolor{red}{see more at local ref?} is that the function is differentiable at all points. }

\subsubsection{Overview}

\subsubsection{What are optimizers}

\subsubsection{Types of optimizers}

\subsubsection{TF optimizers}

tf.train.GradientDescentOptimizer

tf.train.MomentumOptimizer

tf.train.RMSPropOptimizer

tf.train.AdadeltaOptimizer

tf.train.AdagradOptimizer

tf.train.AdagradDAOptimizer

tf.train.AdamOptimizer

tf.train.FtrlOptimizer

tf.train.ProximalGradientDescentOptimizer

tf.train.ProximalAdagradOptimizer

\subsubsection{Advanced}

\paragraph{Gradient Computation}

\paragraph{Gradient Clipping}