\section{Deployment with TF Serving}

\subsection{Overview}
% https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/architecture_overview.md
% https://www.youtube.com/watch?v=q_IkJcPyNl0

\subsubsection{Standard Abstractions}

\textcolor{blue}{Core components with APIs}

\paragraph{ServableHandle}

\paragraph{Manager}

\textcolor{blue}{Uses the Loader to load and unload a model}

\paragraph{Loader}

\textcolor{blue}{Loader for a TensorFlow <saved\_model>}

\textcolor{red}{the loader knows how to load a model and knows how to estimate the resources such as RAM or GPU}

\textcolor{red}{The loader signals to the manager that has an `aspired version' (ready for loading). The manager will then decide when to load this new version based on the version policy plugin (see below).}

\paragraph{Source}


\subsubsection{Plugins into Abstractions}


\paragraph{File System (Source)}

\textcolor{blue}{Monitors the file system.}


\paragraph{Version Policy (Manager)}

\textcolor{blue}{Preserve availability or preserve resources. Preserving availability might be more important in a live, user facing scenario and will keep keep one model loaded, load another beside, then point client to new model -- There's no down time, but more resources are consumed (two models loaded at once).  Preserving resources might be important if using a large model on a resource constrained environment or in an internal application where some downtime is considered acceptable. Under a policy preserving resources, the current model will be unloaded and a new model will be loaded -- cost is a slight hiccup in service, but the benefit is a saving on resources (like memory) since there's only the one model loaded at a time.}

\textcolor{red}{When loading a new model, the original/old model can't be immediately unloaded/deleted in case there are pending/queued jobs. TensorFlow keeps track of these jobs via ref-counting, and only then removes that model once all the jobs have completed}


\subsubsection{FIT}

\paragraph{ServerCore}

\textcolor{blue}{declare set of models to be loaded, pass them to ServerCore, and server core returns a manager of these models with the best practices out of the box.}


\paragraph{Binaries and APIs}

\subparagraph{Predict}

% coming soon?
%\subparagraph{Regress}

%\subparagraph{Classify}

%\subparagraph{MultiInference}

\paragraph{Servables}

\textcolor{blue}{The central abstraction to TensorFlow Serving -- the underlying object that will be used for inference.}

\textcolor{blue}{Servables may be big and complex (composite inference model) to small and simple (lookup table). }

\textcolor{red}{A composite model can be represented as either multiple independent servables or a single composite servable.}

\paragraph{Loaders}

\textcolor{blue}{Statdarize the API for loading and unloading a servable i.e. manage the servable's lifecycle.}

%\paragraph{Sources}
%\textcolor{blue}{}




\subsection{Example}