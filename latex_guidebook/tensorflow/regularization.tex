\subsection{Regularization}

% TODO: this section/file may belong in basics, not here

\textcolor{blue}{Collection of techniques used to help generalize a model -- which may help prevent overfitting. Typically regularization penalizes complexity of a model.}

% TODO: figure of loss plot showing a steep training and shallow+divergent val/test loss

\textcolor{blue}{Helps prevent the model from memorizing noise in the training data.}


\subsubsection{Why Regularization}

\textcolor{blue}{Overfitting --- too complex --- Occam's razor --- hypothesis with the fewest assumptions is best}


\subsubsection{Types of Regularization}

\textcolor{blue}{Regularization is an active area of research.}

\begin{itemize}
	\item Early Stopping
	\item Parameter Norm Penalties
	\begin{itemize}
		\item L1 regularization
		\item L2 regularization
	\end{itemize}
	\item Dataset Augmentation
	\item Noise Robustness
	\item Sparse Representations
	\item Dropout
\end{itemize}


\textcolor{blue}{L1 regularization (Lasso) L2 (Ridge).}

\textcolor{blue}{key difference is the penalty term}


\subsubsection{Regularization Methods and Implementations}

\textcolor{green}{TODO: DIGRAM OF L2 + L1 + elastic nets}

\paragraph{L2 Regularization}

\textcolor{green}{TODO: DIAGRAM OF L2}

\textcolor{blue}{L2, ({Ridge regression}\index{Ridge regression}) may also be known as {Tikhonov regularization}\index{Tikhonov regularization}}

\textcolor{blue}{penalizes model parameters that become too large. Will force most of the parameters to be small, but still non-zero}

% p91(71) of mastering ML w SKL says "when lambda is equal to zero, ridge regression is equal to linear regression"

\paragraph{L1 Regularization}

\textcolor{green}{TODO: DIAGRAM OF L1}

\textcolor{blue}{LASSO (Least Absolute Shrinkage and Selection Operator) --- produces sparse parameters. This will force coefficients to zero and cause the model to depend on a small subset of the features.}

\textcolor{blue}{It could be argued that using L1 regularization may help to make a mode more interpretable, by using less (presumably more important/relevant) features when making predictions.}


\paragraph{Elastic Net Regularization}

\textcolor{blue}{Linearly combines the $L^1$ (feature selection) and $L^2$ (generalizability) penalties used by both LASSO and ridge regression. The cost is having two parameters (as opposed to just one when using either L1 or L2).}

\textcolor{green}{TODO: figure}.


\paragraph{Dropout}

% TODO: explain dropout

% helps learn ``multiple paths''/simulates ensembles


% TODO: this may not belong here...
\subsubsection{Normalization}

\textcolor{blue}{TODO: overview para + importance}

\textcolor{green}{TODO: figure showing differences}

\paragraph{Instance normalization}

\textcolor{blue}{see section in preprocessing \textcolor{red}{local ref?}}

\paragraph{Layer normalization}

\paragraph{Batch normalization}

\paragraph{Group normalization}