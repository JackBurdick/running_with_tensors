\chapter{Foundational Methods}

%% Maybe (Foundational Methods --- supervised)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Regression
\input{./foundations/regression}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Logistic Regression
\input{./foundations/logistic_regression}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% KNN
\input{./foundations/nearest_neighbor}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Support Vector Machines
\input{./foundations/svm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Naive Bayes
\input{./foundations/naive_bayes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Decision Trees
\input{./foundations/decision_trees}


\chapter{Artificial Neural Networks}

\textcolor{blue}{If a perceptron is analogous to a single neuron, an artificial neural network (either feedforward or feedback) would be analogous to a brain.}

\r{powerful and general framework for representing non-linear mappings (function approximation) from input features to outputs, where the form of the mapping is controlled by adjustable parameters (weights and biases). Determining the values for these parameters is the ``learning'' or training.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% perceptron
\input{./foundations/perceptron}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% overview
\input{./foundations/ann_overview}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% feedforward
\input{./foundations/feedforward}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% feedback
\input{./foundations/feedback}


\chapter{Unsupervised}

\r{The reality is that data is not always labeled.}

\r{unsupervised learning is capable of finding hidden patterns in the underlying structure of hte data.}

\r{attempts to represent data with increaingly fewer parameters}

\textcolor{blue}{Discovering hidden structures or patterns in unlabeled training data.}

\TD{Neighborhood-Based Methods \ref{nearest_neighbors} \r{lazy learners  -- learn how to label new instances based on proximity to existing instances}}

% TODO: placement / may need to rename+restructure sections
\textcolor{blue}{unsupervised methods may be commonly used in two main settings:}
\begin{enumerate}[noitemsep,topsep=0pt]
	\item Data Exploration
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Visualization (clustering \textcolor{red}{local ref})
	\end{itemize}
	\item Preprocessing (e.g. prior to a supervised method): unsupervised pretraining may be considered a form of regularization
	\begin{itemize}[noitemsep,topsep=0pt]
		\item Compressing (dimensionality reduction \textcolor{red}{local ref})
		\item Creating new/different representations
	\end{itemize}
\end{enumerate}

r{regularization, feature engineering, detecting outliers -- also used for detecting how different new (incoming) training data is from the current distribution.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% clustering
\input{./foundations/unsupervised/clustering}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Dimensionality Reduction
\input{./foundations/unsupervised/dimensionality_reduction}


\chapter{Semi-supervised}

\input{./foundations/semi_supervised}
