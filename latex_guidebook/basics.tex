\chapter{Basics}

\section{Overview}

%%%%%%%%%%%%%%%%%%%%%%%% obligatory "No Free Lunch"
I'm not sure it's possible to discuss machine learning without at least mentioning the ``No Free Lunch'' theorem, which states ``No single classifier works best across all possible scenarios''

%%%%%%%%%%%%%%%%%%%%%%%% Types of data
\textcolor{blue}{categorical or numerical. Numerical can be discrete or continuous}

%%%%%%%%%%%%%%%%%%%%%%%% Measurement Levels
\textcolor{blue}{qualitative or quantitative.} 

\textcolor{blue}{Qualitative can be nominal (aren't numbers and can't be put in any order -- e.g. the seasons: spring, summer, fall, winter) or ordinal (groups and categories that follow a strict order -- e.g. difficult levels: hard, medium, or easy)}

\textcolor{blue}{Quantitative are represented by numbers but can be interval (0 is meaningless -- e.g. temperature in C or F, where true zero is not 0) or ratio (has a true 0 -- e.g. temperature in K, weight or length)}


%%%%%%%%%%%%%%%%%%%%%%%% Acquiring Data
\input{./nested/basics/acquiring_data}


%%%%%%%%%%%%%%%%%%%%%%%% Data Pre-processing
\input{./nested/basics/data_preprocessing}


%%%%%%%%%%%%%%%%%%%%%%%% Data sampling and partitioning
\input{./nested/basics/sampling_partitioning}

\section{Some Terms}

\emph{input variable(s)} -- predictors, independent variables, features, regressors, controlled variables, exposure variables or simply variables.
 
\emph{output variable(s)} -- response or dependent variable. May also be known as regressands, criterion variables, measured variables, responding variables, explained variables, outcome variables, experimental variables, labels.

\textcolor{blue}{Both input and output variables may take on continuous or discrete values.}

\emph{relationship} $Y = f(x) + \epsilon$ \textcolor{blue}{estimate $f$. prediction and inference}.

\textcolor{blue}{\emph{reducible error\index{reducible error}} -- the estimated function $\hat{f}$ will likely not be perfect, and the reducible error is the error that could be corrected.  The \emph{irreducible error} is an error that can not be corrected. The irreducible error may be larger than zero due to \emph{unmeasured variables} \emph{e.g.} varibles that were not measured and \emph{unmeasurable variation} \emph{e.g.} an individual's feelings/emotions or variation in the production of a product. The irreducible error provides an upper bound on the performance of the predicted $\hat{f}$}

\section{Type of Learning}

\textcolor{blue}{Three main types of machine learning: supervised, unsupervised, and reinforcement.}

% From ML for Predictive Data Analytics
\textcolor{blue}{Another way to group types of learning -- Information-based, similarity-based, probability-based, and error-based}

\subsection{Supervised}

\textcolor{blue}{observe input variables with corresponding output values. A program that predicts an output for a in input by learning from pairs of labeled inputs and outputs. Classification \textcolor{red}{ref} and regression \textcolor{red}{ref} are subcategories of supervised learning}

\subsection{Unsupervised}

\textcolor{blue}{observe input variables without corresponding output values and attempts to discover patterns in the data.}

% p14 of mastering ml agorithms
\textcolor{blue}{There is no error signal to measure, rather, performance metrics report some attribute of structure discovered in the data, such as the distances within and between clusters.}
 
% clustering
\subsubsection{Clustering}

\textcolor{blue}{Finding sub groups where observations are more similar to eachother based on some similarity measure. Clustering is sometimes referred to as ``unsupervised classification'' and is often used to explore a dataset.}

\textcolor{blue}{An example of clustering may be to group a collection of documents into categories, or songs into genres.}

% principal components
\subsubsection{Dimensionality Reduction}

\textcolor{blue}{where the goal is to reduce the dimensionality of the data while retaining as much as the relevant information as possible}

\textcolor{blue}{A high number of features may be computationally costly. Ability to generalize may be reduced if some of the features capture noise or are irrelevant to the underlying relationship. The goal could be to find the features that account for the greatest changes in the response variable}


\subsection{Semi-supervised Learning}

\textcolor{blue}{`semi-supervised learning', another type of learning, makes use of both supervised and unsupervised data.}

\subsection{Reinforcement}

\textcolor{blue}{Reinforcement learning does not learn from labeled pairs of inputs and outputs, rather it learns from `feedback' from decisions that are not explicitly corrected.}

\textcolor{blue}{Goal -- develop an \emph{agent} that improves it's performance based on interactions with an \emph{environment} based on a \emph{reward}}

\subsection{Supervised vs Unsupervised}

\subsection{Classification vs Regression}

\subsubsection{Regression} -- Also called regression analysis \textcolor{red}{local ref?} predicting a continuous or quantitative output value. For example attempting to find a relationship between a given predictor/explainatory variables (age, job title, zip code) and a continuous response (an individuals outcome).

\subsubsection{Classification} -- predicting categorical (discrete) or qualitative output value (such as a non-numerical value). \textcolor{blue}{Binary classification (benign vs malignant) and multi-class classification (identifying many different skin diseases).}

% page 94 of AGtext
One-versus-all \emph{OvA} (also \emph{one-versus-rest}) -- 

One-versus-one (OvO) -- train a binary classifier for every pair


\textcolor{blue}{Binary classification can be extended to multi-class classification via the OvR method.}

%\subsubsection{Bayes Classifier}

\section{Training}

\section{Quality of Fit}

%% regression example

\subsection{Regression Example}

\textcolor{blue}{Mean Squared Error.$\hat{f}(x_i)$ is the prediction that $\hat{f}$ produces for the $i$th sample. The output will be small for predicted values that are similar to the ground truth}

\begin{equation}
{MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{f}(x_i))^2}
\label{eq:MSE_def}
\end{equation}

%% classification example

\subsection{Classification Example}

\textcolor{blue}{The proportion of mistakes that are made.}

\begin{equation}
{error\_rate = \frac{1}{n}\sum_{i=1}^{n}(y_i \ne \hat{y_i})}
\label{eq:class_error_rate_def}
\end{equation}

\textcolor{blue}{$\hat{y_i}$ is the predicted classification label for the $i$th observation using our predictor/model $\hat{f}$ and $y_i$ is the ground truth label}

\section{(Over|Under)fitting}

\subsection{Overfitting}

\textcolor{blue}{Overfitting\index{Overfitting} refers to a case in which a model fits the training data very well but does not fit validation/test set. If a model is overfitting, it is said to have a high variance and is analogous to memorizing the training set.}

\textcolor{blue}{Overfitting can arise from modeling data with too many parameters/too complex of a model.}

\textcolor{green}{TODO: figure showing an example of overfitting}

\subsection{Underfitting}

\textcolor{blue}{Underfitting\index{Underfitting} refers to a case in which a model does not fit the training data well. If a model is underfitting, it is said to have a high bias}

\textcolor{blue}{Underfitting can arise from modeling data with too few parameters/too simple of a model.}

\textcolor{green}{TODO: figure showing an example of underfitting}


\section{Bias Variance Trade-off}

\textcolor{blue}{Two fundamental causes of prediction error in a model -- the bias and the variance.}

\subsection{Variance}
\textcolor{blue}{variance\index{Variance} refers to the amount the model would change (consistency or variability) if it was re-trained/estimated multiple using a different subsets of the training data set. A model that has high variance is sensitive to randomness in the training data}

\textcolor{blue}{A model with high variance may be described as highly flexible and will likely overfit the data.}


\subsection{Bias}
\textcolor{blue}{Bias\index{Bias} refers to the amount of error that is introduced by approximating a problem with a model that is simpler than the (complex) problem}

\textcolor{blue}{A model with high bias will produce similar errors for instances regardless of the training data that is used to train the model -- the model is more strongly ``biased'' to its own assumptions of the relationship (as defined by the model), than the relationship the data may be indicating. A model with high bias may also be described as inflexible and will likely underfit the data.}


% not word-for-word, but example adapted from p35 of ISL
\textcolor{red}{For example, linear regression assumes a linear relationship between the features and labels. However, it is unlikely that a true linear relationship exists and so using linear regression to model this type of particular problem will likely introduce some bias.}

\subsection{Trade-Off}

% TODO: see page 34 of ISL for eq and explaination here

\textcolor{blue}{In general, as a more ``flexible'' model is used, the variance will increase and the bias will decrease.}


% see page 36 of ISL
\textcolor{blue}{It is easy to obtain a model with low bias but high variance (\emph{e.g.} drawing a squiggly line through every training observation) and it is easy to obtain a model with low variance but high bias (\emph{e.g.} drawing a straight line approximating every training observation) but it is difficult to obtain a model that has both low variance and low bias.}

\textcolor{blue}{It should be noted that in a real world example, it maynot be possible to explicitly calculate the test error, bias, or variance.}

\textcolor{green}{TODO: para about using regularization here/finding the right balance \textcolor{red}{local ref to regularization?}}

\subsection{Parametric vs non-parametric}

\subsubsection{parametric}

\textcolor{blue}{parametric models are models that learn a fixed number of parameters, independent from the number of training instances, that able to classify new data points without requiring the original dataset anymore. First, a function form is selected (linear, polynomial, etc.), then the coefficients for the function are learned form the training data.}
	
\textcolor{blue}{Examples of parametric models may be simple artificial neural networks, naive bayes, logistic regression, etc.}

\subsubsection{nonparametric}

%% unsure about this! 
\textcolor{red}{Nonparametric models are not models without parameters, rather they are models were the number of parameters are not fixed, they may grow with the number of training instances}

\textcolor{blue}{An Example of a nonparametric model may be k-Nearest neighbors -- where the model does not assume anything about the form of the mapping function and makes predictions based on the k most similar training instances.}

\textcolor{blue}{A disadvantage to this type of approach is that the computational complexity for classifying new samples grow linearly with the number of samples in the training set.}

\textcolor{blue}{May be useful when little is known about the underlying relationship in the data and there is an abundance of data.}

\subsection{Eager vs Lazy Learners}

\textcolor{green}{TODO: Eager vs Lazy overview}
\textcolor{blue}{Training an eager learner is often more computationally expensive, but typically prediction with the resulting model is inexpensive.}

\subsubsection{Eager Learners}

\textcolor{blue}{Eager learners estimate the parameters of a model that generalize to a training set}

\subsubsection{Lazy Learners}

\textcolor{blue}{Also known as Instance-based Learners}

\textcolor{blue}{Lazy learners store the training dataset with little to no processing.}


\subsection{Generative vs Discriminative Models}

\textcolor{green}{TODO: Generative vs Discriminative models overview}
%\textcolor{blue}{}

\subsubsection{Generative Models}

\textcolor{green}{TODO: Generative Models}

\subsubsection{Discriminative Models}

\textcolor{green}{TODO: Discriminative Models}

\subsection{Strong vs Weak Learners}

\textcolor{green}{TODO: Strong vs Weak learners (classifier, predictor, etc.) overview}
%\textcolor{blue}{}

\subsubsection{Strong Learners}

\textcolor{green}{TODO: Strong Learners are models that are arbitrarily better than weak learners.}

\subsubsection{Weak Learners}

\textcolor{green}{TODO: Weak Learners are models (typically simple models) that perform only slightly better than random chance. }


\section{Online Learning}

% See p.246 of Understanding Machine learning
\textcolor{blue}{difference to \textcolor{red}{PAC learning?}}

\section{Ensemble Methods}

\textcolor{green}{TODO: overview - discussed in more detail in \textcolor{red}{local ref?}}



%%%%%%%%%%%%%%%%%%%%%%%% Evaluation
\input{./nested/basics/evaluation}


%%%%%%%%%%%%%%%%%%%%%%%% Metrics
\input{./nested/basics/metrics}

