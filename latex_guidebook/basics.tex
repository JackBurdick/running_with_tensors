\chapter{Basics}

%%%%%%%%%%%%%%%%%%%%%%%% obligatory "No Free Lunch"
I'm not sure it's possible to discuss machine learning without at least mentioning the ``No Free Lunch'' theorem, which states ``No single classifier works best across all possible scenarios''

%%%%%%%%%%%%%%%%%%%%%%%% Types of data
\textcolor{blue}{categorical or numerical. Numerical can be discrete or continuous}

%%%%%%%%%%%%%%%%%%%%%%%% Measurement Levels
\textcolor{blue}{qualitative or quantitative.} 

\textcolor{blue}{Qualitative can be nominal (aren't numbers and can't be put in any order -- e.g. the seasons: spring, summer, fall, winter) or ordinal (groups and categories that follow a strict order -- e.g. difficult levels: hard, medium, or easy)}

\textcolor{blue}{Quantitative are represented by numbers but can be interval (0 is meaningless -- e.g. temperature in C or F, where true zero is not 0) or ratio (has a true 0 -- e.g. temperature in K, weight or length)}


%%%%%%%%%%%%%%%%%%%%%%%% Acquiring Data
\input{./nested/basics/acquiring_data}


%%%%%%%%%%%%%%%%%%%%%%%% Data Pre-processing
\input{./nested/basics/data_preprocessing}


%%%%%%%%%%%%%%%%%%%%%%%% Data sampling and partitioning
\input{./nested/basics/sampling_partitioning}

\section{Some Terms}

\emph{input variable(s)} -- predictors, independent variables, features, or simply variables.

\emph{output variable(s)} -- response or dependent variable

\emph{relationship} $Y = f(x) + \epsilon$ \textcolor{blue}{estimate $f$. prediction and inference}.

\textcolor{blue}{\emph{reducible error\index{reducible error}} -- the estimated function $\hat{f}$ will likely not be perfect, and the reducible error is the error that could be corrected.  The \emph{irreducible error} is an error that can not be corrected. The irreducible error may be larger than zero due to \emph{unmeasured variables} \emph{e.g.} varibles that were not measured and \emph{unmeasurable variation} \emph{e.g.} an individual's feelings/emotions or variation in the production of a product. The irreducible error provides an upper bound on the performance of the predicted $\hat{f}$}

\section{Type of Learning}

\textcolor{blue}{Three main types of machine learning: supervised, unsupervised, and reinforcement.}

\subsection{Supervised}

\textcolor{blue}{observe input variables with corresponding output values. Classification \textcolor{red}{ref} and regression \textcolor{red}{ref} are subcategories of supervised learning}

\subsection{Unsupervised}

\textcolor{blue}{observe input variables without corresponding output values.}

% clustering
\subsubsection{Clustering}

\textcolor{blue}{Finding sub groups. Clustering is sometimes referred to as ``unsupervised classification''}

% principal components
\subsubsection{Dimensionality Reduction}

\textcolor{blue}{where the goal is to reduce the dimensionality of the data while retaining as much as the relevant information as possible}

\subsection{Reinforcement}

\textcolor{blue}{Goal -- develop an \emph{agent} that improves it's performance based on interactions with an \emph{environment} based on a \emph{reward}}

\subsection{Supervised vs Unsupervised}

\subsection{Classification vs Regression}

\subsubsection{Regression} -- Also called regression analysis \textcolor{red}{local ref?} predicting a continuous or quantitative output value. For example attempting to find a relationship between a given predictor/explainatory variables (age, job title, zip code) and a continuous response (an individuals outcome).

\subsubsection{Classification} -- predicting categorical or qualitative output value (such as a non-numerical value). \textcolor{blue}{Binary classification (benign vs malignant) and multi-class classification (identifying many different skin diseases).}

% page 94 of AGtext
One-versus-all \emph{OvA} (also \emph{one-versus-rest}) -- 

One-versus-one (OvO) -- train a binary classifier for every pair


\textcolor{blue}{Binary classification can be extended to multi-class classification via the OvR method.}

\subsubsection{Bayes Classifier}



\section{Training}

\section{Quality of Fit}

%% regression example

\subsection{Regression Example}

\textcolor{blue}{Mean Squared Error.$\hat{f}(x_i)$ is the prediction that $\hat{f}$ produces for the $i$th sample. The output will be small for predicted values that are similar to the ground truth}

\begin{equation}
{MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{f}(x_i))^2}
\label{eq:MSE_def}
\end{equation}

%% classification example

\subsection{Classification Example}

\textcolor{blue}{The proportion of mistakes that are made.}

\begin{equation}
{error\_rate = \frac{1}{n}\sum_{i=1}^{n}(y_i \ne \hat{y_i})}
\label{eq:class_error_rate_def}
\end{equation}

\textcolor{blue}{$\hat{y_i}$ is the predicted classification label for the $i$th observation using our predictor/model $\hat{f}$ and $y_i$ is the ground truth label}

\section{(Over|Under)fitting}

\subsection{Overfitting}

\textcolor{blue}{Overfitting\index{Overfitting} refers to a case in which a model fits the training data very well but does not fit validation/test set. If a model is overfitting, it is said to have a high variance}

\textcolor{blue}{Overfitting can arise from modeling data with too many parameters/too complex of a model.}

\textcolor{green}{TODO: figure showing an example of overfitting}

\subsection{Underfitting}

\textcolor{blue}{Underfitting\index{Underfitting} refers to a case in which a model does not fit the training data well. If a model is underfitting, it is said to have a high bias}

\textcolor{blue}{Underfitting can arise from modeling data with too few parameters/too simple of a model.}

\textcolor{green}{TODO: figure showing an example of underfitting}


\section{Bias Variance Trade-off}

\subsection{Variance}
\textcolor{blue}{variance\index{Variance} refers to the amount the model would change (consistency or variability) if it was re-trained/estimated multiple using a different subsets of the training data set. A model that has high variance is sensitive to randomness in the training data}



\subsection{Bias}
\textcolor{blue}{Bias\index{Bias} refers to the amount of error that is introduced by approximating a problem with a model that is simpler than the (complex) problem} 
% not word-for-word, but example adapted from p35 of ISL
\textcolor{red}{For example, linear regression assumes a linear relationship between the features and labels. However, it is unlikely that a true linear relationship exists and so using linear regression to model this type of particular problem will likely introduce some bias.}

\subsection{Trade-Off}

% TODO: see page 34 of ISL for eq and explaination here

\textcolor{blue}{In general, as a more ``flexible'' model is used, the variance will increase and the bias will decrease.}

% see page 36 of ISL
\textcolor{blue}{It is easy to obtain a model with low bias but high variance (\emph{e.g.} drawing a squiggly line through every training observation) and it is easy to obtain a model with low variance but high bias (\emph{e.g.} drawing a straight line approximating every training observation) but it is difficult to obtain a model that has both low variance and low bias.}

\textcolor{blue}{It should be noted that in a real world example, it maynot be possible to explicitly calculate the test error, bias, or variance.}

\textcolor{green}{TODO: para about using regularization here/finding the right balance \textcolor{red}{local ref to regularization?}}

\subsection{Parametric vs non-parametric}



%%%%%%%%%%%%%%%%%%%%%%%% Metrics
\input{./nested/basics/metrics}

