\chapter{Basics}

\section{Overview}

%%%%%%%%%%%%%%%%%%%%%%%% obligatory "No Free Lunch"
I'm not sure it's possible to discuss machine learning without at least mentioning the ``No Free Lunch'' theorem, which states ``No single classifier works best across all possible scenarios''

%%%%%%%%%%%%%%%%%%%%%%%% Types of data
\textcolor{blue}{categorical or numerical. Numerical can be discrete or continuous}

%%%%%%%%%%%%%%%%%%%%%%%% Measurement Levels
\textcolor{blue}{qualitative or quantitative.} 

\textcolor{blue}{Qualitative can be nominal (aren't numbers and can't be put in any order -- e.g. the seasons: spring, summer, fall, winter) or ordinal (groups and categories that follow a strict order -- e.g. difficult levels: hard, medium, or easy)}

\textcolor{blue}{Quantitative are represented by numbers but can be interval (0 is meaningless -- e.g. temperature in C or F, where true zero is not 0) or ratio (has a true 0 -- e.g. temperature in K, weight or length)}


%%%%%%%%%%%%%%%%%%%%%%%% Acquiring Data
\input{./nested/basics/acquiring_data}


%%%%%%%%%%%%%%%%%%%%%%%% Data Pre-processing
\input{./nested/basics/data_preprocessing}

%%%%%%%%%%%%%%%%%%%%%%%% Data Type Considerations + Feature Extraction
\input{./nested/basics/data_type_considerations}


%%%%%%%%%%%%%%%%%%%%%%%% Data sampling and partitioning
\input{./nested/basics/sampling_partitioning}

\section{Some Terms}

\emph{input variable(s)} -- predictors, independent variables, features, regressors, controlled variables, exposure variables or simply variables.
 
\emph{output variable(s)} -- response or dependent variable. May also be known as regressands, criterion variables, measured variables, responding variables, explained variables, outcome variables, experimental variables, labels.

\textcolor{blue}{Both input and output variables may take on continuous or discrete values.}

\emph{relationship} $Y = f(x) + \epsilon$ \textcolor{blue}{estimate $f$. prediction and inference}.

\textcolor{blue}{\emph{reducible error\index{reducible error}} -- the estimated function $\hat{f}$ will likely not be perfect, and the reducible error is the error that could be corrected.  The \emph{irreducible error} is an error that can not be corrected. The irreducible error may be larger than zero due to \emph{unmeasured variables} \emph{e.g.} varibles that were not measured and \emph{unmeasurable variation} \emph{e.g.} an individual's feelings/emotions or variation in the production of a product. The irreducible error provides an upper bound on the performance of the predicted $\hat{f}$}

\section{Type of Learning}

\textcolor{blue}{Three main types of machine learning: supervised, unsupervised, and reinforcement.}

% From ML for Predictive Data Analytics
\textcolor{blue}{Another way to group types of learning -- Information-based, similarity-based, probability-based, and error-based}

\subsection{Supervised}

\textcolor{blue}{observe input variables with corresponding output values. A program that predicts an output for a in input by learning from pairs of labeled inputs and outputs. Classification \textcolor{red}{ref} and regression \textcolor{red}{ref} are subcategories of supervised learning}

\subsection{Unsupervised}

\textcolor{blue}{observe input variables without corresponding output values and attempts to discover patterns in the data.}

% p14 of mastering ml agorithms
\textcolor{blue}{There is no error signal to measure, rather, performance metrics report some attribute of structure discovered in the data, such as the distances within and between clusters.}
 
% clustering
\subsubsection{Clustering}

\textcolor{blue}{Finding sub groups where observations are more similar to eachother based on some similarity measure. Clustering is sometimes referred to as ``unsupervised classification'' and is often used to explore a dataset.}

\textcolor{blue}{An example of clustering may be to group a collection of documents into categories, or songs into genres.}

% principal components
\subsubsection{Dimensionality Reduction}

\textcolor{blue}{where the goal is to reduce the dimensionality of the data while retaining as much as the relevant information as possible}

\textcolor{blue}{A high number of features may be computationally costly. Ability to generalize may be reduced if some of the features capture noise or are irrelevant to the underlying relationship. The goal could be to find the features that account for the greatest changes in the response variable}


\subsection{Semi-supervised Learning}

\textcolor{blue}{`semi-supervised learning', another type of learning, makes use of both supervised and unsupervised data.}

\subsection{Reinforcement}

\textcolor{blue}{Reinforcement learning does not learn from labeled pairs of inputs and outputs, rather it learns from `feedback' from decisions that are not explicitly corrected.}

\textcolor{blue}{Goal -- develop an \emph{agent} that improves it's performance based on interactions with an \emph{environment} based on a \emph{reward}}

\subsection{Supervised vs Unsupervised}

\subsection{Classification vs Regression}

\subsubsection{Regression} 

Regression, also called regression analysis \textcolor{red}{local ref?} involves predicting a continuous or quantitative output value. For example attempting to find a relationship between a given predictor/explainatory variables (age, job title, zip code) and a continuous response (an individuals outcome).

\subsubsection{Classification} 

Classification involves predicting categorical (discrete) or qualitative output value (such as a non-numerical value). 

\textcolor{blue}{Binary classification (benign vs malignant) and multi-class classification (identifying many different skin diseases).}

\subsection{Multi-label classification}
\textcolor{blue}{TODO: {multi-label classification}\index{multi-label classification} --- where a classifier assigns multiple labels to each instance}


\subsubsection{Approaches: Problem transformation}

\textcolor{blue}{There are two main approaches to multi-label classification}

\textcolor{blue}{{Problem transformation}\index{Problem transformation} modify the original multi-label problem to a set of single-label classification problems.}

\paragraph{Unique set/combination of labels}

\textcolor{green}{TODO: table and example.}

\textcolor{blue}{Two main concerns with this methodology: i) increasing the number of classes is impractical and will often have very few instances and ii) the classifier can only predict combinations that were seen in the training data.}

\paragraph{Many Binary Classifiers}

% p124[112] of Mastering ML with SKL
\textcolor{blue}{Train a classifier for each label in the training set. The final prediction is the combination of all the predictions from the binary classifiers.}

\textcolor{blue}{The main concern with this approach is that the relationships between labels is ignored.}

\subsubsection{Evaluating Multi-label Classification}

\textcolor{blue}{see \textcolor{red}{local ref}.}

% page 94 of AGtext
One-versus-all \emph{OvA} (also \emph{one-versus-rest}) -- 

One-versus-one (OvO) -- train a binary classifier for every pair


\textcolor{blue}{Binary classification can be extended to multi-class classification via the OvR method.}

%\subsubsection{Bayes Classifier}

\section{Training}

\section{Quality of Fit}

%% regression example

\subsection{Regression Example}

\textcolor{blue}{Mean Squared Error.$\hat{f}(x_i)$ is the prediction that $\hat{f}$ produces for the $i$th sample. The output will be small for predicted values that are similar to the ground truth}

\begin{equation}
{MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{f}(x_i))^2}
\label{eq:MSE_def}
\end{equation}

%% classification example

\subsection{Classification Example}

\textcolor{blue}{The proportion of mistakes that are made.}

\begin{equation}
{error\_rate = \frac{1}{n}\sum_{i=1}^{n}(y_i \ne \hat{y_i})}
\label{eq:class_error_rate_def}
\end{equation}

\textcolor{blue}{$\hat{y_i}$ is the predicted classification label for the $i$th observation using our predictor/model $\hat{f}$ and $y_i$ is the ground truth label}

\section{Describing Learners}

\subsection{(Over$|$Under)fitting}

\subsubsection{Overfitting}

\textcolor{blue}{Overfitting\index{Overfitting} refers to a case in which a model fits the training data very well but does not fit validation/test set. If a model is overfitting, it is said to have a high variance and is analogous to memorizing the training set.}

\textcolor{blue}{Overfitting can arise from modeling data with too many parameters/too complex of a model.}

\textcolor{green}{TODO: figure showing an example of overfitting}

\subsubsection{Underfitting}

\textcolor{blue}{Underfitting\index{Underfitting} refers to a case in which a model does not fit the training data well. If a model is underfitting, it is said to have a high bias}

\textcolor{blue}{Underfitting can arise from modeling data with too few parameters/too simple of a model.}

\textcolor{green}{TODO: figure showing an example of underfitting}


\subsection{Bias Variance Trade-off}

\textcolor{blue}{Two fundamental causes of prediction error in a model -- the bias and the variance.}

\subsubsection{Variance}
\textcolor{blue}{variance\index{Variance} refers to the amount the model would change (consistency or variability) if it was re-trained/estimated multiple using a different subsets of the training data set. A model that has high variance is sensitive to randomness in the training data}

\textcolor{blue}{A model with high variance may be described as highly flexible and will likely overfit the data.}


\subsubsection{Bias}
\textcolor{blue}{Bias\index{Bias} refers to the amount of error that is introduced by approximating a problem with a model that is simpler than the (complex) problem}

\textcolor{blue}{A model with high bias will produce similar errors for instances regardless of the training data that is used to train the model -- the model is more strongly ``biased'' to its own assumptions of the relationship (as defined by the model), than the relationship the data may be indicating. A model with high bias may also be described as inflexible and will likely underfit the data.}


% not word-for-word, but example adapted from p35 of ISL
\textcolor{red}{For example, linear regression assumes a linear relationship between the features and labels. However, it is unlikely that a true linear relationship exists and so using linear regression to model this type of particular problem will likely introduce some bias.}

\subsubsection{Trade-Off}

% TODO: see page 34 of ISL for eq and explaination here

\textcolor{blue}{In general, as a more ``flexible'' model is used, the variance will increase and the bias will decrease.}


% see page 36 of ISL
\textcolor{blue}{It is easy to obtain a model with low bias but high variance (\emph{e.g.} drawing a squiggly line through every training observation) and it is easy to obtain a model with low variance but high bias (\emph{e.g.} drawing a straight line approximating every training observation) but it is difficult to obtain a model that has both low variance and low bias.}

\textcolor{blue}{It should be noted that in a real world example, it may not be possible to explicitly calculate the test error, bias, or variance.}

\textcolor{green}{TODO: para about using regularization here/finding the right balance \textcolor{red}{local ref to regularization?}}

\subsection{Parametric vs non-parametric}

\subsubsection{parametric}

\textcolor{blue}{parametric models are models that learn a fixed number of parameters, independent from the number of training instances, that able to classify new data points without requiring the original dataset anymore. First, a function form is selected (linear, polynomial, etc.), then the coefficients for the function are learned form the training data.}
	
\textcolor{blue}{Examples of parametric models may be simple artificial neural networks, naive bayes, logistic regression, etc.}

\subsubsection{nonparametric}

%% unsure about this! 
\textcolor{red}{Nonparametric models are not models without parameters, rather they are models were the number of parameters are not fixed, they may grow with the number of training instances}

\textcolor{blue}{An Example of a nonparametric model may be k-Nearest neighbors -- where the model does not assume anything about the form of the mapping function and makes predictions based on the k most similar training instances.}

\textcolor{blue}{A disadvantage to this type of approach is that the computational complexity for classifying new samples grow linearly with the number of samples in the training set.}

\textcolor{blue}{May be useful when little is known about the underlying relationship in the data and there is an abundance of data.}

\subsection{Eager vs Lazy Learners}

\textcolor{green}{TODO: Eager vs Lazy overview}
\textcolor{blue}{Training an eager learner is often more computationally expensive, but typically prediction with the resulting model is inexpensive.}

\subsubsection{Eager Learners}

\textcolor{blue}{Eager learners estimate the parameters of a model that generalize to a training set --- build an input-independent model}

\subsubsection{Lazy Learners}

\textcolor{blue}{Also known as Instance-based Learners}

\textcolor{blue}{do not spend time traioning, but may predict responses slowly (relatively) compared to eager learners}

\textcolor{blue}{Lazy learners store the training dataset with little to no processing.}


\subsection{Generative vs Discriminative Models}

\textcolor{green}{TODO: Generative vs Discriminative models overview}
%\textcolor{blue}{}

\subsubsection{Discriminative Models}

\textcolor{green}{TODO: Discriminative Models --- learn a decision boundary that is used to \textit{discriminate} between classes. There exist both probabilistic and non-probabilistic discriminative models}

\paragraph{Probabilistic Discriminative}

\textcolor{blue}{Probabilistic discriminative models learn to estimate the conditional probability i.e. which class is most probable given the input features.}

\paragraph{Non-probabilistic Discriminative}

\textcolor{blue}{Non-probabilistic discriminative models directly map features to classes.}

\subsubsection{Generative Models}

% see p129[117] of Mastering ML w/SKL
\textcolor{green}{TODO: Generative Models --- do not learn a decision boundary, rather, they model the joint probability distribution of the features and classes i.e. they model how the classes generate features. Then, using Bayes' theorem, they are able to estimate the conditional probability of a class given the features.}


% see p130[118] of Mastering ML w/SKL
\textcolor{blue}{One advantage of generative models is that they can be used to generate new examples of data}

\subsection{Strong vs Weak Learners}

\textcolor{green}{TODO: Strong vs Weak learners (classifier, predictor, etc.) overview}
%\textcolor{blue}{}

\subsubsection{Strong Learners}

\textcolor{green}{TODO: Strong Learners are models that are arbitrarily better than weak learners.}

\subsubsection{Weak Learners}

\textcolor{green}{TODO: Weak Learners are models (typically simple models) that perform only slightly better than random chance.}


\section{Online Learning}

% See p.246 of Understanding Machine learning
\textcolor{blue}{difference to \textcolor{red}{PAC learning?}}

\section{Ensemble Methods}

\textcolor{green}{TODO: overview - discussed in more detail in \textcolor{red}{local ref?}}

\section{Kernel Trick}

\textcolor{blue}{Transform the training data onto a higher dimensional feature space}

% see p177[165] of mastering ML with SKL

\textcolor{blue}{The kernel is a function that XXXXXXXXXX}

\textcolor{blue}{Choosing an appropriate kernel can be challenging}

% see p180[168] of mastering ML w/SKL for more on kernels
\textcolor{blue}{Some commonly used kernels include polynomial, sigmoid, Guassian, and linear kernels}

\textcolor{blue}{commonly used in SVMs (see \textcolor{red}{local ref}), the kernel trick can be used with any model that can be expressed in terms of the dot product of two feature vectors.}

\section{Hyper-Parameters}

\subsection{Parameters}

\subsubsection{Learning Rate}

\textcolor{green}{TODO: Learning rate overview}

%%%% learning rates
\textcolor{blue}{cyclic learning rate~\cite{smith2017cyclical}}

\textcolor{blue}{sgdr: stochastic gradient descent with restarts~\cite{loshchilov2016sgdr}. The learning rate is decreased from the max value along a curve (cosine, shown in Eq.\ref{eq:sgdr_def}, where $n_{max}^i$ and $n_{min}^i$ are ranges for the learning rate, $T_i$ represents epochs, $T_{cur}$ is how many epochs have been performed since the last restart). The authors also suggest making each next cycle longer than the previous cycle by a constant $T_mul$ may be beneficial.}

\begin{equation}
{n_t = n_{min}^i + 1/2(n_{max}^i - n_{min}^i)(1 + cos(\frac{T_{cur}}{T_i}\pi))}
\label{eq:sgdr_def}
\end{equation}

\subsubsection{Batch size}

\textcolor{green}{TODO: batch size overview}

%%%%% small batch size
\textcolor{blue}{small minibatch sizes (between 2 and 32) may be better than large batch sizes~\cite{masters2018revisiting}.}

\textcolor{blue}{``generalization gap'' may not be due to large mini-batches, rather, due to the number of updates made to the system~\cite{hoffer2017train}}

%%%% minibatch
\textcolor{blue}{Batch training is almost always slower to converge than on-line/mini-batch training, which is likely due to the fact that on-line/mini-batches learning will follow the error surface, allowing for larger learning rates, and thus faster convergence~\cite{wilson2003general}.}

% incrementing batchsize over time
\textcolor{blue}{Increasing the batch size may achieve similar benefits to decaying the learning rate ~\cite{smith2017don} -- which could lead to use of larger batch sizes, reducing the number of parameter updates and therefore reducing training time.}

\subsection{Hyper-Parameter Optimization}

\subsubsection{Grid Search}

\textcolor{blue}{{Grid search}\index{Grid search} Exhaustive search that trains+evaluates a model for each combination of specified hyperparameter configurations and combinations defined by a Cartesian product of the sets of possible values for each hyperparameter.}

\subsubsection{Randomized Search}

\textcolor{blue}{{Randomized search}\index{Randomized search} }

\textcolor{green}{TODO: figure demonstrating difference between grid and randomized search}

\subsubsection{Other Methods}

\textcolor{blue}{See \textcolor{red}{local ref? --- advanced methods and research}}

%%%%%%%%%%%%%%%%%%%%%%%% Optimizers
\input{./nested/basics/optimizers}

%%%%%%%%%%%%%%%%%%%%%%%% Evaluation
\input{./nested/basics/evaluation}


%%%%%%%%%%%%%%%%%%%%%%%% Metrics
\input{./nested/basics/metrics}

