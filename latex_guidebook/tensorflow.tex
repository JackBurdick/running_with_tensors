\chapter{ML with TensorFlow}

Overview

\section{Introduction}

\section{Artificial Neural Networks}

\section{Convolutional Neural Networks}

\section{Recurrent Neural Networks}

\section{Weight Initialization}

\input{./tensorflow/activation_functions}


\section{Optimizers}

\subsection{What are optimizers}

\subsection{Types of optimizers}

\subsection{TF optimizers}

tf.train.GradientDescentOptimizer

tf.train.MomentumOptimizer

tf.train.RMSPropOptimizer

tf.train.AdadeltaOptimizer

tf.train.AdagradOptimizer

tf.train.AdagradDAOptimizer

tf.train.AdamOptimizer

tf.train.FtrlOptimizer

tf.train.ProximalGradientDescentOptimizer

tf.train.ProximalAdagradOptimizer

\subsection{Advanced}

\subsubsection{Gradient Computation}

\subsubsection{Gradient Clipping}


\section{Hyperparameters}

\subsection{Training Related}

\subsubsection{Learning Rate}

\subsubsection{Batch Size}

\subsubsection{Number of Training Iterations}

\subsubsection{Momentum}

\subsubsection{Weight Update}

\textcolor{red}{SGD, CG, L-BFGS, more complex more hyper-parameters}

\subsubsection{Stopping Criteria}

\subsection{Model Related}

\subsubsection{Architecture}

\subsubsection{Weight Initialization}

\subsubsection{Weight-decay}

L1

L2

\subsubsection{Drop-out}

\section{Hyper-parameter optimization}

\textcolor{blue}{OVERVIEW}

\subsubsection{Coordinate Descent}

All hyper-parameters remain fixed, except for the hyper-parameter of interest. The hyper-parameter of interest is then adjusted such that the validation error is minimized.

\subsubsection{Grid Search}

\subsubsection{Random Search}

\subsubsection{Automated / Model-based Methods}


\section{Regulariazation}

\section{Image Augmentation}

\section{Serving}

\section{TensorBoard}

\section{Estimators}

\section{Metrics}

\section{Eager}

\section{Model Persistence}

