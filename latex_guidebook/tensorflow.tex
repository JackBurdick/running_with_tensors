\chapter{ML with TensorFlow}

Overview

\section{Introduction}

\section{Artificial Neural Networks}

\subsection{History}

\textcolor{blue}{McCullock and Pitts -- nerve cell as a simple logic gate with binary outputs.  Where multiple input signals are accumulated and if they exceed a certain threshold an output signal is generated}

\textcolor{blue}{Frank Rosenblatt -- the perceptron: an algorithm that would learn the optimal weight coefficients to the input features in order to determine whether an output signal should be produced. The early activation function \textcolor{red}{See more in local ref?} was a simple unit step function.}

\textcolor{blue}{Adaline (\textbf{ADA}ptive \textbf{LI}near \textbf{NE}uron) -- rather than the weights being updated based on a unit step function like the perceptron, the weights are updated based on a linear activation function. A continuous output value (rather than discrete) is used to compute the model error and update the weights.}

\textcolor{blue}{Linear activation function is used for weight updates but a unit step function can still be used to predict the class labels.\textcolor{red}{TODO: figure showing this}}

\section{Convolutional Neural Networks}

\section{Recurrent Neural Networks}

\section{Weight Initialization}

%%%%% activation functions
\input{./tensorflow/activation_functions}

%%%%% Optimizers
\input{./tensorflow/optimizers}


\section{Hyperparameters}

\subsection{Training Related}

\subsubsection{Learning Rate}

\paragraph{Too High vs Too Low}

\textcolor{blue}{TODO: figure showing a convex cost function and the result of a learning rate being too high (overshoot, diverge) and too small (local minima)}

\subsubsection{Batch Size}

\subsubsection{Number of Training Iterations}

\subsubsection{Momentum}

\subsubsection{Weight Update}

\textcolor{red}{SGD, CG, L-BFGS, more complex more hyper-parameters}

\subsubsection{Stopping Criteria}

\subsection{Model Related}

\subsubsection{Architecture}

\subsubsection{Weight Initialization}

\subsubsection{Weight-decay}

L1

L2

\subsubsection{Drop-out}

\section{Hyper-parameter optimization}

\textcolor{blue}{OVERVIEW}

\subsubsection{Coordinate Descent}

All hyper-parameters remain fixed, except for the hyper-parameter of interest. The hyper-parameter of interest is then adjusted such that the validation error is minimized.

\subsubsection{Grid Search}

\textcolor{green}{TODO: grid search explanation}

\subsubsection{Random Search}

\textcolor{green}{TODO: random search explanation}

\subsubsection{Grid vs Random Search}

\textcolor{green}{TODO: grid vs random search figure}

\subsubsection{Automated / Model-based Methods}


\section{Regularization}

\subsection{Why Regularization}

\subsection{Types of Regularization}

\subsubsection{Regularization Methods and Implementations}

\paragraph{L2 Regularization}

\paragraph{L1 Regularization}

\section{Image Augmentation}

\section{Serving}

\section{TensorBoard}

\section{Estimators}

\section{Metrics}

\section{Eager}

\section{Model Persistence}

