\chapter{Generative}

\r{Nice resource: "Generative Deep Learning"\cite{foster2019generative} and \TD{An Introduction to Deep Generative Modeling \cite{DBLP:journals/corr/abs-2103-05180}}}

\section{Generative Adversarial Networks (GANs)}

\TD{Generative Adversarial Networks \cite{Goodfellow2014GenerativeAN}}

\TD{``Inverse PM -- semi-famous interaction, stemming from this review describing/inquiring about the relation to https://web.archive.org/web/20160411075236/http://media.nips.cc/nipsbooks/nipspapers/paper\_files/nips27/reviews/1384.html '' "predictability minimisation" or PM\cite{schmidhuber1992learning}}

% TODO: how as this not been done yet? I think I've written some pretty nice text on this before
\r{Discriminator and generator}

\r{Tries to learn the underlying structure of the data}

%TODO:

\TD{Self-Attention Generative Adversarial Networks \cite{Zhang2019SelfAttentionGA} uses attention\cite{DBLP:journals/corr/abs-1711-07971}}

\TD{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks \cite{Radford2015UnsupervisedRL}}

\TD{Mode collapse}

\section{Variational AutoEncoder}

\TD{Auto-Encoding Variational Bayes \cite{Kingma2014AutoEncodingVB}}