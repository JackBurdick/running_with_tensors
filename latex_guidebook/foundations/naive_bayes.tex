%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Naive Bayes
\section{Naive Bayes}

\textcolor{blue}{history: Thomas Bayes in 18th century, published posthumously by Richard Price.}

\textcolor{blue}{typically performs well on small datasets. Compared to logistic regression it is more biased.}

\subsection{Bayes' Theorem}

\textcolor{blue}{Bayes' theorem is a formula (Eq.\ref{eq:bayes_def}) that is used for calculating the probability of an event using prior knowledge and related conditions.}

\begin{equation}
{P(A|B)=\frac{P(B|A)P(A)}{P(B)}}
\label{eq:bayes_def}
\end{equation}

\r{the quantity on the left side $P(A|B)$ is called the posterior probability. $P(B|A)$ is the conditional probability. $P(A)$ is the prior probability.$P(B)$ is a normalization factor \textcolor{red}{ensures that the posterior probabilities sum to unity.}}

\begin{equation}
{P(y|x_1,\dots,x_n)=\frac{P(x_1,\dots,x_n|y)P(y)}{P(x_1,\dots,x_n)}}
\label{eq:bayes_exp_def}
\end{equation}

% see p131(119) of Mastering ML with SKL
\textcolor{green}{TODO: more...}

% see p131(119) of Mastering ML with SKL
\textcolor{blue}{Variants: multinomial, Gaussian, and Bernoulli.}

% gaussian = any continous data > avg value as well as st.deviation for each feat. for each class % usually high dimensional, other two usually sparse
% bernoulli = binary data
% multi = assumes count data > average balue of each feature for each class 

\textcolor{blue}{The model is considered Naive because it assumes that all the features are conditionally independent given the response variable.}


\textcolor{blue}{NB assumes all training instances are {independent and identically distributed (i.i.d)}\index{independent and identically distributed (i.i.d)} -- training instances must be independent from each other and drawn from the same probability distribution.}

\textcolor{blue}{TODO: example of iid -- dice roll, coin toss}

% see p 131[119] of Mastering ML with SKL on independence.