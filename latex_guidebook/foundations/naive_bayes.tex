%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Naive Bayes
\section{Naive Bayes}

\textcolor{blue}{typically performs well on small datasets. Compared to logistic regression it is more biased.}

\subsection{Bayes' Theorem}

\textcolor{blue}{Bayes' theorem is a formula (Eq.\ref{eq:bayes_def}) that is used for calculating the probability of an event using prior knowledge and related conditions.}

\begin{equation}
{P(A|B)=\frac{P(B|A)P(A)}{P(B)}}
\label{eq:bayes_def}
\end{equation}

\begin{equation}
{P(y|x_1,\dots,x_n)=\frac{P(x_1,\dots,x_n|y)P(y)}{P(x_1,\dots,x_n)}}
\label{eq:bayes_exp_def}
\end{equation}

% see p131(119) of Mastering ML with SKL
\textcolor{green}{TODO: more...}

% see p131(119) of Mastering ML with SKL
\textcolor{blue}{Variants: multinomial, Gaussian, and Bernoulli.}

\textcolor{blue}{The model is considered Naive because it assumes that all the features are conditionally independent given the response variable.}

\textcolor{blue}{NB assumes all training instances are {independent and identically distributed (i.i.d)}\index{independent and identically distributed (i.i.d)} -- training instances must be independent from each other and drawn from the same probability distribution.}