\section{Perceptron}

\textcolor{green}{TODO: overview}

\textcolor{green}{Obligatory Biology figure}

\subsection{History}

\textcolor{blue}{McCullock and Pitts -- nerve cell as a simple logic gate with binary outputs.  Where multiple input signals are accumulated and if they exceed a certain threshold an output signal is generated}

\r{$x_i$ represent the input (maybe raw features or output from previous neurons), weights $w_i$ represetn the strengths of the interconnections (analygous to synapses) between the neurons, and the bias $w_0$ represent the threshold for a neuron to be activated.}

\textcolor{blue}{Frank Rosenblatt at Cornell Aeonautical Laboratory in late 1950s -- motivated by efforts to simulate the human brain -- the perceptron: an algorithm that would learn the optimal weight coefficients to the input features in order to determine whether an output signal should be produced. The early activation function \textcolor{red}{See more in local ref?} was a simple unit step function.}

\textcolor{blue}{Adaline (\textbf{ADA}ptive \textbf{LI}near \textbf{NE}uron) \textcolor{green}{(TODO: WIdrow and Hoff (1960))} -- rather than the weights being updated based on a unit step function like the perceptron, the weights are updated based on a linear activation function. A continuous output value (rather than discrete) is used to compute the model error and update the weights.}

\textcolor{blue}{Linear activation function is used for weight updates but a unit step function can still be used to predict the class labels.\textcolor{red}{TODO: figure showing this}}

\subsection{Overview}

\textcolor{blue}{advantage: perceptrons are capable of online learning --- the models parameters can be updated on a single training instance rather than a batch of instances}

\textcolor{blue}{Though not used too frequently in practice, the perceptron is a building block that later sections in this chapter on artificial neural networks are built upon.}

\subsection{Activation Function Basics}

\textcolor{blue}{Basic activation function (Eq.\ref{eq:act_func_basic_def}) where $w$ represents the model's parameters and $b$ represents a bias terms and $act$ is representative of the activation function.}

\begin{equation}
{y = act (\sum_{i=1}^{n}(w_i x_i + b)}
\label{eq:act_func_basic_def}
\end{equation}

\textcolor{blue}{There are many different types of activation functions that may be used. A more in-depth discussion on activation functions is discussed in \textcolor{red}{local ref?}}

\textcolor{blue}{The linear combination of parameters and inputs may sometimes be referred to as the preactivation}

\textcolor{blue}{Rosenblatt's original activation function is the {Heaviside step function}\index{Heaviside step function} ({unit step function}\index{unit step function}), show in Eq.\ref{eq:heaviside_step_func}] \textcolor{green}{TODO: figure}}

\begin{equation}
{out(x) = \left\{
	\begin{array}{ll}
	1 & \quad $when $ x \geq 0 \\
	0 & \quad x < 0
	\end{array}
	\right.}
\label{eq:heaviside_step_func}
\end{equation}

% see p164[152] of Mastering ML w/SKL *para*


\subsection{Limitations}

\textcolor{blue}{}

\textcolor{blue}{nonlinear activation function is necessary to break linearity since computing a series of weighted sums is equivalent to computing a single weighted sum}

\r{around the 1960s --- shown that they could solve a number of problems readily, while unable to solve other problems (that superfically appeared to be a similar level of difficulty). \TD{Perceptrons (book), Minsky and Papert 1969}}

\r{can only classify data sets by a linear hyperplane. However, it is possible to solve linearly inseparable data, provided the data can first be preprocessed. The difficulty lies in the fact that the (pre)processing elemetns are fixed in advance and cannot adapt.}

\subsection{Extending to model linearly inseparable data}

\subsubsection{Kernelization}

\textcolor{blue}{projecting linearly inseparable data into a higher dimensional space (with the intent to make the data linearly separable)}

\subsubsection{Directed Graph}

\textcolor{blue}{Artificial neural network (discussed next, in \textcolor{red}{local ref?}) is a universal function approximator make of a directed graph of perceptrons}

\subsection{Notes}

\textcolor{blue}{If the activation function is a logistic sigmoid activation function, the model is the same as logistic regression.  The difference, however, is that the perceptron can be trained with an online, error driven, approach}