\section{Feedback or Recurrent}

\textcolor{green}{TODO: Overview}

%%%% popular layer types

\textcolor{blue}{LSTM}

\textcolor{blue}{GRU}

\r{RNNs or ``\textit{\textbf{r}}ecurrent \textit{\textbf{n}}eural \textit{\textbf{n}}etworks'' are used for a variety of purposes but are typically designed with sequences of data as an input in mind. They are similarin concept to a standard/feed-forward netowrk, with the major distinction being that they also have connections that point ``backwards'' i.e. they have connections that feed into themselves.}

\r{Are capable fo working on sequences of arbitrary lengths, rather than fixed-sized inputs}

\subsection{Foundation}

\r{An example of an RNN diagram is shown in \TD{fig}. However, this representation is misleading since it does not show ``every'' connection in the model --- most notably, the recurrent connections.  RNNs may also be often represented in diagrams as ``unrolled'' (\TD{fig}). The unrolled RNN is easier to visualize how these recurrent connections are included.  This makes it easier to understand how each timestep is dependent on not only the current input (at the particular time step), but also dependent on ``all'' previous time steps. It is often stated that at a certain timestep (n), the output has ``memory'' since it is a function of all the previous time steps.}


\footnotetext{the term ``all'' is emphasized here since it is the goal to include information from all previous time steps. This is true in theory, however, this is not always the case in practice. This is discussed further in \ALR{}}

\subsection{Simple RNN and Recurrent Neuron}

\TD{Diagram of the inside of a RNN neuron}

\subsubsection{Common Use Cases}

\begin{enumerate}[noitemsep,topsep=0pt]
	\item Sequence to Sequence
	\begin{enumerate}[noitemsep,topsep=0pt]
		\item Sequence to Sequence
		\item Delayed Sequence to Sequence
	\end{enumerate}
	\item Sequence to Vector
	\item Vector to Sequence
\end{enumerate}


\paragraph{Sequence to Sequence}

\subparagraph{Overview}

\TD{todo}

\r{Sequence to sequence models are typically associated with translation tasks.}

\r{stock prices --- a model may be given prices of a stock for a given number of days and the model will attempt to predict the prices shifted by a day in the future.}


\paragraph{Sequence to Vector}

\r{Sequence to vector models may be used to predict a vector associated with a time series}

\r{all outputs are typically ignored, except for the last one.}

\r{model may be given a sequence of text and attempt to predict a sentiment for a given sequence.}

\subparagraph{Overview}

\TD{todo}

\paragraph{Vector to Sequence}

\r{implementation --- model given a single value at the first timestep and (0s for the rest of the timesteps) and a sequence could be output.}

\r{may be sued in image captioning --- where an image is fed to a network in the first timestep and a sequence associated with that image may be output.}

\subparagraph{Overview}

\TD{todo}

\paragraph{Delayed Sequence to Sequence}

\r{Delayed sequence to sequence models are two step models, composed of an encoder and decoder. They are typically associated with translation tasks.  In this architecture, the encoder \textit{encodes} the sequence to a vector and the decoder \textit{decodes} the vector to a sequence.}l

\r{This type of model may perform better than a standard sequence to sequence model for translation since the last word or part of a sequence may influence the overall meaning of the original input.}

\subparagraph{Overview}

\TD{todo}


\section{Common Problems}

Two well known main problems with RNNs.

\begin{enumerate}[noitemsep,topsep=0pt]
	\item Maintaining states are expensive
	\item Vanishing and/or exploding gradients
\end{enumerate}

\TD{hardware acceleration}

\subsection{Maintaining States}


\subsection{Addressing Vanishing and Exploding Gradients}

\r{Propagating signals through a long/deep network without loosing (vanishing gradient) or overamplifying the signal (exploding gradient) is difficult.  There have been a few advances to address this issue.}

\begin{enumerate}[noitemsep,topsep=0pt]
	\item Architecture (different cell types, memory schemes)
	\item Initialization Strategies
	\item Activation Function
\end{enumerate}




\section{Architecture}

\TD{a different section focusing on this}

\subsection{Attention}
% TODO: will need to figure out how to organize these sections, also, it does not belong in the feedback file...
% but I do think these types of sections (``types of layers'' belong in single section.. architecture -> operations ?)

% original attention paper
\TD{Neural Machine Translation by Jointly Learning to Align and Translate \cite{Bahdanau2015NeuralMT}}


\subsection{Cell Advancements}

\subsubsection{LSTM}

Introduced in 1997 %\cite{hochreiter1997long}

\r{detect long term dependencies in sequence}

\r{two state vectors, short and long term}

\r{Main motivation: learning what to store in the long-term state and what to ``forget''.}

\r{at each time step, some information is ``stored'' and some information is ``forgotten''.}

\paragraph{Fully Connected Layers}


\begin{enumerate}[noitemsep,topsep=0pt]
	\item Main
	\item \textit{Gate Controllers}
	\begin{enumerate}[noitemsep,topsep=0pt]
		\item Forget
		\item Input
		\item Output
	\end{enumerate}
\end{enumerate}

\r{The gate controllers use a logistic activation fuction (output a range from 0 to 1). This output is then fed through an element-wise multiplication function and thus if the value is $0$, the gate is ``closed'', and $1$ if the gate is ``open''.}

\r{These gates are able to potentially:}

\begin{enumerate}[noitemsep,topsep=0pt]
	\item Recognize an important input
	\item Store the important input in a long-term state ()
	\item Preserve the information for as long as it's needed
	\item Extract the important information when needed
\end{enumerate}


\subparagraph{Main}

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{\TD{Main Layer DIAGRAM}}
	%\label{}
\end{figure}

\r{This allows for the same basic functionality as a ``standard'' RNN cell --- however, the output, rather than being only sent to the next cell, is now partially stored in the long-term state.}


\subparagraph{Forget}

\r{Determines which part of the long-term state is forgotten/erased.}

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{\TD{Forget Layer DIAGRAM}}
	%\label{}
\end{figure}



\subparagraph{Input}

\r{Determines which part of the output from the \textbf{main layer} are kept in the long-term state.}

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{\TD{Input Layer DIAGRAM}}
	%\label{}
\end{figure}

\subparagraph{Output}

\r{Determines which part of the long term state is ``relevant'' (read and output).}

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{example-image-a}\hfil
	\caption{\TD{Output Layer DIAGRAM}}
	%\label{}
\end{figure}


\paragraph{Other}

\subparagraph{Peephole Connections}

\r{In basic LSTM cells, the gate controller can only look at the input and previous short-term state. Peephole connections, proposed in 2000 \TD{cite gers2000recurrent} add an extra connection that allows for the gate controller to also see information from the long term state as well. }

\r{The previous long-term state also becomes an input to the forget and input gate. The current long-term state becomes an intput to the output gate.}



\subsubsection{GRU}

\r{The GRU (gated recurrent unit) is a varient of the LSTM cell \TD{cite - cho2014learning}. The main modifications include:}

\begin{itemize}[noitemsep,topsep=0pt]
	\item Both state vectors are merged into one state vector
	\item A single gate controller determines the \textbf{Forget} and \textbf{Input} gate
	\begin{itemize}[noitemsep,topsep=0pt]
		\item If the gate output is a 1, the input is open and the forget gate is closed. If the gate output is 0, the input gate is closed and the forget gate is open
	\end{itemize}
	\item \r{The output gate is removed and a new controller exists that controls which part of ht previous state will be ``shown'' to the main layer}. At each timestep the full state vector is output.
\end{itemize}



\subsection{Initialization}

\subsection{Activation Functions}

\subsection{Notes -- add}

\r{A recent paper \TD{greff2017lstm}, comares three LSTM variants and makes three main observations:}

\begin{itemize}[noitemsep,topsep=0pt]
	\item no significant architecture improvements over LSTMs
	\item forget gate and the output activation function are the most critical components
	\item \TD{hyperparams...}
\end{itemize}










