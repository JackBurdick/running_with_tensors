\section{Feedback or Recurrent}

\textcolor{green}{TODO: Overview}

%%%% popular layer types

\textcolor{blue}{LSTM}

\textcolor{blue}{GRU}

\r{RNNs or ``\textit{\textbf{r}}ecurrent \textit{\textbf{n}}eural \textit{\textbf{n}}etworks'' are used for a variety of purposes but are typically designed with sequences of data as an input in mind. They are similarin concept to a standard/feed-forward netowrk, with the major distinction being that they also have connections that point ``backwards'' i.e. they have connections that feed into themselves.}

\r{Are capable fo working on sequences of arbitrary lengths, rather than fixed-sized inputs}

\subsection{Foundation}

\r{An example of an RNN diagram is shown in \TD{fig}. However, this representation is misleading since it does not show ``every'' connection in the model --- most notably, the recurrent connections.  RNNs may also be often represented in diagrams as ``unrolled'' (\TD{fig}). The unrolled RNN is easier to visualize how these recurrent connections are included.  This makes it easier to understand how each timestep is dependent on not only the current input (at the particular time step), but also dependent on ``all'' previous time steps. It is often stated that at a certain timestep (n), the output has ``memory'' since it is a function of all the previous time steps.}


\footnotetext{the term ``all'' is emphasized here since it is the goal to include information from all previous time steps. This is true in theory, however, this is not always the case in practice. This is discussed further in \ALR{}}

\subsection{Simple RNN and Recurrent Neuron}

\TD{Diagram of the inside of a RNN neuron}

\subsubsection{Common Use Cases}

\begin{enumerate}[noitemsep,topsep=0pt]
	\item Sequence to Sequence
	\begin{enumerate}[noitemsep,topsep=0pt]
		\item Sequence to Sequence
		\item Delayed Sequence to Sequence
	\end{enumerate}
	\item Sequence to Vector
	\item Vector to Sequence
\end{enumerate}


\paragraph{Sequence to Sequence}

\subparagraph{Overview}

\TD{todo}

\r{Sequence to sequence models are typically associated with translation tasks.}

\r{stock prices --- a model may be given prices of a stock for a given number of days and the model will attempt to predict the prices shifted by a day in the future.}


\paragraph{Sequence to Vector}

\r{Sequence to vector models may be used to predict a vector associated with a time series}

\r{all outputs are typically ignored, except for the last one.}

\r{model may be given a sequence of text and attempt to predict a sentiment for a given sequence.}

\subparagraph{Overview}

\TD{todo}

\paragraph{Vector to Sequence}

\r{implementation --- model given a single value at the first timestep and (0s for the rest of the timesteps) and a sequence could be output.}

\r{may be sued in image captioning --- where an image is fed to a network in the first timestep and a sequence associated with that image may be output.}

\subparagraph{Overview}

\TD{todo}

\paragraph{Delayed Sequence to Sequence}

\r{Delayed sequence to sequence models are two step models, composed of an encoder and decoder. They are typically associated with translation tasks.  In this architecture, the encoder \textit{encodes} the sequence to a vector and the decoder \textit{decodes} the vector to a sequence.}l

\r{This type of model may perform better than a standard sequence to sequence model for translation since the last word or part of a sequence may influence the overall meaning of the original input.}

\subparagraph{Overview}

\TD{todo}


\section{Common Problems}

Two well known main problems with RNNs.

\begin{enumerate}[noitemsep,topsep=0pt]
	\item Maintaining states are expensive
	\item Vanishing and/or exploding gradients
\end{enumerate}

\paragraph{Sequence to Vector}