\section{Artificial Neural Networks (ANN)}

\textcolor{blue}{Will be focusing on principals and basic feed forward networks}

\subsection{History}

\textcolor{blue}{McCullock and Pitts -- nerve cell as a simple logic gate with binary outputs.  Where multiple input signals are accumulated and if they exceed a certain threshold an output signal is generated}

\textcolor{blue}{Frank Rosenblatt -- the perceptron: an algorithm that would learn the optimal weight coefficients to the input features in order to determine whether an output signal should be produced. The early activation function \textcolor{red}{See more in local ref?} was a simple unit step function.}

\textcolor{blue}{Adaline (\textbf{ADA}ptive \textbf{LI}near \textbf{NE}uron) -- rather than the weights being updated based on a unit step function like the perceptron, the weights are updated based on a linear activation function. A continuous output value (rather than discrete) is used to compute the model error and update the weights.}

\textcolor{blue}{Linear activation function is used for weight updates but a unit step function can still be used to predict the class labels.\textcolor{red}{TODO: figure showing this}}

\textcolor{blue}{A feedforward network can be represented as a directed acyclic graph -- a directed graph in which there exist no cycles in the underlying graph, with nodes representing neurons and connected by edges}

\textcolor{blue}{The most computationally expensive component is calculating the gradient of the loss function with respect to the parameters of the network}

% see page 233 of Understanding Machine Learning
\textcolor{blue}{Artificial neural networks are {universal approximators}\index{universal approximators} -- \textcolor{red}{expand}}

\subsection{Types: Feedback vs Feed-forward}

\subsubsection{Feed-forward}

\textcolor{blue}{Directed acyclic graph of artificial neurons.}

\subsubsection{Feedback}

\subsection{Learning: Backpropagation}

\subsection{Multi-layer perceptrons}