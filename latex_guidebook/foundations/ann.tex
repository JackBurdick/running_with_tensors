\section{Artificial Neural Networks (ANN)}

\textcolor{blue}{Will be focusing on principals and basic feed forward networks}

\subsection{History}

\textcolor{blue}{McCullock and Pitts -- nerve cell as a simple logic gate with binary outputs.  Where multiple input signals are accumulated and if they exceed a certain threshold an output signal is generated}

\textcolor{blue}{Frank Rosenblatt -- the perceptron: an algorithm that would learn the optimal weight coefficients to the input features in order to determine whether an output signal should be produced. The early activation function \textcolor{red}{See more in local ref?} was a simple unit step function.}

\textcolor{blue}{Adaline (\textbf{ADA}ptive \textbf{LI}near \textbf{NE}uron) -- rather than the weights being updated based on a unit step function like the perceptron, the weights are updated based on a linear activation function. A continuous output value (rather than discrete) is used to compute the model error and update the weights.}

\textcolor{blue}{Linear activation function is used for weight updates but a unit step function can still be used to predict the class labels.\textcolor{red}{TODO: figure showing this}}