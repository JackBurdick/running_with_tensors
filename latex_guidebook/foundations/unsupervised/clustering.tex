\section{Clustering}

\textcolor{blue}{Clustering, may be referred to as cluster analysis, involves grouping observations such that instances in the same group (cluster) are more similar to one another than they are to instances from another group --- where ``similarity'' is defined by `some metric'. Clustering may be used to explore a dataset}

\textcolor{blue}{Clustering may be used to perform {image quantization}\index{image quantization}, a lossy compression methods that will replace similar colors with a single color to reduce the size of the image file.}

\r{Finding sub groups where observations are more similar to eachother based on some similarity measure. Clustering is sometimes referred to as ``unsupervised classification'' and is often used to explore a dataset.}

\r{An example of clustering may be to group a collection of documents into categories, or songs into genres. examples --- recommendation system}

\subsection{Common Algorithms}

\r{Major clustering algorithms}
\begin{itemize}[noitemsep,topsep=0pt]
	\item k-means
	\item hierarchical clustering
	\item DBSCAN
\end{itemize}

\subsubsection{K-means}

\r{iterative process -- moves centroid (center of cluster) to mean position of instances then reassigning instances to clusters by closest centroid}

\r{will continue to iterate until some stopping criteria is satisfied.}

\r{To avoid unlucky initialization leading to convergence on local optima, K-means may be repeated many times (dozens to hundreds) with a random initialization. The initialization values that lead to the minimum cost function is then determined to be best.}

\r{$k$ (positive integer less than the number of instances in the training set) specifies the number of clusters/centroids}

\r{minimizes the within-cluster/group variation (also known as the initeria)}

\r{the more clusters, the initeria decreases}

\TD{TODO: k-means cost function}

\r{important to note that K-means will not necessarily converge on the global optimum. Different runs will result in slightly different cluster assignments.}

\r{randomly assigns each observation to one of the k clusters}

\r{reassigns obervations in order to minimize the Euclidean distance between the observation and cluster's center.}

\r{several runs -- lowest total sum of within-cluster variation}

\TD{TODO: variations such as mini-batch k-means}

\r{}

\paragraph{Local Optima}

\textcolor{blue}{unlucky initialization}

\paragraph{Selecting K}

\TD{todo: Overview}

\r{No right/theoretical answer, need to perform trial and error: optimal number found by minimizing the cost function}

\paragraph{Elbow Method}

\textcolor{blue}{The elbow method is used to select the optimal number of clusters if $k$ is not specified by the problems context.}

% rough
\textcolor{blue}{plot of the final cost function value is plotted against the value of $k$. As $k$ increases, the average distortion will decrease as each cluster will have fewer instances and they will be closer to their respective centroid. This decline in dispersion will decline most at a point and then slowly continue to decline -- the point at which the dispersion decreases the most can be considered the elbow (and considered by the elbow method to be the best value for $k$)}

\textcolor{green}{TODO: diagram showing data clusters}

\textcolor{green}{TODO: diagram showing $k$ vs dispersion related to above cluster diagram}



\subsubsection{Hierarchical Clustering}

\TD{overview}

\TD{agglomerative --- tree-based clustering that builds a dendogram --- upside-down tree}

\r{after evaluation, the user can decide which size of tree tey want to use.}

\r{by default Euclidian distance is used, but other similarity metrics can also be used \TD{link to reference}}

\TD{figure example}

% TODO: this likely diens't fit here
\TD{Ward --- Ward's minimum variance method}

\subsubsection{DBSCAN}

\TD{\textbf{D}ensity-\textbf{b}ased \textbf{s}patial \textbf{c}lustering of \textbf{a}pplications with \textbf{n}oise.}

\r{user defined: minimum number of instances and distance}

\r{any instance that is not within a specified distance is labeled as an outlier}

\r{arbitrarily shaped clusters}

\subsubsection{HDBSCAN}

\r{Hierarchical DBSCAN --- groups on density then uses distance measure on the created groups iteratively}


\subsection{Evaluating}

\textcolor{blue}{There are no labels - can still evaluate using intrinsic measures}

\textcolor{blue}{measuring distortion of clusters}

\subsubsection{Silhouette Coefficient}

\textcolor{blue}{The {silhouette coefficient}\index{silhouette coefficient} is a measure of compactness and separation of clusters. The silhoette coefficient is calculated per instance or for a set of instances (where the value is calculated to be the mean of the individual instances in the group). Eq.\ref{eq:silhouette_coef_def} can be used to calculate the silhouette per instance, where $d$ is the mean distance between the instance and the instances in the next closest cluster and $b$ is hte mean distance between the instances in the indicated cluster.}

\begin{equation}
{s = \frac{db}{max(d,b)}}
\label{eq:silhouette_coef_def}
\end{equation}





