\section{Clustering}

\textcolor{blue}{Clustering, may be referred to as cluster analysis, involves grouping observations such that instances in the same group (cluster) are more similar to one another than they are to instances from another group --- where ``similarity'' is defined by `some metric'. Clustering may be used to explore a dataset}

\textcolor{blue}{examples --- recommendation system}

\textcolor{blue}{Clustering may be used to perform {image quantization}\index{image quantization}, a lossy compression methods that will replace similar colors with a single color to reduce the size of the image file.}

\subsection{Common Algorithms}

\begin{itemize}[noitemsep,topsep=0pt]
	\item k-means
	\item hierarchical clustering
	\item DBSCAN
\end{itemize}

\subsubsection{K-means}

\r{iterative process -- moves centroid (center of cluster) to mean position of instances then reassigning instances to clusters by closest centroid}

\r{will continue to iterate until some stopping criteria is satisfied.}

\r{To avoid unlucky initialization leading to convergence on local optima, K-means may be repeated many times (dozens to hundreds) with a random initialization. The initialization values that lead to the minimum cost function is then determined to be best.}

\r{$k$ (positive integer less than the number of instances in the training set) specifies the number of clusters/centroids}

\r{minimizes the within-cluster/group variation (also known as initeria)}

\TD{TODO: k-means cost function}

\r{important to note that K-means will not necessarily converge on the global optimum}

\r{randomly assigns each observation to one of the k clusters}

\r{reassigns obervations in order to minimize the Euclidean distance between the observation and cluster's center.}

\r{lowest total sum of within-cluster variation}

\TD{TODO: variations such as mini-batch k-means}

\paragraph{Local Optima}

\textcolor{blue}{unlucky initialization}

\paragraph{Selecting K}

\textcolor{green}{todo: Overview}

\textcolor{blue}{optimal number found by minimizing the cost function}

\paragraph{Elbow Method}

\textcolor{blue}{The elbow method is used to select the optimal number of clusters if $k$ is not specified by the problems context.}

% rough
\textcolor{blue}{plot of the final cost function value is plotted against the value of $k$. As $k$ increases, the average distortion will decrease as each cluster will have fewer instances and they will be closer to their respective centroid. This decline in dispersion will decline most at a point and then slowly continue to decline -- the point at which the dispersion decreases the most can be considered the elbow (and considered by the elbow method to be the best value for $k$)}

\textcolor{green}{TODO: diagram showing data clusters}

\textcolor{green}{TODO: diagram showing $k$ vs dispersion related to above cluster diagram}


\subsubsection{Hierarchical Clustering}

\TD{overview}


\subsubsection{DBSCAN}

\TD{overview}



\subsection{Evaluating}

\textcolor{blue}{There are no labels - can still evaluate using intrinsic measures}

\textcolor{blue}{measuring distortion of clusters}

\subsubsection{Silhouette Coefficient}

\textcolor{blue}{The {silhouette coefficient}\index{silhouette coefficient} is a measure of compactness and separation of clusters. The silhoette coefficient is calculated per instance or for a set of instances (where the value is calculated to be the mean of the individual instances in the group). Eq.\ref{eq:silhouette_coef_def} can be used to calculate the silhouette per instance, where $d$ is the mean distance between the instance and the instances in the next closest cluster and $b$ is hte mean distance between the instances in the indicated cluster.}

\begin{equation}
{s = \frac{db}{max(d,b)}}
\label{eq:silhouette_coef_def}
\end{equation}





