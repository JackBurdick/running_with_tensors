\section{Dimensionality Reduction}

\textcolor{green}{todo: Overview}

\textcolor{blue}{Dimensionality reduction is typically used to reduce the dimensions in a feature representation while retaining as much information as possible.}

\textcolor{blue}{Motivation: i) mitigate issues caused by the curse of dimensionality ii) compress data iii) visualize and explore datasets and improve interpretability -- interpreting data in high dimensions is harder than in lower dimension spaces (particularly three or less). May also be used to compress data before being used by another learning algorithm.}

\textcolor{blue}{Example --- projecting 3D data into a 2D space.}

\subsection{Principal Component Analysis}

\textcolor{green}{{Principal Component Analysis (PCA)}\index{Principal Component Analysis (PCA)} may also be known as the {Karhunen-Love Transform (KLM)}\index{Karhunen-Love Transform (KLM)} }

%p232[220] of Masstering ML w/SKL
\textcolor{red}{``PCA is most useful when'the variance of the dataset is distributed unevenly across the dimensions'' }

% TODO: get var/covar defs (currently in regression tex)
\textcolor{blue}{carvariances between each par of dimensions in a dataset are described in a {covarience matrix}\index{covarience matrix} }