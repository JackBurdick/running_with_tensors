\section{Support Vector Machines (SVM)}

\textcolor{blue}{Support Vector Machine (SVM)\index{Support Vector Machine (SVM)}. In order to minimize misclassification errors, the optimization objective is to maximize the margin (distance between the decision boundary (separating hyperplane) and the nearest training samples. These margins are called support vectors). Maximizing the margins, in theory, tend to have lower generalization error, where smaller margins may be more prone to overfitting.}

\textcolor{blue}{(Slack parameter?)}

\textcolor{blue}{Variable can be used to control the width of the margin and help tune the bias-variance trade-off.}

\textcolor{green}{TODO: figure showing difference in width of margins}

\subsection{Kernel SVM}

\textcolor{blue}{kernelized to solve nonlinear classification problems}

\subsubsection{The `Kernel Trick'}

\textcolor{green}{TODO: paras about the kernel trick}

\textcolor{blue}{Transform the training data onto a higher dimensional feature space}