\section{Support Vector Machines (SVM)}

\textcolor{blue}{Support Vector Machine (SVM)\index{Support Vector Machine (SVM)}. In order to minimize misclassification errors, the optimization objective is to maximize the margin (distance between the decision boundary (separating hyperplane) and the nearest training samples. These margins are called support vectors). Maximizing the margins, in theory, tend to have lower generalization error, where smaller margins may be more prone to overfitting.}

\textcolor{blue}{Can be used for classification and regression}

\textcolor{blue}{(Slack parameter?)}

\textcolor{blue}{Variable can be used to control the width of the margin and help tune the bias-variance trade-off.}

\textcolor{green}{TODO: figure showing difference in width of margins}

\textcolor{blue}{potential negatives -- don't scale particularly well: large datasets may present runtime and memory complexity issues, careful preprocessing and parameter tuning is important.}

\subsection{Maximizing Geometric Margin}

% p183[171] of mastering ml with skl
\textcolor{red}{``quadratic programming problem''}

\subsubsection{Sequential Minimal Optimization}

\textcolor{blue}{Sequential Minimal Optimization (SMO)\index{Sequential Minimal Optimization (SMO)} is an algorithm used find the parameters that maximize the geometric margin}

\subsection{Kernel SVM}

\textcolor{blue}{kernelized to solve nonlinear classification problems}

\subsubsection{The `Kernel Trick'}

\textcolor{green}{TODO: paras about the kernel trick}

\textcolor{blue}{see sec \textcolor{red}{local ref} for more information on the `kernel trick'}

