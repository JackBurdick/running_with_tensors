%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Decision Trees
\section{Decision Trees}

\textcolor{blue}{\textcolor{green}{(TODO: revise this para!)} Decision trees make classification decisions based on a series of questions that separate the data into subsets. These questions are chained and result in a tree of questions/decisions where the leaves are considered pure i.e. they contain samples that belong to the same class.}

\textcolor{blue}{Minimum Description Length (MDL) - describes how ``deep'' the tree is allowed to grow.}

\textcolor{blue}{Importance of pruning -- Decision trees can be very deep and can easily lead to overfitting. To help prevent this situation, a limit is set for the maximal depth of a tree. }

\textcolor{blue}{The main advantage to using decision trees is that they are easily interpretable and may often resemble a program developed by hand. Another advantage is that decision trees do not require data to be standardized and they can also tolerate missing feature values.}

\textcolor{blue}{Can be used to determine which features are most useful, support multi-output tasks, and a single decision tree can be used for multi-class classification without having to employ strategies like one versus all.}

\textcolor{blue}{Leaf nodes specify the predicted value of a response variable. When decision trees are used for classification, the leaf nodes represent classes. When used for regression tasks, the leaf nodes are averaged to produce an estimate a response variable.}

% from coursera lec
\textcolor{red}{finding the optimal splitting for a tree is considered an NP-hard problem -- greedy algorithms are used to create splitting schemes that are as close to optimal as possible.}

%\textcolor{blue}{feature importance}

\subsection{Criterion -- Maximizing Information Gain}

\textcolor{blue}{Term - Information gain -- difference between the impurity of the parent node and the sum of the child node impurities -- the lower the impurity of the child nodes compared to the parent node, the higher the information gain}

\textcolor{blue}{Three commonly used splitting criteria used in binary decision trees: (i) Gini Impurity, (ii) entropy, and (iii) classification error}

\textcolor{blue}{There are no hard rules to determine which criterion to use in a particular instance.}

% see p148[136] of Mastering ML with SKL
\subsubsection{Gini Impurity}

\textcolor{blue}{Measures the proportions of classes in a set}

\textcolor{red}{is greatest when each classes has an equal probability of being selected, like entropy}

\subsubsection{Entropy}

% see p140[128] of Mastering ML with SKL
\textcolor{green}{Entropy is used to quantify the amount of uncertainty}

\paragraph{Information Gain}

% see p144[132] of Mastering ML with SKL
\textcolor{blue}{Information Gain is the measure of reduction in entropy and is the difference between the entropy of the parent node and the weighted average of the children nodes' entropies}

\subsubsection{Classification Error}

\subsubsection{ID3 Algorithm}

\textcolor{blue}{ID3 or ``Iterative Dichotomizer 3'', created by Ross Quinlan.}

\textcolor{blue}{considered greedy --- making locally optimal decisions, not necessarily globally optimal decisions}

\textcolor{blue}{top-down, recursive, depth-first partitioning of the dataset.}

\textcolor{blue}{where each feature is selected because the feature reduces uncertainty in the node more than the other features.}

\textcolor{blue}{Assumes categorical features without any missing values.}

\textcolor{red}{Can be extended to handle continuous descriptive features and continuous target features}

\TD{Quinlan, 1986}

\subsubsection{C4.5 Algorithm}

\textcolor{blue}{Variant of the {ID3 algorithm}\index{ID3 algorithm} that can handle continuous categorical descriptive features and missing features}

\textcolor{blue}{supports pruning}

\textcolor{red}{uses post-pruning}

\textcolor{blue}{{J48}\index{J48} is an open source implementation of the C4.5 algorithm}

\subsubsection{CART Algorithm}

\textcolor{blue}{The CART algorithm is another variant of the ID3 algorithm.}

\textcolor{blue}{Uses the Gini index}

\textcolor{blue}{Can handle continuous target features}

\TD{Breiman et al., 1984}

\subsection{Pruning}

\textcolor{blue}{Can help address overfitting. Pruning is an attempt to reduce the size of the tree while still maintaining a similar empirical error.}

\subsubsection{Pre-pruning}

\subsubsection{Post-pruning}

\section{Random Forests}

% see Breiman, 2001
\r{Ensemble method. Combine various decision trees, where each tree is slightly different. Further, some may be weak learners (\textcolor{red}{local ref}) and some may be strong learners (\textcolor{red}{local ref}). The final classification will be determined by majority vote from the number of trees.}

%% whoa, this needs to be re-written....
\r{The general idea behind using an ensemble of trees in a random forest, much like other ensemble methods (which are discussed more in depth in \textcolor{red}{local ref}) is that each component (decision tree in this case), though it may predict the data well, it is likely to overfit the training data, and each component, being slightly different may overfit on different features in the data and the amount overall overfitting is reduced by averaging their results.  each component (tree) randomized by either i) selecting the features to split on or ii) selecting the data to train on}

\r{``soft'' prediction -- each tree provides a probability for each possible output lable and the probabilities are averaged and the class with the highest probability is predicted.}

% usually the square root of the total number of predictors
\TD{number of predictors considered for each split}

% implementation note
\textcolor{blue}{It is important to set the random state and/or seed the random number generator as each random forest could have drastically different results. However, this randomness should decrease with the number of trees implemented in the forest as using more trees will lead to a more robust ensemble.}



