\section{Artificial Neural Networks (ANN)}

\textcolor{blue}{Principals and basic feed forward networks}

\textcolor{blue}{The most computationally expensive component is calculating the gradient of the loss function with respect to the parameters of the network}

% see page 233 of Understanding Machine Learning
\textcolor{blue}{Artificial neural networks are {universal approximators}\index{universal approximators} -- \textcolor{red}{expand}}

\textcolor{blue}{universal approximation theorem \textcolor{green}{(Hornik 1989, Cybenko, 1989)}. Regardless of the function that is attempted to being learned, a large MLP will be able to \textbf{represent} this function. However, it is not guaranteed that the large MLP, despite being a universal \textcolor{red}{approximator} capable of representing the function, is able to \textit{learn} the function}

\subsection{Multi-layer Perceptron}

\textcolor{blue}{Not a single multi-layer perceptron with multiple layers, rather it is a network composed of multiple layers of perceptrons.}

\subsection{Architecture}

\textcolor{blue}{{input layer}\index{input layer}, {hidden layer}\index{hidden layer}, {output layer}\index{output layer}}

\textcolor{blue}{the input layer is not counted in the number of layers in a network}

\textcolor{green}{TODO: diagram of neural network showing layers}

\subsection{Components}

\textcolor{green}{TODO: labeled diagram of nodes (weights and biases), connections, activation functions}

\subsubsection{Nodes / units}

\paragraph{Initialization}

\textcolor{green}{TODO: initialization methods and for different layers}



\subsubsection{Activation Function}

\textcolor{green}{TODO: I think this is where I'll talk about activation functions}

\textcolor{green}{TODO: explain the need for non-linearity.}

\textcolor{green}{TODO: step function to sigmoid function -- smoothed version of the step function -- can understand how an input changes the output.}


\subsection{Characterization}

\subsubsection{Types: Feed-forward vs Feedback}

\textcolor{blue}{Feed-forward --- Directed acyclic graph of artificial neurons. Feedback contain feedback connections that are fed back into itself. When feedforward are include these feedback connections, they become considered recurrent neural networks.}

\subsubsection{Terminology}

\textcolor{blue}{Considered \textit{networks} since they are typically composed of many different functions --- creating a ``network''.}

\textcolor{blue}{Considered \textit{neural} since they are \textbf{loosely} inspired by neuroscience.}

\textcolor{blue}{layer --- a layer may be considered a group of units that act in parallel. The layer will extract representations from the input, that are (in theory) more useful to the specific task.  Chaining together these layers results in a form of progressive {data distillation}\index{data distillation}.}

\textcolor{blue}{Visible and Hidden Layers. Visible layers are called visible since they contain variables that are ``visible'', where as the hidden layers extract increasingly abstract features -- hidden since their values are not given in the raw data, but rather an output from a previous layer.}



\subsection{Learning: Backpropagation}

% see p196[184] of Mastering ML w/SKL
\textcolor{green}{TODO: whoooo, this is going to be a big one. understand how each component contributes to the error and adjust accordingly.}

\textcolor{blue}{Iterative algorithm consisting of two main components --- in order, the forward, then the reverse pass}

\textcolor{blue}{In the forward pass inputs are propagated through the network}

\textcolor{blue}{In the Backward pass, errors from the forward function and  are propagated in reverse through the network (from cost function to input layer) and each node is updated -- \textcolor{green}{TODO: expand}.}

\subsubsection{Backward Propagation}

% see p197-201[180] of Mastering ML w/SKL

\textcolor{green}{TODO: figure showing sample calculation}


% See p207 of DL

\textcolor{blue}{also sometimes called {reverse-mode differentiation}\index{reverse-mode differentiation}.  Calculate the contribution that each parameter had on the loss value}

\textcolor{blue}{{symbolic differentiation}\index{symbolic differentiation} --- compute a gradient function for the chain (chain rule) mapping parameter values to gradient values}

\subsubsection{Chain Rule}

\textcolor{red}{See \textcolor{red}{local ref to math prereq section}}

\textcolor{green}{TODO: chain rule}

\textcolor{blue}{Backpropagation is typically used with an optimization algorithm (see \textcolor{red}{local ref?})}

\subsection{Multi-layer perceptrons}