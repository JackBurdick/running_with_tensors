\section{Nearest Neighbors}
\label{nearest_neighbors}

\textcolor{blue}{KNN is a lazy learner\index{lazy learner} (a special case of instance-based nonparametric model (see \textcolor{red}{local ref?})): the model memorizes the training dataset rather than learn a discriminative function. Uses instances that are nearest the test instance to predict the value of the response variable.}

\textcolor{blue}{The intuition (and subsequently the assumption made by this method) behind nearest neighbors is that instances look alike must be alike.}

\textcolor{blue}{Can be used for classification (binary, multi-class, multi-label) or regression. To adapt a KNN for regression, the approach is modified to return a prediction of the average target value from the nearest neighbors, rather than the majority label.}

\textcolor{blue}{Due to the curse-of-dimensionality, In practice, nearest neighbors will usually be performed after a dimensionality reduction preprocessing step.}

\TD{collaborative filtering}

\TD{content-based filtering}

%\subsection{Use Examples}

%\textcolor{blue}{search}
%\textcolor{blue}{Recommender systems}

\textcolor{blue}{Simple algorithm that neighbors are representations of training instances in a metric space}

\subsection{Overview}


% page 188 of FofMLforpred data analytics

\textcolor{blue}{set local models (neighborhoods), where each is defined by a subset of the training data (in nearest neighbors, this is a single instance).}

\textcolor{blue}{A global prediction model based on the full dataset creates a decision boundary between regions of the features space which designates the \textcolor{red}{label?(term)}}

\subsubsection{Distance metric}

\textcolor{blue}{A distance metric (See \textcolor{red}{local ref?} for more information) is used to determine the closeness or the distance between instances.}

\subsection{K-Nearest Neighbors}

\textcolor{blue}{Since the nearest neighbor algorithm relies on local models, each defined by a single training instance, it is quite sensitive to noise. To address this issue, rather than rely on a single instance, a \textcolor{red}{label} is calculated from a set of $k$ nearest neighbors}

\textcolor{blue}{KNN classification involves i) choosing the number of $k$ (nearest neighbors) and a distance metric, ii) finding the $k$ nearest neighbors, and iii) assigning a class label by majority vote.}

\textcolor{blue}{The optimal value for $k$ will greatly influence the bias-variance trade-off. If the value is too low, there exists a risk of the algorithm being too sensitive to noise and overfitting, where as if the value of $k$ is too high, there is the possibility that a true pattern will be lost.}

\textcolor{blue}{$k$ is usually defined as an odd number in order to prevent ties.}

\subsubsection{Regression Considerations}

\textcolor{blue}{Feature values are associated with a real value instead of a label. The prediction is the mean or weighted mean of response variable of the $k$ nearest neighbors.}

\subsection{Considering Imbalanced Data}

\textcolor{blue}{If dealing with imbalanced data, as $k$ increases, the majority label will dominate the feature space.}

\subsubsection{Weighted K-Nearest Neighbors}


\subsubsection{Distance Weighted K-Nearest Neighbors}

\textcolor{blue}{The weight of each instance is a function of the inverse distance to the instance from the specified location. An easy way to implement this is to calculate the reciprocal of the squared distance (Eq.~\ref{eq:weighted_dist_knn}), where $n$ is the neighbor and $m$ is the specified location}

\begin{equation}
{\frac{1}{{dist(m,n)}^2}}
\label{eq:weighted_dist_knn}
\end{equation}

\textcolor{blue}{When calculating weight of each instance, $k$ is set to be a large value and may even be equal to the number of instances in the training set so that all training instances are included in the prediction process.}

\textcolor{blue}{Votes from neighbors that are close to the specified location are assigned a high weight, while distant neighbors are assigned a lower weight value.}

\subsection{Considerations}

% see page 225 of Understanding Machine Learning

\r{}

\subsubsection{Memory}

\subsection{Other Variations}

% see p196 of FofMLforpred data analytics for more information
\textcolor{blue}{k-d tree, k-dimmensional tree -- balanced binary tree}

\textcolor{blue}{R-Trees}

\textcolor{blue}{B-Trees}

\textcolor{blue}{M-Trees}

\textcolor{blue}{VoRTrees}