%%%%% Optimizers

% adam optimizer paper
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

% Nadam optimizer paper
@article{dozat2016incorporating,
	title={Incorporating nesterov momentum into adam},
	author={Dozat, Timothy},
	year={2016}
}

% adadelta optimizer
@article{zeiler2012adadelta,
	title={ADADELTA: an adaptive learning rate method},
	author={Zeiler, Matthew D},
	journal={arXiv preprint arXiv:1212.5701},
	year={2012}
}

% adagrad optimizer
@article{duchi2011adaptive,
	title={Adaptive subgradient methods for online learning and stochastic optimization},
	author={Duchi, John and Hazan, Elad and Singer, Yoram},
	journal={Journal of Machine Learning Research},
	volume={12},
	number={Jul},
	pages={2121--2159},
	year={2011}
}

% momentum optimizer
@article{qian1999momentum,
	title={On the momentum term in gradient descent learning algorithms},
	author={Qian, Ning},
	journal={Neural networks},
	volume={12},
	number={1},
	pages={145--151},
	year={1999},
	publisher={Elsevier}
}


%%%%%%%%%%%%%%%%%%% Others

% snapshot ensembles
@article{huang2017snapshot,
	title={Snapshot ensembles: Train 1, get M for free},
	author={Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E and Weinberger, Kilian Q},
	journal={arXiv preprint arXiv:1704.00109},
	year={2017}
}

% cyclic learning rates
@inproceedings{smith2017cyclical,
	title={Cyclical learning rates for training neural networks},
	author={Smith, Leslie N},
	booktitle={Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on},
	pages={464--472},
	year={2017},
	organization={IEEE}
}

% sgd with restarts, cosine annealing
@article{loshchilov2016sgdr,
	title={SGDR: stochastic gradient descent with restarts},
	author={Loshchilov, Ilya and Hutter, Frank},
	journal={Learning},
	volume={10},
	pages={3},
	year={2016}
}

% small batch size (<32)
@article{masters2018revisiting,
	title={Revisiting Small Batch Training for Deep Neural Networks},
	author={Masters, Dominic and Luschi, Carlo},
	journal={arXiv preprint arXiv:1804.07612},
	year={2018}
}

% minibatch vs batch, small batches are better
@article{wilson2003general,
	title={The general inefficiency of batch training for gradient descent learning},
	author={Wilson, D Randall and Martinez, Tony R},
	journal={Neural Networks},
	volume={16},
	number={10},
	pages={1429--1451},
	year={2003},
	publisher={Elsevier}
}

% generalization gap, Ghost-BN, not the batch size, the number of updates
@inproceedings{hoffer2017train,
	title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
	author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1729--1739},
	year={2017}
}

% increasing batch size
@article{smith2017don,
	title={Don't Decay the Learning Rate, Increase the Batch Size},
	author={Smith, Samuel L and Kindermans, Pieter-Jan and Le, Quoc V},
	journal={arXiv preprint arXiv:1711.00489},
	year={2017}
}

% Pooling/pooling not being used
@article{ruderman2018learned,
	title={Learned Deformation Stability in Convolutional Neural Networks},
	author={Ruderman, Avraham and Rabinowitz, Neil and Morcos, Ari S and Zoran, Daniel},
	journal={arXiv preprint arXiv:1804.04438},
	year={2018}
}







