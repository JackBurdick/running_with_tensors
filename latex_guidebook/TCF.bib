%%%%% Optimizers

% adam optimizer paper
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

% Nadam optimizer paper
@article{dozat2016incorporating,
	title={Incorporating nesterov momentum into adam},
	author={Dozat, Timothy},
	year={2016}
}

% adadelta optimizer
@article{zeiler2012adadelta,
	title={ADADELTA: an adaptive learning rate method},
	author={Zeiler, Matthew D},
	journal={arXiv preprint arXiv:1212.5701},
	year={2012}
}

% adagrad optimizer
@article{duchi2011adaptive,
	title={Adaptive subgradient methods for online learning and stochastic optimization},
	author={Duchi, John and Hazan, Elad and Singer, Yoram},
	journal={Journal of Machine Learning Research},
	volume={12},
	number={Jul},
	pages={2121--2159},
	year={2011}
}

% momentum optimizer
@article{qian1999momentum,
	title={On the momentum term in gradient descent learning algorithms},
	author={Qian, Ning},
	journal={Neural networks},
	volume={12},
	number={1},
	pages={145--151},
	year={1999},
	publisher={Elsevier}
}



% snapshot ensembles
@article{huang2017snapshot,
	title={Snapshot ensembles: Train 1, get M for free},
	author={Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E and Weinberger, Kilian Q},
	journal={arXiv preprint arXiv:1704.00109},
	year={2017}
}

