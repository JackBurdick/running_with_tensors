% no free lunch
@article{wolpert1997no,
	title={No free lunch theorems for optimization},
	author={Wolpert, David H and Macready, William G and others},
	journal={IEEE transactions on evolutionary computation},
	volume={1},
	number={1},
	pages={67--82},
	year={1997}
}

%%%%% Optimizers

% adam optimizer paper
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

% Nadam optimizer paper
@article{dozat2016incorporating,
	title={Incorporating nesterov momentum into adam},
	author={Dozat, Timothy},
	year={2016}
}

% adadelta optimizer
@article{zeiler2012adadelta,
	title={ADADELTA: an adaptive learning rate method},
	author={Zeiler, Matthew D},
	journal={arXiv preprint arXiv:1212.5701},
	year={2012}
}

% adagrad optimizer
@article{duchi2011adaptive,
	title={Adaptive subgradient methods for online learning and stochastic optimization},
	author={Duchi, John and Hazan, Elad and Singer, Yoram},
	journal={Journal of Machine Learning Research},
	volume={12},
	number={Jul},
	pages={2121--2159},
	year={2011}
}

% momentum optimizer
@article{qian1999momentum,
	title={On the momentum term in gradient descent learning algorithms},
	author={Qian, Ning},
	journal={Neural networks},
	volume={12},
	number={1},
	pages={145--151},
	year={1999},
	publisher={Elsevier}
}


%%%%%%%%%%%%%%%%%%% Others

% snapshot ensembles
@article{huang2017snapshot,
	title={Snapshot ensembles: Train 1, get M for free},
	author={Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E and Weinberger, Kilian Q},
	journal={arXiv preprint arXiv:1704.00109},
	year={2017}
}

% cyclic learning rates
@inproceedings{smith2017cyclical,
	title={Cyclical learning rates for training neural networks},
	author={Smith, Leslie N},
	booktitle={Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on},
	pages={464--472},
	year={2017},
	organization={IEEE}
}

% sgd with restarts, cosine annealing
@article{loshchilov2016sgdr,
	title={SGDR: stochastic gradient descent with restarts},
	author={Loshchilov, Ilya and Hutter, Frank},
	journal={Learning},
	volume={10},
	pages={3},
	year={2016}
}

% small batch size (<32)
@article{masters2018revisiting,
	title={Revisiting Small Batch Training for Deep Neural Networks},
	author={Masters, Dominic and Luschi, Carlo},
	journal={arXiv preprint arXiv:1804.07612},
	year={2018}
}

% minibatch vs batch, small batches are better
@article{wilson2003general,
	title={The general inefficiency of batch training for gradient descent learning},
	author={Wilson, D Randall and Martinez, Tony R},
	journal={Neural Networks},
	volume={16},
	number={10},
	pages={1429--1451},
	year={2003},
	publisher={Elsevier}
}

% generalization gap, Ghost-BN, not the batch size, the number of updates
@inproceedings{hoffer2017train,
	title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
	author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1729--1739},
	year={2017}
}

% increasing batch size
@article{smith2017don,
	title={Don't Decay the Learning Rate, Increase the Batch Size},
	author={Smith, Samuel L and Kindermans, Pieter-Jan and Le, Quoc V},
	journal={arXiv preprint arXiv:1711.00489},
	year={2017}
}

% Pooling/pooling not being used
@article{ruderman2018learned,
	title={Learned Deformation Stability in Convolutional Neural Networks},
	author={Ruderman, Avraham and Rabinowitz, Neil and Morcos, Ari S and Zoran, Daniel},
	journal={arXiv preprint arXiv:1804.04438},
	year={2018}
}

%%%%%%%%%%%%%%%%%%% vizualizations
@Misc{deeplearnjs,
	title        = {deeplearn.js. [{O}nline]},
	howpublished = {\url{https://deeplearnjs.org/}},
	note         = {Accessed: 2018-06-21},
}

@Misc{tf_playground,
	title        = {A Neural Network Playground. [{O}nline]},
	howpublished = {\url{http://playground.tensorflow.org}},
	note         = {Accessed: 2018-06-21},
}

@Misc{convnet_js,
	title        = {ConvNetJS. [{O}nline]},
	howpublished = {\url{http://cs.stanford.edu/people/karpathy/convnetjs/}},
	note         = {Accessed: 2018-06-21},
}

%%%%%%%%%%%%%%%%%%% Cloud Providers

@Misc{cloudHW_amazon_aws,
	title        = {Amazon AWS. [{O}nline]},
	howpublished = {\url{https://aws.amazon.com/}},
	note         = {Accessed: 2018-06-21},
}

@Misc{cloudHW_micro_azure,
	title        = {Microsoft Azure. [{O}nline]},
	howpublished = {\url{https://azure.microsoft.com/en-us/pricing/details/virtual-machines/series/}},
	note         = {Accessed: 2018-06-21},
}

@Misc{cloudHW_google_cloud,
	title        = {Google Cloud. [{O}nline]},
	howpublished = {\url{https://cloud.google.com/gpu/}},
	note         = {Accessed: 2018-06-21},
}

@Misc{cloudHW_floydhub,
	title        = {FloydHub. [{O}nline]},
	howpublished = {\url{https://www.floydhub.com/}},
	note         = {Accessed: 2018-06-21},
}

@Misc{cloudHW_nvidia_cloud,
	title        = {Nvidia GPU Cloud. [{O}nline]},
	howpublished = {\url{https://www.nvidia.com/en-us/gpu-cloud/}},
	note         = {Accessed: 2018-06-21},
}

%%%%%%%% device placement
% automatic deviceplacement white paper, internal google API
@article{abadi2016tensorflow_device_placement,
	title={Tensorflow: Large-scale machine learning on heterogeneous distributed systems},
	author={Abadi, Mart{\'\i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and others},
	journal={arXiv preprint arXiv:1603.04467},
	year={2016}
}


% math
% derivatives
@book{griewank2008evaluating,
	title={Evaluating derivatives: principles and techniques of algorithmic differentiation},
	author={Griewank, Andreas and Walther, Andrea},
	volume={105},
	year={2008},
	publisher={Siam}
}

% optimizer book
@book{kochenderfer2019algorithms,
	title={Algorithms for Optimization},
	author={Kochenderfer, Mykel J and Wheeler, Tim A},
	year={2019},
	publisher={Mit Press}
}

% reverse accumulation
@article{linnainmaa1970representation,
	title={The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors},
	author={Linnainmaa, Seppo},
	journal={Master's Thesis (in Finnish), Univ. Helsinki},
	pages={6--7},
	year={1970}
}

% backpropagation paper..
@article{alber2018backprop,
	title={Backprop evolution},
	author={Alber, Maximilian and Bello, Irwan and Zoph, Barret and Kindermans, Pieter-Jan and Ramachandran, Prajit and Le, Quoc},
	journal={arXiv preprint arXiv:1808.02822},
	year={2018}
}


% neural network paper from 1990
@article{hansen1990neural,
	title={Neural network ensembles},
	author={Hansen, Lars Kai and Salamon, Peter},
	journal={IEEE Transactions on Pattern Analysis \& Machine Intelligence},
	number={10},
	pages={993--1001},
	year={1990},
	publisher={IEEE}
}

% reverse accumulation --> backpropagation algorithm
@article{rumelhart1988learning,
	title={Learning representations by back-propagating errors},
	author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J and others},
	journal={Cognitive modeling},
	volume={5},
	number={3},
	pages={1},
	year={1988}
}

% optimizer, adadelta
@article{zeiler2012adadelta,
	title={ADADELTA: an adaptive learning rate method},
	author={Zeiler, Matthew D},
	journal={arXiv preprint arXiv:1212.5701},
	year={2012}
}

% hypergradient descent, optimizer
@article{baydin2017online,
	title={Online learning rate adaptation with hypergradient descent},
	author={Baydin, Atilim Gunes and Cornish, Robert and Rubio, David Martinez and Schmidt, Mark and Wood, Frank},
	journal={arXiv preprint arXiv:1703.04782},
	year={2017}
}

% AMSGrad, optimizer
@article{reddi2019convergence,
	title={On the convergence of adam and beyond},
	author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
	journal={arXiv preprint arXiv:1904.09237},
	year={2019}
}
