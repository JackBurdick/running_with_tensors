\contentsline {part}{I\hspace {1em}Background}{1}
\contentsline {chapter}{\numberline {1}Introduction}{3}
\contentsline {section}{\numberline {1.1}Motivation for this text}{3}
\contentsline {section}{\numberline {1.2}What}{3}
\contentsline {subsection}{\numberline {1.2.1}Rule based vs ``learning''}{4}
\contentsline {subsection}{\numberline {1.2.2}High Level Overview}{4}
\contentsline {subsection}{\numberline {1.2.3}Deep Learning}{4}
\contentsline {subsubsection}{\numberline {1.2.3.1}Why Now?}{4}
\contentsline {section}{\numberline {1.3}Why ML}{6}
\contentsline {section}{\numberline {1.4}Notes}{6}
\contentsline {subsection}{\numberline {1.4.1}Target Audience}{6}
\contentsline {subsection}{\numberline {1.4.2}How to Read this Book}{6}
\contentsline {chapter}{\numberline {2}Resources and Communities}{7}
\contentsline {section}{\numberline {2.1}Online Communities}{7}
\contentsline {section}{\numberline {2.2}Blogs}{7}
\contentsline {section}{\numberline {2.3}Online Courses}{7}
\contentsline {section}{\numberline {2.4}Text Books}{7}
\contentsline {chapter}{\numberline {3}Prerequisites}{9}
\contentsline {section}{\numberline {3.1}Math Notation}{9}
\contentsline {section}{\numberline {3.2}Calculus}{9}
\contentsline {subsection}{\numberline {3.2.1}Derivatives}{11}
\contentsline {subsection}{\numberline {3.2.2}Chain Rule}{12}
\contentsline {section}{\numberline {3.3}Boolean Logic}{12}
\contentsline {section}{\numberline {3.4}Linear Algebra}{12}
\contentsline {subsection}{\numberline {3.4.1}Overview}{12}
\contentsline {subsubsection}{\numberline {3.4.1.1}Scalars (0D tensors)}{12}
\contentsline {subsubsection}{\numberline {3.4.1.2}Vectors (1D tensors)}{12}
\contentsline {subsubsection}{\numberline {3.4.1.3}Matrices (2D tensors)}{13}
\contentsline {subsection}{\numberline {3.4.2}Matrix Arithmetic}{13}
\contentsline {subsubsection}{\numberline {3.4.2.1}Matrices}{13}
\contentsline {paragraph}{\numberline {3.4.2.1.1}Addition, Subtraction}{13}
\contentsline {paragraph}{\numberline {3.4.2.1.2}Multiplication, Division}{13}
\contentsline {subsubsection}{\numberline {3.4.2.2}Scalar}{13}
\contentsline {paragraph}{\numberline {3.4.2.2.1}Addition, Subtraction}{13}
\contentsline {paragraph}{\numberline {3.4.2.2.2}Multiplication, Division}{13}
\contentsline {section}{\numberline {3.5}Graph Theory}{13}
\contentsline {part}{II\hspace {1em}Data}{15}
\contentsline {section}{\numberline {3.6}Bias}{18}
\contentsline {subsection}{\numberline {3.6.1}Types of Bias}{18}
\contentsline {subsubsection}{\numberline {3.6.1.1}Interaction Bias}{18}
\contentsline {subsubsection}{\numberline {3.6.1.2}Latent Bias}{18}
\contentsline {subsubsection}{\numberline {3.6.1.3}Selection Bias}{18}
\contentsline {subsubsection}{\numberline {3.6.1.4}Recency Bias}{18}
\contentsline {subsection}{\numberline {3.6.2}Evaluation}{18}
\contentsline {section}{\numberline {3.7}Data Acquisition}{18}
\contentsline {subsection}{\numberline {3.7.1}Resources}{18}
\contentsline {subsection}{\numberline {3.7.2}Public Indexing}{18}
\contentsline {subsection}{\numberline {3.7.3}Notable Database Sites}{18}
\contentsline {subsection}{\numberline {3.7.4}Datasets to be familiar with}{19}
\contentsline {subsubsection}{\numberline {3.7.4.1}Common}{19}
\contentsline {paragraph}{\numberline {3.7.4.1.1}MNIST}{19}
\contentsline {subsubsection}{\numberline {3.7.4.2}Others (referenced in this text)}{19}
\contentsline {subsection}{\numberline {3.7.5}Data Portal Search}{19}
\contentsline {subsection}{\numberline {3.7.6}Generating Fake Data}{19}
\contentsline {section}{\numberline {3.8}Data Preparation}{19}
\contentsline {subsection}{\numberline {3.8.1}Data Pre-processing}{21}
\contentsline {subsection}{\numberline {3.8.2}Handling Missing Data}{21}
\contentsline {subsubsection}{\numberline {3.8.2.1}Filtering Out}{21}
\contentsline {subsubsection}{\numberline {3.8.2.2}Filling In}{22}
\contentsline {subsubsection}{\numberline {3.8.2.3}Handling Categorical Data}{22}
\contentsline {paragraph}{\numberline {3.8.2.3.1}Encoding}{22}
\contentsline {subsubsection}{\numberline {3.8.2.4}Feature Scaling, Normalization}{22}
\contentsline {paragraph}{\numberline {3.8.2.4.1}Mean Normalization}{23}
\contentsline {paragraph}{\numberline {3.8.2.4.2}Min-Max scaling (Normalization)}{23}
\contentsline {paragraph}{\numberline {3.8.2.4.3}Standardization}{23}
\contentsline {subparagraph}{Robust Scaler}{23}
\contentsline {subsubsection}{\numberline {3.8.2.5}Others}{23}
\contentsline {paragraph}{\numberline {3.8.2.5.1}Removing Duplicates}{23}
\contentsline {paragraph}{\numberline {3.8.2.5.2}Outliers}{23}
\contentsline {paragraph}{\numberline {3.8.2.5.3}Discretization and Binning}{24}
\contentsline {subsubsection}{\numberline {3.8.2.6}Where to do preprocessing}{24}
\contentsline {subsubsection}{\numberline {3.8.2.7}Feature Engineering}{24}
\contentsline {section}{\numberline {3.9}Feature Extraction from Various Datatypes}{24}
\contentsline {subsection}{\numberline {3.9.1}Feature Engineering}{24}
\contentsline {subsubsection}{\numberline {3.9.1.1}Kernel}{24}
\contentsline {subsubsection}{\numberline {3.9.1.2}Feature Crosses}{25}
\contentsline {subsection}{\numberline {3.9.2}2D Images}{26}
\contentsline {subsection}{\numberline {3.9.3}3D Imagery}{26}
\contentsline {subsection}{\numberline {3.9.4}Video}{26}
\contentsline {subsection}{\numberline {3.9.5}Natural Language}{26}
\contentsline {subsubsection}{\numberline {3.9.5.1}Terminology}{26}
\contentsline {subsubsection}{\numberline {3.9.5.2}Pre-processing}{27}
\contentsline {paragraph}{\numberline {3.9.5.2.1}Stop Word Filtering}{27}
\contentsline {paragraph}{\numberline {3.9.5.2.2}Tokenization}{27}
\contentsline {paragraph}{\numberline {3.9.5.2.3}Lemmatization}{27}
\contentsline {paragraph}{\numberline {3.9.5.2.4}Stemming}{27}
\contentsline {subparagraph}{Porter Stemming}{27}
\contentsline {subsubsection}{\numberline {3.9.5.3}Encoding}{28}
\contentsline {paragraph}{\numberline {3.9.5.3.1}Encoding Methods}{28}
\contentsline {subparagraph}{Bag-of-Words}{28}
\contentsline {paragraph}{\numberline {3.9.5.3.2}tf-idf}{28}
\contentsline {subsubsection}{\numberline {3.9.5.4}Embedding}{28}
\contentsline {subparagraph}{glove}{28}
\contentsline {subparagraph}{word2vec}{28}
\contentsline {subsubsection}{\numberline {3.9.5.5}Other Notes}{28}
\contentsline {subsection}{\numberline {3.9.6}Audio}{28}
\contentsline {section}{\numberline {3.10}Partitioning Data}{28}
\contentsline {subsection}{\numberline {3.10.1}Types of Splits}{29}
\contentsline {subsection}{\numberline {3.10.2}k-Fold Cross Validation}{31}
\contentsline {subsubsection}{\numberline {3.10.2.1}k-Fold Validation with shuffling}{31}
\contentsline {subsection}{\numberline {3.10.3}Sampling}{31}
\contentsline {paragraph}{\numberline {3.10.3.0.1}Representative Data}{32}
\contentsline {paragraph}{\numberline {3.10.3.0.2}Representative Data}{32}
\contentsline {part}{III\hspace {1em}Foundations}{33}
\contentsline {chapter}{\numberline {4}Basics}{35}
\contentsline {section}{\numberline {4.1}Overview}{35}
\contentsline {section}{\numberline {4.2}Workflow Overview / Blue Print}{35}
\contentsline {section}{\numberline {4.3}Some Terms}{36}
\contentsline {section}{\numberline {4.4}Type of Learning}{37}
\contentsline {subsection}{\numberline {4.4.1}Supervised vs Unsupervised}{37}
\contentsline {subsection}{\numberline {4.4.2}Classification vs Regression}{37}
\contentsline {subsubsection}{\numberline {4.4.2.1}Regression}{37}
\contentsline {subsubsection}{\numberline {4.4.2.2}Classification}{38}
\contentsline {subsection}{\numberline {4.4.3}Multi-label classification}{38}
\contentsline {subsubsection}{\numberline {4.4.3.1}Approaches: Problem transformation}{38}
\contentsline {paragraph}{\numberline {4.4.3.1.1}Unique set/combination of labels}{38}
\contentsline {paragraph}{\numberline {4.4.3.1.2}Many Binary Classifiers}{39}
\contentsline {subsubsection}{\numberline {4.4.3.2}Evaluating Multi-label Classification}{39}
\contentsline {section}{\numberline {4.5}Training}{39}
\contentsline {subsection}{\numberline {4.5.1}Cost, Loss Function}{39}
\contentsline {subsection}{\numberline {4.5.2}Error Functions}{40}
\contentsline {subsubsection}{\numberline {4.5.2.1}forward pass}{40}
\contentsline {subsubsection}{\numberline {4.5.2.2}backward pass}{40}
\contentsline {subsubsection}{\numberline {4.5.2.3}Least-squares techniques}{40}
\contentsline {paragraph}{\numberline {4.5.2.3.1}Sum-of-squares error function}{40}
\contentsline {paragraph}{\numberline {4.5.2.3.2}Normal Equations}{40}
\contentsline {paragraph}{\numberline {4.5.2.3.3}Singular Value Decomposition (SVD)}{41}
\contentsline {paragraph}{\numberline {4.5.2.3.4}Gradient Descent}{41}
\contentsline {subsubsection}{\numberline {4.5.2.4}Global vs Local Minima}{41}
\contentsline {section}{\numberline {4.6}Quality of Fit}{42}
\contentsline {subsection}{\numberline {4.6.1}Regression Example}{42}
\contentsline {subsection}{\numberline {4.6.2}Classification Example}{42}
\contentsline {section}{\numberline {4.7}Describing Learners}{43}
\contentsline {subsection}{\numberline {4.7.1}Parametric and non-parametric}{43}
\contentsline {subsubsection}{\numberline {4.7.1.1}parametric}{43}
\contentsline {subsubsection}{\numberline {4.7.1.2}nonparametric}{43}
\contentsline {subsubsection}{\numberline {4.7.1.3}parametric vs nonparametric}{44}
\contentsline {subsection}{\numberline {4.7.2}Eager vs Lazy Learners}{44}
\contentsline {subsubsection}{\numberline {4.7.2.1}Eager Learners}{44}
\contentsline {subsubsection}{\numberline {4.7.2.2}Lazy Learners}{44}
\contentsline {subsection}{\numberline {4.7.3}Generative vs Discriminative Models}{44}
\contentsline {subsubsection}{\numberline {4.7.3.1}Discriminative Models}{45}
\contentsline {paragraph}{\numberline {4.7.3.1.1}Probabilistic Discriminative}{45}
\contentsline {paragraph}{\numberline {4.7.3.1.2}Non-probabilistic Discriminative}{45}
\contentsline {subsubsection}{\numberline {4.7.3.2}Generative Models}{45}
\contentsline {subsection}{\numberline {4.7.4}Strong vs Weak Learners}{45}
\contentsline {section}{\numberline {4.8}Online Learning}{46}
\contentsline {section}{\numberline {4.9}Kernel Methods}{46}
\contentsline {subsection}{\numberline {4.9.1}Kernel Trick}{46}
\contentsline {subsection}{\numberline {4.9.2}Kernels}{46}
\contentsline {subsubsection}{\numberline {4.9.2.1}Polynomial}{46}
\contentsline {subsubsection}{\numberline {4.9.2.2}Gaussian / RBF (Radial Basis Function)}{47}
\contentsline {subsection}{\numberline {4.9.3}Less Common Kernels}{47}
\contentsline {section}{\numberline {4.10}Hyper-Parameters}{47}
\contentsline {subsection}{\numberline {4.10.1}Parameters: "tuning knobs"}{47}
\contentsline {subsubsection}{\numberline {4.10.1.1}Learning Rate}{47}
\contentsline {paragraph}{\numberline {4.10.1.1.1}research}{47}
\contentsline {subsubsection}{\numberline {4.10.1.2}Batch size}{48}
\contentsline {subsection}{\numberline {4.10.2}Hyper-Parameter Optimization}{48}
\contentsline {subsubsection}{\numberline {4.10.2.1}Coordinate Descent}{48}
\contentsline {subsubsection}{\numberline {4.10.2.2}Grid Search}{48}
\contentsline {subsubsection}{\numberline {4.10.2.3}Randomized Search}{49}
\contentsline {subsubsection}{\numberline {4.10.2.4}Other Methods: Automated / Model-based Methods}{49}
\contentsline {paragraph}{\numberline {4.10.2.4.1}Bayesian Methods}{49}
\contentsline {chapter}{\numberline {5}Estimating Model Parameters}{51}
\contentsline {section}{\numberline {5.1}Initialization}{51}
\contentsline {subsection}{\numberline {5.1.1}Parameter types (the initialization of)}{51}
\contentsline {paragraph}{\numberline {5.1.1.0.1}Weights}{51}
\contentsline {paragraph}{\numberline {5.1.1.0.2}Biases}{51}
\contentsline {subsection}{\numberline {5.1.2}Normal Vs Uniform}{52}
\contentsline {subsection}{\numberline {5.1.3}Strategies}{52}
\contentsline {subsubsection}{\numberline {5.1.3.1}Glorot or Xavier}{52}
\contentsline {subsubsection}{\numberline {5.1.3.2}he}{52}
\contentsline {subsubsection}{\numberline {5.1.3.3}Implementation}{52}
\contentsline {chapter}{\numberline {6}Optimization}{53}
\contentsline {section}{\numberline {6.1}Parameterized}{53}
\contentsline {subsection}{\numberline {6.1.1}Descent Direction Methods}{53}
\contentsline {subsection}{\numberline {6.1.2}First-order}{54}
\contentsline {subsubsection}{\numberline {6.1.2.1}Gradient Descent}{54}
\contentsline {paragraph}{\numberline {6.1.2.1.1}Batch Gradient Descent}{54}
\contentsline {paragraph}{\numberline {6.1.2.1.2}Stochastic Gradient Descent}{54}
\contentsline {paragraph}{\numberline {6.1.2.1.3}Mini-batch Gradient Descent}{55}
\contentsline {subsubsection}{\numberline {6.1.2.2}Conjugate Gradient}{55}
\contentsline {subsubsection}{\numberline {6.1.2.3}Momentum Descent}{55}
\contentsline {subsubsection}{\numberline {6.1.2.4}Nesterov Momentum Descent}{55}
\contentsline {subsubsection}{\numberline {6.1.2.5}Adagrad Descent}{56}
\contentsline {paragraph}{\numberline {6.1.2.5.1}Adagrad Extensions: (RMSProp, Adadelta, Adam)}{56}
\contentsline {subparagraph}{RMSProp}{56}
\contentsline {subparagraph}{Adadelta}{56}
\contentsline {subparagraph}{Adam}{56}
\contentsline {subsubsection}{\numberline {6.1.2.6}AdaMax}{56}
\contentsline {subsubsection}{\numberline {6.1.2.7}Hypergradient Descent}{56}
\contentsline {subsubsection}{\numberline {6.1.2.8}FTRL}{57}
\contentsline {subsection}{\numberline {6.1.3}Nadam}{57}
\contentsline {subsubsection}{\numberline {6.1.3.1}AMSGrad}{57}
\contentsline {subsection}{\numberline {6.1.4}second-order}{57}
\contentsline {subsubsection}{\numberline {6.1.4.1}Newton's Method}{57}
\contentsline {subsubsection}{\numberline {6.1.4.2}Secant Method}{57}
\contentsline {subsubsection}{\numberline {6.1.4.3}Quasi-Newton Method}{57}
\contentsline {section}{\numberline {6.2}Non-Parameterized}{57}
\contentsline {subsection}{\numberline {6.2.1}Direct methods}{57}
\contentsline {subsection}{\numberline {6.2.2}Stochastic methods}{58}
\contentsline {subsection}{\numberline {6.2.3}Population methods}{58}
\contentsline {subsection}{\numberline {6.2.4}Further optimization information}{58}
\contentsline {subsection}{\numberline {6.2.5}Parallelizing and distributing SGD}{58}
\contentsline {chapter}{\numberline {7}Genetic Algorithms}{59}
\contentsline {chapter}{\numberline {8}Evaluation}{61}
\contentsline {subsection}{\numberline {8.0.1}Creating a Test Set}{61}
\contentsline {subsection}{\numberline {8.0.2}Qualitative Evaluation}{62}
\contentsline {subsubsection}{\numberline {8.0.2.1}(Over$|$Under)fitting and Capacity}{62}
\contentsline {paragraph}{\numberline {8.0.2.1.1}Overfitting}{62}
\contentsline {paragraph}{\numberline {8.0.2.1.2}Underfitting}{62}
\contentsline {subparagraph}{Solution}{63}
\contentsline {subsubsection}{\numberline {8.0.2.2}Bias Variance Trade-off}{63}
\contentsline {paragraph}{\numberline {8.0.2.2.1}Variance}{63}
\contentsline {paragraph}{\numberline {8.0.2.2.2}Bias}{63}
\contentsline {paragraph}{\numberline {8.0.2.2.3}Trade-Off}{64}
\contentsline {subsection}{\numberline {8.0.3}Qualitative Evalutation: Performance Metrics}{65}
\contentsline {subsubsection}{\numberline {8.0.3.1}Confusion Matrix}{65}
\contentsline {subsubsection}{\numberline {8.0.3.2}Classification Metrics}{66}
\contentsline {subsubsection}{\numberline {8.0.3.3}Precision-Recall curve}{68}
\contentsline {subsubsection}{\numberline {8.0.3.4}Regression Metrics}{68}
\contentsline {subsubsection}{\numberline {8.0.3.5}Additional Metrics}{69}
\contentsline {paragraph}{\numberline {8.0.3.5.1}Linear Evaluation}{69}
\contentsline {paragraph}{\numberline {8.0.3.5.2}Distance Metrics}{69}
\contentsline {paragraph}{\numberline {8.0.3.5.3}Multi-label Classification}{70}
\contentsline {subsubsection}{\numberline {8.0.3.6}Choosing the "right" metrics}{70}
\contentsline {chapter}{\numberline {9}Improving Generalizability}{71}
\contentsline {subsection}{\numberline {9.0.1}Parameter Regularization}{71}
\contentsline {subsubsection}{\numberline {9.0.1.1}Why Regularization}{71}
\contentsline {subsubsection}{\numberline {9.0.1.2}Types of Regularization}{71}
\contentsline {subsubsection}{\numberline {9.0.1.3}Early Stopping}{72}
\contentsline {subsubsection}{\numberline {9.0.1.4}Parameter Norm Penalties}{72}
\contentsline {paragraph}{\numberline {9.0.1.4.1}L2 Regularization}{72}
\contentsline {paragraph}{\numberline {9.0.1.4.2}L1 Regularization}{73}
\contentsline {paragraph}{\numberline {9.0.1.4.3}Elastic Net Regularization}{73}
\contentsline {subsection}{\numberline {9.0.2}Dataset Augmentation}{73}
\contentsline {subsubsection}{\numberline {9.0.2.1}Two Dimensional Data}{74}
\contentsline {subsubsection}{\numberline {9.0.2.2}Learning Augmentation}{74}
\contentsline {paragraph}{\numberline {9.0.2.2.1}AutoAugment}{74}
\contentsline {subsection}{\numberline {9.0.3}Dropout}{74}
\contentsline {subsection}{\numberline {9.0.4}Ensemble Methods}{76}
\contentsline {subsection}{\numberline {9.0.5}Adversarial Training}{76}
\contentsline {subsection}{\numberline {9.0.6}Transfer Learning}{76}
\contentsline {subsubsection}{\numberline {9.0.6.1}Normalization}{77}
\contentsline {paragraph}{\numberline {9.0.6.1.1}Instance normalization}{77}
\contentsline {paragraph}{\numberline {9.0.6.1.2}Layer normalization}{77}
\contentsline {paragraph}{\numberline {9.0.6.1.3}Batch normalization}{77}
\contentsline {paragraph}{\numberline {9.0.6.1.4}Group normalization}{78}
\contentsline {section}{\numberline {9.1}Adversarial Examples}{78}
\contentsline {section}{\numberline {9.2}Output regularization}{78}
\contentsline {chapter}{\numberline {10}Distributed Methods}{79}
\contentsline {subsubsection}{\numberline {10.0.0.1}Data Parallelism}{79}
\contentsline {subsubsection}{\numberline {10.0.0.2}Model Parallelism}{79}
\contentsline {subsubsection}{\numberline {10.0.0.3}Federated learning}{79}
\contentsline {chapter}{\numberline {11}Ethics}{81}
\contentsline {section}{\numberline {11.1}Overview}{81}
\contentsline {section}{\numberline {11.2}Pieces to fit together}{81}
\contentsline {section}{\numberline {11.3}Should something be published}{81}
\contentsline {section}{\numberline {11.4}Further information}{81}
\contentsline {section}{\numberline {11.5}bias}{82}
\contentsline {section}{\numberline {11.6}Thoughts}{82}
\contentsline {part}{IV\hspace {1em}Algorithms}{83}
\contentsline {chapter}{\numberline {12}Foundational Methods}{85}
\contentsline {section}{\numberline {12.1}Regression}{85}
\contentsline {subsection}{\numberline {12.1.1}Simple Linear Regression}{85}
\contentsline {subsubsection}{\numberline {12.1.1.1}OLS}{86}
\contentsline {subsubsection}{\numberline {12.1.1.2}Cost}{86}
\contentsline {subsubsection}{\numberline {12.1.1.3}Evaluation}{87}
\contentsline {subsection}{\numberline {12.1.2}Multiple Linear Regression}{87}
\contentsline {subsection}{\numberline {12.1.3}Polynomial Regression}{87}
\contentsline {section}{\numberline {12.2}Logistic Regression}{88}
\contentsline {section}{\numberline {12.3}Nearest Neighbors}{88}
\contentsline {subsection}{\numberline {12.3.1}Overview}{89}
\contentsline {subsubsection}{\numberline {12.3.1.1}Distance metric}{89}
\contentsline {subsection}{\numberline {12.3.2}K-Nearest Neighbors}{89}
\contentsline {subsubsection}{\numberline {12.3.2.1}Regression Considerations}{89}
\contentsline {subsection}{\numberline {12.3.3}Considering Imbalanced Data}{90}
\contentsline {subsubsection}{\numberline {12.3.3.1}Weighted K-Nearest Neighbors}{90}
\contentsline {subsubsection}{\numberline {12.3.3.2}Distance Weighted K-Nearest Neighbors}{90}
\contentsline {subsection}{\numberline {12.3.4}Considerations}{90}
\contentsline {subsubsection}{\numberline {12.3.4.1}Memory}{90}
\contentsline {subsection}{\numberline {12.3.5}Other Variations}{90}
\contentsline {section}{\numberline {12.4}Support Vector Machines (SVM)}{91}
\contentsline {subsection}{\numberline {12.4.1}Maximizing Geometric Margin}{91}
\contentsline {subsubsection}{\numberline {12.4.1.1}Sequential Minimal Optimization}{91}
\contentsline {subsection}{\numberline {12.4.2}Kernel SVM}{91}
\contentsline {subsubsection}{\numberline {12.4.2.1}The `Kernel Trick'}{91}
\contentsline {section}{\numberline {12.5}Naive Bayes}{91}
\contentsline {subsection}{\numberline {12.5.1}Bayes' Theorem}{92}
\contentsline {section}{\numberline {12.6}Decision Trees}{92}
\contentsline {subsection}{\numberline {12.6.1}Criterion -- Maximizing Information Gain}{93}
\contentsline {subsubsection}{\numberline {12.6.1.1}Gini Impurity}{93}
\contentsline {subsubsection}{\numberline {12.6.1.2}Entropy}{93}
\contentsline {paragraph}{\numberline {12.6.1.2.1}Information Gain}{93}
\contentsline {subsubsection}{\numberline {12.6.1.3}Classification Error}{94}
\contentsline {subsubsection}{\numberline {12.6.1.4}ID3 Algorithm}{94}
\contentsline {subsubsection}{\numberline {12.6.1.5}C4.5 Algorithm}{94}
\contentsline {subsubsection}{\numberline {12.6.1.6}CART Algorithm}{94}
\contentsline {subsection}{\numberline {12.6.2}Pruning}{94}
\contentsline {subsubsection}{\numberline {12.6.2.1}Pre-pruning}{95}
\contentsline {subsubsection}{\numberline {12.6.2.2}Post-pruning}{95}
\contentsline {section}{\numberline {12.7}Random Forests}{95}
\contentsline {chapter}{\numberline {13}Artificial Neural Networks}{97}
\contentsline {section}{\numberline {13.1}Perceptron}{97}
\contentsline {subsection}{\numberline {13.1.1}History}{97}
\contentsline {subsection}{\numberline {13.1.2}Overview}{98}
\contentsline {subsection}{\numberline {13.1.3}Activation Function Basics}{98}
\contentsline {subsection}{\numberline {13.1.4}Limitations}{99}
\contentsline {subsection}{\numberline {13.1.5}Extending to model linearly inseparable data}{99}
\contentsline {subsubsection}{\numberline {13.1.5.1}Kernelization}{99}
\contentsline {subsubsection}{\numberline {13.1.5.2}Directed Graph}{99}
\contentsline {subsection}{\numberline {13.1.6}Notes}{100}
\contentsline {section}{\numberline {13.2}Artificial Neural Networks (ANN)}{100}
\contentsline {subsection}{\numberline {13.2.1}Multi-layer Perceptron}{100}
\contentsline {subsection}{\numberline {13.2.2}Architecture}{100}
\contentsline {subsection}{\numberline {13.2.3}Components}{101}
\contentsline {subsubsection}{\numberline {13.2.3.1}Nodes / units}{101}
\contentsline {paragraph}{\numberline {13.2.3.1.1}Initialization}{101}
\contentsline {subsubsection}{\numberline {13.2.3.2}Activation Function}{101}
\contentsline {subsubsection}{\numberline {13.2.3.3}Why Non-linear}{103}
\contentsline {subsubsection}{\numberline {13.2.3.4}Advancements}{103}
\contentsline {subsubsection}{\numberline {13.2.3.5}Popular Activation Functions}{103}
\contentsline {paragraph}{\numberline {13.2.3.5.1}Smooth Non-linear}{103}
\contentsline {subparagraph}{Sigmoid}{103}
\contentsline {subparagraph}{ELU}{103}
\contentsline {subparagraph}{Softplus}{106}
\contentsline {paragraph}{\numberline {13.2.3.5.2}Not Smooth Non-linear}{106}
\contentsline {subparagraph}{ReLU}{106}
\contentsline {subparagraph}{Leaky ReLU}{107}
\contentsline {subparagraph}{ReLU6}{107}
\contentsline {subparagraph}{PReLU}{108}
\contentsline {subsection}{\numberline {13.2.4}Characterization}{109}
\contentsline {subsubsection}{\numberline {13.2.4.1}Types: Feed-forward vs Feedback}{109}
\contentsline {paragraph}{\numberline {13.2.4.1.1}Feed-forward}{109}
\contentsline {subparagraph}{Layered networks}{109}
\contentsline {subparagraph}{General topologies}{109}
\contentsline {paragraph}{\numberline {13.2.4.1.2}Feedback}{111}
\contentsline {subsubsection}{\numberline {13.2.4.2}Terminology}{111}
\contentsline {subsection}{\numberline {13.2.5}Learning: Backpropagation}{111}
\contentsline {paragraph}{\numberline {13.2.5.0.1}Forward pass}{111}
\contentsline {paragraph}{\numberline {13.2.5.0.2}Backward pass}{112}
\contentsline {subsubsection}{\numberline {13.2.5.1}Back-propagation efficiency}{112}
\contentsline {subsubsection}{\numberline {13.2.5.2}Chain Rule}{112}
\contentsline {subsection}{\numberline {13.2.6}Multi-layer perceptrons}{112}
\contentsline {subsection}{\numberline {13.2.7}Operations}{112}
\contentsline {subsection}{\numberline {13.2.8}Convolution}{112}
\contentsline {subsection}{\numberline {13.2.9}Pooling}{113}
\contentsline {section}{\numberline {13.3}Feed-forward}{115}
\contentsline {section}{\numberline {13.4}Feedback or Recurrent}{115}
\contentsline {subsection}{\numberline {13.4.1}Foundation}{116}
\contentsline {subsection}{\numberline {13.4.2}Simple RNN and Recurrent Neuron}{116}
\contentsline {subparagraph}{Overview}{116}
\contentsline {section}{\numberline {13.5}Common Problems}{116}
\contentsline {subsection}{\numberline {13.5.1}Maintaining States}{116}
\contentsline {subsection}{\numberline {13.5.2}Addressing Vanishing and Exploding Gradients}{116}
\contentsline {section}{\numberline {13.6}Architecture}{117}
\contentsline {subsection}{\numberline {13.6.1}Cell Advancements}{117}
\contentsline {subsubsection}{\numberline {13.6.1.1}LSTM}{117}
\contentsline {paragraph}{\numberline {13.6.1.1.1}Fully Connected Layers}{117}
\contentsline {subparagraph}{Main}{117}
\contentsline {subparagraph}{Forget}{117}
\contentsline {subparagraph}{Input}{119}
\contentsline {subparagraph}{Output}{119}
\contentsline {paragraph}{\numberline {13.6.1.1.2}Other}{119}
\contentsline {subparagraph}{Peephole Connections}{119}
\contentsline {subsubsection}{\numberline {13.6.1.2}GRU}{120}
\contentsline {subsection}{\numberline {13.6.2}Initialization}{120}
\contentsline {subsection}{\numberline {13.6.3}Activation Functions}{120}
\contentsline {subsection}{\numberline {13.6.4}Notes -- add}{120}
\contentsline {chapter}{\numberline {14}Common Operations}{121}
\contentsline {section}{\numberline {14.1}Dense}{121}
\contentsline {section}{\numberline {14.2}Convolutions}{121}
\contentsline {section}{\numberline {14.3}Pooling}{121}
\contentsline {section}{\numberline {14.4}Recurrent Cells}{121}
\contentsline {section}{\numberline {14.5}Capsule Networks}{121}
\contentsline {section}{\numberline {14.6}Attention}{122}
\contentsline {section}{\numberline {14.7}NTM (Neural Turing Machines)}{122}
\contentsline {chapter}{\numberline {15}Applied Neural Networks}{123}
\contentsline {section}{\numberline {15.1}Single Modality}{123}
\contentsline {subsection}{\numberline {15.1.1}N-Dimensional Structure}{123}
\contentsline {subsubsection}{\numberline {15.1.1.1}One-Dimensional Structure (\textit {e.g.} sequences, text, etc.)}{123}
\contentsline {paragraph}{\numberline {15.1.1.1.1}Sequence to Sequence}{123}
\contentsline {subparagraph}{Overview}{123}
\contentsline {subparagraph}{advancements}{124}
\contentsline {paragraph}{\numberline {15.1.1.1.2}Sequence to Vector}{124}
\contentsline {subparagraph}{Overview}{124}
\contentsline {paragraph}{\numberline {15.1.1.1.3}Vector to Sequence}{124}
\contentsline {subparagraph}{Overview}{124}
\contentsline {paragraph}{\numberline {15.1.1.1.4}Delayed Sequence to Sequence}{124}
\contentsline {subsubsection}{\numberline {15.1.1.2}Two-Dimensional Structure (\textit {e.g.} Imagery)}{125}
\contentsline {subsubsection}{\numberline {15.1.1.3}N-Dimensional Structure (\textit {e.g.} Video)}{125}
\contentsline {section}{\numberline {15.2}Multimodal}{125}
\contentsline {subsection}{\numberline {15.2.1}N-Dimensional Structure}{125}
\contentsline {chapter}{\numberline {16}Unsupervised}{127}
\contentsline {subsection}{\numberline {16.0.1}TODO}{127}
\contentsline {section}{\numberline {16.1}Clustering}{128}
\contentsline {subsection}{\numberline {16.1.1}Common Algorithms}{128}
\contentsline {subsubsection}{\numberline {16.1.1.1}K-means}{128}
\contentsline {paragraph}{\numberline {16.1.1.1.1}Local Optima}{129}
\contentsline {paragraph}{\numberline {16.1.1.1.2}Selecting K}{129}
\contentsline {paragraph}{\numberline {16.1.1.1.3}Elbow Method}{129}
\contentsline {subsubsection}{\numberline {16.1.1.2}Hierarchical Clustering}{129}
\contentsline {subsubsection}{\numberline {16.1.1.3}DBSCAN}{130}
\contentsline {subsubsection}{\numberline {16.1.1.4}HDBSCAN}{130}
\contentsline {subsection}{\numberline {16.1.2}Evaluating}{130}
\contentsline {subsubsection}{\numberline {16.1.2.1}Silhouette Coefficient}{130}
\contentsline {section}{\numberline {16.2}Dimensionality Reduction}{130}
\contentsline {subsection}{\numberline {16.2.1}Principal Component Analysis}{131}
\contentsline {subsubsection}{\numberline {16.2.1.1}Linear}{131}
\contentsline {paragraph}{\numberline {16.2.1.1.1}Incremental PCA}{131}
\contentsline {paragraph}{\numberline {16.2.1.1.2}Sparse PCA}{132}
\contentsline {subsubsection}{\numberline {16.2.1.2}Nonlinear}{132}
\contentsline {paragraph}{\numberline {16.2.1.2.1}Kernel PCA}{132}
\contentsline {subsection}{\numberline {16.2.2}Singular value decomposition (SVD)}{132}
\contentsline {subsection}{\numberline {16.2.3}Random Projection}{132}
\contentsline {subsubsection}{\numberline {16.2.3.1}Gaussian Random Projection}{132}
\contentsline {subsubsection}{\numberline {16.2.3.2}Sparse Random Projection}{132}
\contentsline {section}{\numberline {16.3}Nonlinear dimensionality reduction}{133}
\contentsline {subsection}{\numberline {16.3.1}Isomap}{133}
\contentsline {subsection}{\numberline {16.3.2}Multidimensional Scaling (MDS)}{133}
\contentsline {subsection}{\numberline {16.3.3}Locally Linear Embedding (LLE)}{133}
\contentsline {subsection}{\numberline {16.3.4}t-Distributed Stochastic Neighbor Embedding (t-SNE)}{133}
\contentsline {section}{\numberline {16.4}Non-geometric, no distance metric}{134}
\contentsline {subsection}{\numberline {16.4.1}Dictionary Learning}{134}
\contentsline {subsection}{\numberline {16.4.2}Independent Component Analysis (ICA)}{134}
\contentsline {subsection}{\numberline {16.4.3}TODO: others}{134}
\contentsline {subsection}{\numberline {16.4.4}Autoencoders}{134}
\contentsline {paragraph}{\numberline {16.4.4.0.1}Undercomplete vs Overcomplete}{135}
\contentsline {subsection}{\numberline {16.4.5}Generative Adversarial Networks}{135}
\contentsline {subsection}{\numberline {16.4.6}Hidden Markov Model}{135}
\contentsline {chapter}{\numberline {17}Semi-supervised}{137}
\contentsline {section}{\numberline {17.1}Semi-Supervised}{137}
\contentsline {subsection}{\numberline {17.1.1}Examples}{137}
\contentsline {chapter}{\numberline {18}Common Architectures}{139}
\contentsline {subsection}{\numberline {18.0.1}Image}{139}
\contentsline {subsubsection}{\numberline {18.0.1.1}Image Classification}{139}
\contentsline {subsection}{\numberline {18.0.2}Object Detection}{140}
\contentsline {subsection}{\numberline {18.0.3}Semantic Segmentation}{140}
\contentsline {subsection}{\numberline {18.0.4}Instance Segmentation}{140}
\contentsline {section}{\numberline {18.1}Text: Natural Language Processing}{140}
\contentsline {section}{\numberline {18.2}Structured data: Tablular}{140}
\contentsline {section}{\numberline {18.3}Other Common Architectures}{140}
\contentsline {part}{V\hspace {1em}Ensembling}{141}
\contentsline {chapter}{\numberline {19}Ensemble Methods}{143}
\contentsline {section}{\numberline {19.1}Overview}{143}
\contentsline {subsection}{\numberline {19.1.1}Approaches to Creating Ensembles}{143}
\contentsline {subsubsection}{\numberline {19.1.1.1}Bagging}{143}
\contentsline {subsubsection}{\numberline {19.1.1.2}Boosting}{144}
\contentsline {paragraph}{\numberline {19.1.1.2.1}Examples}{144}
\contentsline {subparagraph}{AdaBoost}{144}
\contentsline {subsubsection}{\numberline {19.1.1.3}Bagging Vs Boosting}{145}
\contentsline {subsection}{\numberline {19.1.2}Stacking}{145}
\contentsline {subsection}{\numberline {19.1.3}Examples}{145}
\contentsline {subsubsection}{\numberline {19.1.3.1}Random Forests}{145}
\contentsline {chapter}{\numberline {20}Term dump}{147}
\contentsline {subsection}{\numberline {20.0.1}Distributions}{147}
\contentsline {part}{VI\hspace {1em}multitask}{149}
\contentsline {chapter}{\numberline {21}Multi-Task Learning}{151}
\contentsline {section}{\numberline {21.1}Overview}{151}
\contentsline {subsection}{\numberline {21.1.1}Same loss function, different data distribution}{151}
\contentsline {subsection}{\numberline {21.1.2}Different loss function}{152}
\contentsline {subsection}{\numberline {21.1.3}training network}{152}
\contentsline {subsection}{\numberline {21.1.4}conditioning task descriptor}{152}
\contentsline {subsection}{\numberline {21.1.5}optimizing the objective}{153}
\contentsline {section}{\numberline {21.2}Relationships}{154}
\contentsline {subsection}{\numberline {21.2.1}Architecture}{154}
\contentsline {subsection}{\numberline {21.2.2}loss}{155}
\contentsline {subsection}{\numberline {21.2.3}Training Dynamics}{155}
\contentsline {section}{\numberline {21.3}interesting considerations}{155}
\contentsline {section}{\numberline {21.4}Parameter Sharing}{155}
\contentsline {subsection}{\numberline {21.4.1}Hard parameter sharing}{155}
\contentsline {subsection}{\numberline {21.4.2}Soft parameter sharing}{156}
\contentsline {subsection}{\numberline {21.4.3}Other}{156}
\contentsline {section}{\numberline {21.5}Mechanisms}{156}
\contentsline {part}{VII\hspace {1em}meta-learning}{159}
\contentsline {chapter}{\numberline {22}Meta Learning}{161}
\contentsline {section}{\numberline {22.1}Overview}{161}
\contentsline {subsection}{\numberline {22.1.1}k-shot learning}{161}
\contentsline {section}{\numberline {22.2}evaluation}{161}
\contentsline {section}{\numberline {22.3}Adaptation approaches}{162}
\contentsline {subsection}{\numberline {22.3.1}Black-Box Adaptation}{162}
\contentsline {section}{\numberline {22.4}Optimization-based meta-learning}{162}
\contentsline {section}{\numberline {22.5}Non-Parametric Few-Shot Learning}{163}
\contentsline {section}{\numberline {22.6}Meta-Learning Algorithms}{163}
\contentsline {part}{VIII\hspace {1em}Brief Reference}{165}
\contentsline {chapter}{\numberline {23}Optimization}{167}
\contentsline {section}{\numberline {23.1}Quantization}{167}
\contentsline {subsection}{\numberline {23.1.1}When}{167}
\contentsline {subsubsection}{\numberline {23.1.1.1}during training}{167}
\contentsline {subsubsection}{\numberline {23.1.1.2}post-training}{167}
\contentsline {section}{\numberline {23.2}Weight Pruning}{167}
\contentsline {section}{\numberline {23.3}Topology}{167}
\contentsline {subsection}{\numberline {23.3.1}Distillation}{168}
\contentsline {subsection}{\numberline {23.3.2}Tensor Decomposition}{168}
\contentsline {chapter}{\numberline {24}Mistakes and Bloopers}{169}
\contentsline {chapter}{\numberline {25}Environment}{171}
\contentsline {chapter}{\numberline {26}Common Libraries}{173}
\contentsline {part}{IX\hspace {1em}Moving Forward}{175}
\contentsline {section}{\numberline {26.1}What's Next}{177}
\contentsline {part}{X\hspace {1em}End To End Examples}{179}
\contentsline {chapter}{\numberline {27}EndToEnd}{181}
\contentsline {section}{\numberline {27.1}Structured}{181}
\contentsline {subsection}{\numberline {27.1.1}Linear Regression}{181}
\contentsline {subsection}{\numberline {27.1.2}KNN}{181}
\contentsline {section}{\numberline {27.2}Image}{181}
\contentsline {subsection}{\numberline {27.2.1}Image Classification}{181}
\contentsline {subsection}{\numberline {27.2.2}Image Segmentation}{181}
\contentsline {subsection}{\numberline {27.2.3}Adversarial Exmaples}{181}
\contentsline {subsection}{\numberline {27.2.4}AutoEncoder}{181}
\contentsline {subsection}{\numberline {27.2.5}Generative Adversarial Network}{181}
\contentsline {section}{\numberline {27.3}TimeSeries}{182}
\contentsline {section}{\numberline {27.4}Text}{182}
\contentsline {subsection}{\numberline {27.4.1}Sentiment Analysis}{182}
\contentsline {section}{\numberline {27.5}Audio}{182}
\contentsline {subsection}{\numberline {27.5.1}Audio to Text}{182}
\contentsline {part}{XI\hspace {1em}Yeahml}{183}
\contentsline {chapter}{\numberline {28}Overview}{185}
\contentsline {chapter}{\numberline {29}How To}{187}
\contentsline {part}{XII\hspace {1em}appendix}{189}
\contentsline {chapter}{\numberline {30}TODO:}{191}
\contentsline {section}{\numberline {30.1}Imputing Missing Values}{191}
\contentsline {section}{\numberline {30.2}Anomaly Detection}{191}
\contentsline {subsection}{\numberline {30.2.1}methods}{191}
\contentsline {subsubsection}{\numberline {30.2.1.1}PCA}{191}
\contentsline {section}{\numberline {30.3}labeling data}{192}
\contentsline {section}{\numberline {30.4}Group Segmentation}{192}
\contentsline {part}{XIII\hspace {1em}Dump Space}{193}
\contentsline {subsection}{\numberline {30.4.1}Restricted Boltzmann Machines}{198}
\contentsline {subsection}{\numberline {30.4.2}Greek letters}{200}
\contentsline {subsection}{\numberline {30.4.3}Basic Log Math}{200}
\contentsline {subsection}{\numberline {30.4.4}Basic Matrix Math}{200}
\contentsline {subsection}{\numberline {30.4.5}Basic Linear Algebra}{200}
\contentsline {subsection}{\numberline {30.4.6}Representation Learning}{200}
\contentsline {subsection}{\numberline {30.4.7}others}{200}
\contentsline {section}{\numberline {30.5}research to include}{201}
\contentsline {subsection}{\numberline {30.5.1}Adversarial Examples}{201}
\contentsline {subsection}{\numberline {30.5.2}GANs}{201}
\contentsline {part}{XIV\hspace {1em}Research}{203}
\contentsline {chapter}{\numberline {31}Ongoing Research}{205}
