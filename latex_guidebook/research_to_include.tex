\section{research to include}


\TD{A Survey on Multi-Task Learning \cite{DBLP:journals/corr/ZhangY17aa}}


%% 
\TD{On Calibration of Modern Neural Networks \cite{DBLP:journals/corr/GuoPSW17}}


%% 
\TD{synthetic petri dish -- inner and outer loop \cite{rawal2020synthetic}}


% use style gan to create augmented data.
\TD{DermGAN: Synthetic Generation of Clinical Skin Images with Pathology \cite{Ghorbani2019DermGANSG}}

\TD{Visual attention \cite{DBLP:journals/corr/XuBKCCSZB15}}

% TODO: index for ptr-nets / pointer networks
\TD{Pointer Networks (Ptr-Nets) \cite{Vinyals2015PointerN}}

\TD{Non-local Neural Networks \cite{DBLP:journals/corr/abs-1711-07971}}

% augmented AI
\TD{Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance \cite{Bansal2020DoesTW}}


% blog: https://ai.googleblog.com/2020/06/spinenet-novel-architecture-for-object.html 
\TD{SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization \cite{Du2019SpineNetLS}}


% TODO: https://arxiv.org/abs/2006.16668 --- tomorrow?

% https://twitter.com/zacharynado/status/1276252197915942927
% ~``improve calibration on dataset shift''
\TD{Evaluating Prediction-Time Batch Normalization for Robustness under Covariate Shift \cite{Nado2020EvaluatingPB}}

% 1000s of tasks with little forgetting: https://twitter.com/Mitchnw/status/1278711255977492482
\TD{Supermasks in Superposition \cite{Wortsman2020SupermasksIS}}


% no normalization or skip connections - image classification, https://github.com/HaozhiQi/ISONet
\TD{Deep Isometric Learning for Visual Recognition \cite{Qi2020DeepIL}}


% training on synthetic data
\TD{Synthetic Data for Deep Learning \cite{Nikolenko2019SyntheticDF}}


% ``heads learn redundant key/query projections'' --> share
% https://github.com/epfml/collaborative-attention
\TD{Multi-Head Attention: Collaborate Instead of Concatenate \cite{Cordonnier2020MultiHeadAC}}

% Sparsely-Gated MoE > 600B -- significant: 1T weights, but issues --
\TD{GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding \cite{Lepikhin2020GShardSG}}


% multi-task === task relatedness
\TD{Learning Task Relatedness in Multi-Task Learning for Images in Context \cite{DBLP:journals/corr/abs-1904-03011}}

% 
\TD{Bag of Tricks for Image Classification with Convolutional Neural Networks \cite{DBLP:journals/corr/abs-1812-01187}}

% TODO: read https://openai.com/blog/deep-double-descent/
\TD{Deep Double Descent: Where Bigger Models and More Data Hurt \cite{Nakkiran2020DeepDD}}


% TODO: group convolutions: https://colah.github.io/posts/2014-12-Groups-Convolution/


% TODO: create section for Test-Time Augmentation (TTA)
% TODO: index TTA
\TD{Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation \cite{Molchanov2020GreedyPS} (TTA)}



% NOTE: possible paper on hyper parameter optimization
\TD{Bayesian Optimization for Selecting Efficient Machine Learning Models \cite{Wang2020BayesianOF}}

% 
\TD{The Hardware Lottery \cite{Hooker2020TheHL}}


% RigL -- training sparse networks
\TD{Rigging the Lottery: Making All Tickets Winners \cite{Evci2019RiggingTL}}

\TD{Perceiver: General Perception with Iterative Attention \cite{Jaegle2021PerceiverGP}}
\TD{pretraining with \textit{random} images, i.e. not Imagenet --- Self-supervised Pretraining of Visual Features in the Wild \cite{Goyal2021SelfsupervisedPO}}

\TD{RegNet --- Designing Network Design Spaces \cite{Radosavovic2020DesigningND}}

Conncurrent papers released looking to replace attention with MLPs.
\r{\begin{itemize}[noitemsep,topsep=0pt]
		\item\TD{ResMLP: Feedforward networks for image classification with data-efficient training \cite{Touvron2021ResMLPFN}}
		\item \TD{RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition \cite{Ding2021RepMLPRC}}
		\item \TD{Pay Attention to MLPs \cite{Liu2021PayAT}}
		\item \TD{Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet \cite{MelasKyriazi2021DoYE}}
	\end{itemize}
}


