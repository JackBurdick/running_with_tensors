\begin{thebibliography}{10}

\bibitem{cloudHW_amazon_aws}
{\em Amazon aws. [{O}nline]}.
\newblock \url{https://aws.amazon.com/}.
\newblock Accessed: 2018-06-21.

\bibitem{convnet_js}
{\em Convnetjs. [{O}nline]}.
\newblock \url{http://cs.stanford.edu/people/karpathy/convnetjs/}.
\newblock Accessed: 2018-06-21.

\bibitem{deeplearnjs}
{\em deeplearn.js. [{O}nline]}.
\newblock \url{https://deeplearnjs.org/}.
\newblock Accessed: 2018-06-21.

\bibitem{cloudHW_floydhub}
{\em Floydhub. [{O}nline]}.
\newblock \url{https://www.floydhub.com/}.
\newblock Accessed: 2018-06-21.

\bibitem{cloudHW_google_cloud}
{\em Google cloud. [{O}nline]}.
\newblock \url{https://cloud.google.com/gpu/}.
\newblock Accessed: 2018-06-21.

\bibitem{cloudHW_micro_azure}
{\em Microsoft azure. [{O}nline]}.
\newblock
  \url{https://azure.microsoft.com/en-us/pricing/details/virtual-machines/series/}.
\newblock Accessed: 2018-06-21.

\bibitem{tf_playground}
{\em A neural network playground. [{O}nline]}.
\newblock \url{http://playground.tensorflow.org}.
\newblock Accessed: 2018-06-21.

\bibitem{cloudHW_nvidia_cloud}
{\em Nvidia gpu cloud. [{O}nline]}.
\newblock \url{https://www.nvidia.com/en-us/gpu-cloud/}.
\newblock Accessed: 2018-06-21.

\bibitem{abadi2016tensorflow_device_placement}
{\sc M.~Abadi, A.~Agarwal, P.~Barham, E.~Brevdo, Z.~Chen, C.~Citro, G.~S.
  Corrado, A.~Davis, J.~Dean, M.~Devin, et~al.}, {\em Tensorflow: Large-scale
  machine learning on heterogeneous distributed systems}, arXiv preprint
  arXiv:1603.04467,  (2016).

\bibitem{dozat2016incorporating}
{\sc T.~Dozat}, {\em Incorporating nesterov momentum into adam},  (2016).

\bibitem{duchi2011adaptive}
{\sc J.~Duchi, E.~Hazan, and Y.~Singer}, {\em Adaptive subgradient methods for
  online learning and stochastic optimization}, Journal of Machine Learning
  Research, 12 (2011), pp.~2121--2159.

\bibitem{hoffer2017train}
{\sc E.~Hoffer, I.~Hubara, and D.~Soudry}, {\em Train longer, generalize
  better: closing the generalization gap in large batch training of neural
  networks}, in Advances in Neural Information Processing Systems, 2017,
  pp.~1729--1739.

\bibitem{huang2017snapshot}
{\sc G.~Huang, Y.~Li, G.~Pleiss, Z.~Liu, J.~E. Hopcroft, and K.~Q. Weinberger},
  {\em Snapshot ensembles: Train 1, get m for free}, arXiv preprint
  arXiv:1704.00109,  (2017).

\bibitem{kingma2014adam}
{\sc D.~P. Kingma and J.~Ba}, {\em Adam: A method for stochastic optimization},
  arXiv preprint arXiv:1412.6980,  (2014).

\bibitem{loshchilov2016sgdr}
{\sc I.~Loshchilov and F.~Hutter}, {\em Sgdr: stochastic gradient descent with
  restarts}, Learning, 10 (2016), p.~3.

\bibitem{masters2018revisiting}
{\sc D.~Masters and C.~Luschi}, {\em Revisiting small batch training for deep
  neural networks}, arXiv preprint arXiv:1804.07612,  (2018).

\bibitem{qian1999momentum}
{\sc N.~Qian}, {\em On the momentum term in gradient descent learning
  algorithms}, Neural networks, 12 (1999), pp.~145--151.

\bibitem{ruderman2018learned}
{\sc A.~Ruderman, N.~Rabinowitz, A.~S. Morcos, and D.~Zoran}, {\em Learned
  deformation stability in convolutional neural networks}, arXiv preprint
  arXiv:1804.04438,  (2018).

\bibitem{smith2017cyclical}
{\sc L.~N. Smith}, {\em Cyclical learning rates for training neural networks},
  in Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on,
  IEEE, 2017, pp.~464--472.

\bibitem{smith2017don}
{\sc S.~L. Smith, P.-J. Kindermans, and Q.~V. Le}, {\em Don't decay the
  learning rate, increase the batch size}, arXiv preprint arXiv:1711.00489,
  (2017).

\bibitem{wilson2003general}
{\sc D.~R. Wilson and T.~R. Martinez}, {\em The general inefficiency of batch
  training for gradient descent learning}, Neural Networks, 16 (2003),
  pp.~1429--1451.

\bibitem{zeiler2012adadelta}
{\sc M.~D. Zeiler}, {\em Adadelta: an adaptive learning rate method}, arXiv
  preprint arXiv:1212.5701,  (2012).

\end{thebibliography}
