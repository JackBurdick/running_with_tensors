\begin{thebibliography}{100}

\bibitem{cloudHW_amazon_aws}
{\em Amazon aws. [{O}nline]}.
\newblock \url{https://aws.amazon.com/}.
\newblock Accessed: 2018-06-21.

\bibitem{convnet_js}
{\em Convnetjs. [{O}nline]}.
\newblock \url{http://cs.stanford.edu/people/karpathy/convnetjs/}.
\newblock Accessed: 2018-06-21.

\bibitem{deeplearnjs}
{\em deeplearn.js. [{O}nline]}.
\newblock \url{https://deeplearnjs.org/}.
\newblock Accessed: 2018-06-21.

\bibitem{cloudHW_floydhub}
{\em Floydhub. [{O}nline]}.
\newblock \url{https://www.floydhub.com/}.
\newblock Accessed: 2018-06-21.

\bibitem{cloudHW_google_cloud}
{\em Google cloud. [{O}nline]}.
\newblock \url{https://cloud.google.com/gpu/}.
\newblock Accessed: 2018-06-21.

\bibitem{cloudHW_micro_azure}
{\em Microsoft azure. [{O}nline]}.
\newblock
  \url{https://azure.microsoft.com/en-us/pricing/details/virtual-machines/series/}.
\newblock Accessed: 2018-06-21.

\bibitem{tf_playground}
{\em A neural network playground. [{O}nline]}.
\newblock \url{http://playground.tensorflow.org}.
\newblock Accessed: 2018-06-21.

\bibitem{cloudHW_nvidia_cloud}
{\em Nvidia gpu cloud. [{O}nline]}.
\newblock \url{https://www.nvidia.com/en-us/gpu-cloud/}.
\newblock Accessed: 2018-06-21.

\bibitem{abadi2016tensorflow_device_placement}
{\sc M.~Abadi, A.~Agarwal, P.~Barham, E.~Brevdo, Z.~Chen, C.~Citro, G.~S.
  Corrado, A.~Davis, J.~Dean, M.~Devin, et~al.}, {\em Tensorflow: Large-scale
  machine learning on heterogeneous distributed systems}, arXiv preprint
  arXiv:1603.04467,  (2016).

\bibitem{alber2018backprop}
{\sc M.~Alber, I.~Bello, B.~Zoph, P.-J. Kindermans, P.~Ramachandran, and
  Q.~Le}, {\em Backprop evolution}, arXiv preprint arXiv:1808.02822,  (2018).

\bibitem{Bahdanau2015NeuralMT}
{\sc D.~Bahdanau, K.~Cho, and Y.~Bengio}, {\em Neural machine translation by
  jointly learning to align and translate}, CoRR, abs/1409.0473 (2015).

\bibitem{DBLP:journals/corr/abs-1106-0245}
{\sc J.~Baxter}, {\em A model of inductive bias learning}, CoRR, abs/1106.0245
  (2011).

\bibitem{baydin2017online}
{\sc A.~G. Baydin, R.~Cornish, D.~M. Rubio, M.~Schmidt, and F.~Wood}, {\em
  Online learning rate adaptation with hypergradient descent}, arXiv preprint
  arXiv:1703.04782,  (2017).

\bibitem{DBLP:journals/corr/BritzGLL17}
{\sc D.~Britz, A.~Goldie, M.~Luong, and Q.~V. Le}, {\em Massive exploration of
  neural machine translation architectures}, CoRR, abs/1703.03906 (2017).

\bibitem{bromley1994signature}
{\sc J.~Bromley, I.~Guyon, Y.~LeCun, E.~S{\"a}ckinger, and R.~Shah}, {\em
  Signature verification using a" siamese" time delay neural network}, in
  Advances in neural information processing systems, 1994, pp.~737--744.

\bibitem{caruana1993multitask}
{\sc R.~Caruana}, {\em Multitask learning: A knowledge-based source of
  inductive bias icml}, Google Scholar Google Scholar Digital Library Digital
  Library,  (1993).

\bibitem{DBLP:journals/corr/ChenCZH17}
{\sc W.~Chen, X.~Chen, J.~Zhang, and K.~Huang}, {\em Beyond triplet loss: a
  deep quadruplet network for person re-identification}, CoRR, abs/1704.01719
  (2017).

\bibitem{DBLP:journals/corr/ChengDL16}
{\sc J.~Cheng, L.~Dong, and M.~Lapata}, {\em Long short-term memory-networks
  for machine reading}, CoRR, abs/1601.06733 (2016).

\bibitem{cubuk2018autoaugment}
{\sc E.~D. Cubuk, B.~Zoph, D.~Mane, V.~Vasudevan, and Q.~V. Le}, {\em
  Autoaugment: Learning augmentation policies from data}, arXiv preprint
  arXiv:1805.09501,  (2018).

\bibitem{devries2017improved}
{\sc T.~DeVries and G.~W. Taylor}, {\em Improved regularization of
  convolutional neural networks with cutout}, arXiv preprint arXiv:1708.04552,
  (2017).

\bibitem{dozat2016incorporating}
{\sc T.~Dozat}, {\em Incorporating nesterov momentum into adam},  (2016).

\bibitem{duchi2011adaptive}
{\sc J.~Duchi, E.~Hazan, and Y.~Singer}, {\em Adaptive subgradient methods for
  online learning and stochastic optimization}, Journal of Machine Learning
  Research, 12 (2011), pp.~2121--2159.

\bibitem{duong2015low}
{\sc L.~Duong, T.~Cohn, S.~Bird, and P.~Cook}, {\em Low resource dependency
  parsing: Cross-lingual parameter sharing in a neural network parser}, in
  Proceedings of the 53rd Annual Meeting of the Association for Computational
  Linguistics and the 7th International Joint Conference on Natural Language
  Processing (Volume 2: Short Papers), 2015, pp.~845--850.

\bibitem{garnelo2018conditional}
{\sc M.~Garnelo, D.~Rosenbaum, C.~J. Maddison, T.~Ramalho, D.~Saxton,
  M.~Shanahan, Y.~W. Teh, D.~J. Rezende, and S.~Eslami}, {\em Conditional
  neural processes}, arXiv preprint arXiv:1807.01613,  (2018).

\bibitem{Ghorbani2019DermGANSG}
{\sc A.~Ghorbani, V.~Natarajan, D.~Coz, and Y.~Liu}, {\em Dermgan: Synthetic
  generation of clinical skin images with pathology}, ArXiv, abs/1911.08716
  (2019).

\bibitem{glorot2010understanding}
{\sc X.~Glorot and Y.~Bengio}, {\em Understanding the difficulty of training
  deep feedforward neural networks}, in Proceedings of the thirteenth
  international conference on artificial intelligence and statistics, 2010,
  pp.~249--256.

\bibitem{DBLP:journals/corr/GravesWD14}
{\sc A.~Graves, G.~Wayne, and I.~Danihelka}, {\em Neural turing machines},
  CoRR, abs/1410.5401 (2014).

\bibitem{griewank2008evaluating}
{\sc A.~Griewank and A.~Walther}, {\em Evaluating derivatives: principles and
  techniques of algorithmic differentiation}, vol.~105, Siam, 2008.

\bibitem{guo2016entity}
{\sc C.~Guo and F.~Berkhahn}, {\em Entity embeddings of categorical variables},
  arXiv preprint arXiv:1604.06737,  (2016).

\bibitem{ha2018world}
{\sc D.~Ha and J.~Schmidhuber}, {\em World models}, arXiv preprint
  arXiv:1803.10122,  (2018).

\bibitem{hansen1990neural}
{\sc L.~K. Hansen and P.~Salamon}, {\em Neural network ensembles}, IEEE
  Transactions on Pattern Analysis \& Machine Intelligence,  (1990),
  pp.~993--1001.

\bibitem{DBLP:journals/corr/HashimotoXTS16}
{\sc K.~Hashimoto, C.~Xiong, Y.~Tsuruoka, and R.~Socher}, {\em A joint
  many-task model: Growing a neural network for multiple {NLP} tasks}, CoRR,
  abs/1611.01587 (2016).

\bibitem{he2015delving}
{\sc K.~He, X.~Zhang, S.~Ren, and J.~Sun}, {\em Delving deep into rectifiers:
  Surpassing human-level performance on imagenet classification}, in
  Proceedings of the IEEE international conference on computer vision, 2015,
  pp.~1026--1034.

\bibitem{hinton2015distilling}
{\sc G.~Hinton, O.~Vinyals, and J.~Dean}, {\em Distilling the knowledge in a
  neural network}, arXiv preprint arXiv:1503.02531,  (2015).

\bibitem{ho2019population}
{\sc D.~Ho, E.~Liang, I.~Stoica, P.~Abbeel, and X.~Chen}, {\em Population based
  augmentation: Efficient learning of augmentation policy schedules}, arXiv
  preprint arXiv:1905.05393,  (2019).

\bibitem{hoffer2017train}
{\sc E.~Hoffer, I.~Hubara, and D.~Soudry}, {\em Train longer, generalize
  better: closing the generalization gap in large batch training of neural
  networks}, in Advances in Neural Information Processing Systems, 2017,
  pp.~1729--1739.

\bibitem{hornik1991approximation}
{\sc K.~Hornik}, {\em Approximation capabilities of multilayer feedforward
  networks}, Neural networks, 4 (1991), pp.~251--257.

\bibitem{huang2017snapshot}
{\sc G.~Huang, Y.~Li, G.~Pleiss, Z.~Liu, J.~E. Hopcroft, and K.~Q. Weinberger},
  {\em Snapshot ensembles: Train 1, get m for free}, arXiv preprint
  arXiv:1704.00109,  (2017).

\bibitem{inoue2018data}
{\sc H.~Inoue}, {\em Data augmentation by pairing samples for images
  classification}, arXiv preprint arXiv:1801.02929,  (2018).

\bibitem{DBLP:journals/corr/KendallGC17}
{\sc A.~Kendall, Y.~Gal, and R.~Cipolla}, {\em Multi-task learning using
  uncertainty to weigh losses for scene geometry and semantics}, CoRR,
  abs/1705.07115 (2017).

\bibitem{kingma2014adam}
{\sc D.~P. Kingma and J.~Ba}, {\em Adam: A method for stochastic optimization},
  arXiv preprint arXiv:1412.6980,  (2014).

\bibitem{kirillov2019panoptic}
{\sc A.~Kirillov, R.~Girshick, K.~He, and P.~Doll{\'a}r}, {\em Panoptic feature
  pyramid networks}, in Proceedings of the IEEE Conference on Computer Vision
  and Pattern Recognition, 2019, pp.~6399--6408.

\bibitem{kochenderfer2019algorithms}
{\sc M.~J. Kochenderfer and T.~A. Wheeler}, {\em Algorithms for Optimization},
  Mit Press, 2019.

\bibitem{lecun2012efficient}
{\sc Y.~A. LeCun, L.~Bottou, G.~B. Orr, and K.-R. M{\"u}ller}, {\em Efficient
  backprop}, in Neural networks: Tricks of the trade, Springer, 2012,
  pp.~9--48.

\bibitem{lemley2017smart}
{\sc J.~Lemley, S.~Bazrafkan, and P.~Corcoran}, {\em Smart augmentation
  learning an optimal data augmentation strategy}, Ieee Access, 5 (2017),
  pp.~5858--5869.

\bibitem{lim2019fast}
{\sc S.~Lim, I.~Kim, T.~Kim, C.~Kim, and S.~Kim}, {\em Fast autoaugment}, arXiv
  preprint arXiv:1905.00397,  (2019).

\bibitem{linnainmaa1970representation}
{\sc S.~Linnainmaa}, {\em The representation of the cumulative rounding error
  of an algorithm as a taylor expansion of the local rounding errors}, Master's
  Thesis (in Finnish), Univ. Helsinki,  (1970), pp.~6--7.

\bibitem{DBLP:journals/corr/abs-1803-10704}
{\sc S.~Liu, E.~Johns, and A.~J. Davison}, {\em End-to-end multi-task learning
  with attention}, CoRR, abs/1803.10704 (2018).

\bibitem{DBLP:journals/corr/Long015a}
{\sc M.~Long and J.~Wang}, {\em Learning multiple tasks with deep relationship
  networks}, CoRR, abs/1506.02117 (2015).

\bibitem{loshchilov2016sgdr}
{\sc I.~Loshchilov and F.~Hutter}, {\em Sgdr: stochastic gradient descent with
  restarts}, Learning, 10 (2016), p.~3.

\bibitem{DBLP:journals/corr/LuKZCJF16}
{\sc Y.~Lu, A.~Kumar, S.~Zhai, Y.~Cheng, T.~Javidi, and R.~S. Feris}, {\em
  Fully-adaptive feature sharing in multi-task networks with applications in
  person attribute classification}, CoRR, abs/1611.05377 (2016).

\bibitem{DBLP:journals/corr/LuongPM15}
{\sc M.~Luong, H.~Pham, and C.~D. Manning}, {\em Effective approaches to
  attention-based neural machine translation}, CoRR, abs/1508.04025 (2015).

\bibitem{ma2018modeling}
{\sc J.~Ma, Z.~Zhao, X.~Yi, J.~Chen, L.~Hong, and E.~H. Chi}, {\em Modeling
  task relationships in multi-task learning with multi-gate
  mixture-of-experts}, in Proceedings of the 24th ACM SIGKDD International
  Conference on Knowledge Discovery \& Data Mining, 2018, pp.~1930--1939.

\bibitem{masters2018revisiting}
{\sc D.~Masters and C.~Luschi}, {\em Revisiting small batch training for deep
  neural networks}, arXiv preprint arXiv:1804.07612,  (2018).

\bibitem{DBLP:journals/corr/MisraSGH16}
{\sc I.~Misra, A.~Shrivastava, A.~Gupta, and M.~Hebert}, {\em Cross-stitch
  networks for multi-task learning}, CoRR, abs/1604.03539 (2016).

\bibitem{miyato2018virtual}
{\sc T.~Miyato, S.-i. Maeda, M.~Koyama, and S.~Ishii}, {\em Virtual adversarial
  training: a regularization method for supervised and semi-supervised
  learning}, IEEE transactions on pattern analysis and machine intelligence, 41
  (2018), pp.~1979--1993.

\bibitem{munkhdalai2017meta}
{\sc T.~Munkhdalai and H.~Yu}, {\em Meta networks}, in Proceedings of the 34th
  International Conference on Machine Learning-Volume 70, JMLR. org, 2017,
  pp.~2554--2563.

\bibitem{papernot2018deep}
{\sc N.~Papernot and P.~McDaniel}, {\em Deep k-nearest neighbors: Towards
  confident, interpretable and robust deep learning}, arXiv preprint
  arXiv:1803.04765,  (2018).

\bibitem{pereyra2017regularizing}
{\sc G.~Pereyra, G.~Tucker, J.~Chorowski, {\L}.~Kaiser, and G.~Hinton}, {\em
  Regularizing neural networks by penalizing confident output distributions},
  arXiv preprint arXiv:1701.06548,  (2017).

\bibitem{qian1999momentum}
{\sc N.~Qian}, {\em On the momentum term in gradient descent learning
  algorithms}, Neural networks, 12 (1999), pp.~145--151.

\bibitem{Radford2015UnsupervisedRL}
{\sc A.~Radford, L.~Metz, and S.~Chintala}, {\em Unsupervised representation
  learning with deep convolutional generative adversarial networks}, CoRR,
  abs/1511.06434 (2015).

\bibitem{rawal2020synthetic}
{\sc A.~Rawal, J.~Lehman, F.~P. Such, J.~Clune, and K.~O. Stanley}, {\em
  Synthetic petri dish: A novel surrogate model for rapid architecture search},
  arXiv preprint arXiv:2005.13092,  (2020).

\bibitem{reddi2019convergence}
{\sc S.~J. Reddi, S.~Kale, and S.~Kumar}, {\em On the convergence of adam and
  beyond}, arXiv preprint arXiv:1904.09237,  (2019).

\bibitem{reed2014training}
{\sc S.~Reed, H.~Lee, D.~Anguelov, C.~Szegedy, D.~Erhan, and A.~Rabinovich},
  {\em Training deep neural networks on noisy labels with bootstrapping}, arXiv
  preprint arXiv:1412.6596,  (2014).

\bibitem{DBLP:journals/corr/Ruder17a}
{\sc S.~Ruder}, {\em An overview of multi-task learning in deep neural
  networks}, CoRR, abs/1706.05098 (2017).

\bibitem{ruder2019latent}
{\sc S.~Ruder, J.~Bingel, I.~Augenstein, and A.~S{\o}gaard}, {\em Latent
  multi-task architecture learning}, in Proceedings of the AAAI Conference on
  Artificial Intelligence, vol.~33, 2019, pp.~4822--4829.

\bibitem{ruderman2018learned}
{\sc A.~Ruderman, N.~Rabinowitz, A.~S. Morcos, and D.~Zoran}, {\em Learned
  deformation stability in convolutional neural networks}, arXiv preprint
  arXiv:1804.04438,  (2018).

\bibitem{rumelhart1988learning}
{\sc D.~E. Rumelhart, G.~E. Hinton, R.~J. Williams, et~al.}, {\em Learning
  representations by back-propagating errors}, Cognitive modeling, 5 (1988),
  p.~1.

\bibitem{salimans2016weight}
{\sc T.~Salimans and D.~P. Kingma}, {\em Weight normalization: A simple
  reparameterization to accelerate training of deep neural networks}, in
  Advances in Neural Information Processing Systems, 2016, pp.~901--909.

\bibitem{santoro2016meta}
{\sc A.~Santoro, S.~Bartunov, M.~Botvinick, D.~Wierstra, and T.~Lillicrap},
  {\em Meta-learning with memory-augmented neural networks}, in International
  conference on machine learning, 2016, pp.~1842--1850.

\bibitem{DBLP:journals/corr/SchroffKP15}
{\sc F.~Schroff, D.~Kalenichenko, and J.~Philbin}, {\em Facenet: {A} unified
  embedding for face recognition and clustering}, CoRR, abs/1503.03832 (2015).

\bibitem{sennrich2015improving}
{\sc R.~Sennrich, B.~Haddow, and A.~Birch}, {\em Improving neural machine
  translation models with monolingual data}, arXiv preprint arXiv:1511.06709,
  (2015).

\bibitem{shazeer2017outrageously}
{\sc N.~Shazeer, A.~Mirhoseini, K.~Maziarz, A.~Davis, Q.~Le, G.~Hinton, and
  J.~Dean}, {\em Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer}, arXiv preprint arXiv:1701.06538,  (2017).

\bibitem{shrivastava2017learning}
{\sc A.~Shrivastava, T.~Pfister, O.~Tuzel, J.~Susskind, W.~Wang, and R.~Webb},
  {\em Learning from simulated and unsupervised images through adversarial
  training}, in Proceedings of the IEEE conference on computer vision and
  pattern recognition, 2017, pp.~2107--2116.

\bibitem{simonyan2014very}
{\sc K.~Simonyan and A.~Zisserman}, {\em Very deep convolutional networks for
  large-scale image recognition}, arXiv preprint arXiv:1409.1556,  (2014).

\bibitem{smith2017cyclical}
{\sc L.~N. Smith}, {\em Cyclical learning rates for training neural networks},
  in Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on,
  IEEE, 2017, pp.~464--472.

\bibitem{smith2018disciplined}
\leavevmode\vrule height 2pt depth -1.6pt width 23pt, {\em A disciplined
  approach to neural network hyper-parameters: Part 1--learning rate, batch
  size, momentum, and weight decay}, arXiv preprint arXiv:1803.09820,  (2018).

\bibitem{smith2017don}
{\sc S.~L. Smith, P.-J. Kindermans, and Q.~V. Le}, {\em Don't decay the
  learning rate, increase the batch size}, arXiv preprint arXiv:1711.00489,
  (2017).

\bibitem{snell2017prototypical}
{\sc J.~Snell, K.~Swersky, and R.~Zemel}, {\em Prototypical networks for
  few-shot learning}, in Advances in neural information processing systems,
  2017, pp.~4077--4087.

\bibitem{sogaard2016deep}
{\sc A.~S{\o}gaard and Y.~Goldberg}, {\em Deep multi-task learning with low
  level tasks supervised at lower layers}, in Proceedings of the 54th Annual
  Meeting of the Association for Computational Linguistics (Volume 2: Short
  Papers), 2016, pp.~231--235.

\bibitem{springenberg2014striving}
{\sc J.~T. Springenberg, A.~Dosovitskiy, T.~Brox, and M.~Riedmiller}, {\em
  Striving for simplicity: The all convolutional net}, arXiv preprint
  arXiv:1412.6806,  (2014).

\bibitem{standley2019tasks}
{\sc T.~Standley, A.~R. Zamir, D.~Chen, L.~Guibas, J.~Malik, and S.~Savarese},
  {\em Which tasks should be learned together in multi-task learning?}, arXiv
  preprint arXiv:1905.07553,  (2019).

\bibitem{DBLP:journals/corr/SutskeverVL14}
{\sc I.~Sutskever, O.~Vinyals, and Q.~V. Le}, {\em Sequence to sequence
  learning with neural networks}, CoRR, abs/1409.3215 (2014).

\bibitem{szegedy2016rethinking}
{\sc C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna}, {\em
  Rethinking the inception architecture for computer vision}, in Proceedings of
  the IEEE conference on computer vision and pattern recognition, 2016,
  pp.~2818--2826.

\bibitem{tran2017bayesian}
{\sc T.~Tran, T.~Pham, G.~Carneiro, L.~Palmer, and I.~Reid}, {\em A bayesian
  data augmentation approach for learning deep models}, in Advances in neural
  information processing systems, 2017, pp.~2797--2806.

\bibitem{DBLP:journals/corr/VaswaniSPUJGKP17}
{\sc A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin}, {\em Attention is all you need}, CoRR,
  abs/1706.03762 (2017).

\bibitem{Vinyals2015PointerN}
{\sc O.~Vinyals, M.~Fortunato, and N.~Jaitly}, {\em Pointer networks}, ArXiv,
  abs/1506.03134 (2015).

\bibitem{DBLP:journals/corr/abs-1711-07971}
{\sc X.~Wang, R.~B. Girshick, A.~Gupta, and K.~He}, {\em Non-local neural
  networks}, CoRR, abs/1711.07971 (2017).

\bibitem{weng2018attention}
{\sc L.~Weng}, {\em Attention? attention!}, lilianweng.github.io/lil-log,
  (2018).

\bibitem{wilson2003general}
{\sc D.~R. Wilson and T.~R. Martinez}, {\em The general inefficiency of batch
  training for gradient descent learning}, Neural Networks, 16 (2003),
  pp.~1429--1451.

\bibitem{wolpert1997no}
{\sc D.~H. Wolpert, W.~G. Macready, et~al.}, {\em No free lunch theorems for
  optimization}, IEEE transactions on evolutionary computation, 1 (1997),
  pp.~67--82.

\bibitem{xie2016disturblabel}
{\sc L.~Xie, J.~Wang, Z.~Wei, M.~Wang, and Q.~Tian}, {\em Disturblabel:
  Regularizing cnn on the loss layer}, in Proceedings of the IEEE Conference on
  Computer Vision and Pattern Recognition, 2016, pp.~4753--4762.

\bibitem{xie2019unsupervised}
{\sc Q.~Xie, Z.~Dai, E.~Hovy, M.-T. Luong, and Q.~V. Le}, {\em Unsupervised
  data augmentation}, arXiv preprint arXiv:1904.12848,  (2019).

\bibitem{xie2017aggregated}
{\sc S.~Xie, R.~Girshick, P.~Doll{\'a}r, Z.~Tu, and K.~He}, {\em Aggregated
  residual transformations for deep neural networks}, in Proceedings of the
  IEEE conference on computer vision and pattern recognition, 2017,
  pp.~1492--1500.

\bibitem{DBLP:journals/corr/XuBKCCSZB15}
{\sc K.~Xu, J.~Ba, R.~Kiros, K.~Cho, A.~C. Courville, R.~Salakhutdinov, R.~S.
  Zemel, and Y.~Bengio}, {\em Show, attend and tell: Neural image caption
  generation with visual attention}, CoRR, abs/1502.03044 (2015).

\bibitem{DBLP:journals/corr/YangH16}
{\sc Y.~Yang and T.~M. Hospedales}, {\em Deep multi-task representation
  learning: {A} tensor factorisation approach}, CoRR, abs/1605.06391 (2016).

\bibitem{yun2019cutmix}
{\sc S.~Yun, D.~Han, S.~J. Oh, S.~Chun, J.~Choe, and Y.~Yoo}, {\em Cutmix:
  Regularization strategy to train strong classifiers with localizable
  features}, arXiv preprint arXiv:1905.04899,  (2019).

\bibitem{zeiler2012adadelta}
{\sc M.~D. Zeiler}, {\em Adadelta: an adaptive learning rate method}, arXiv
  preprint arXiv:1212.5701,  (2012).

\bibitem{zhang2017mixup}
{\sc H.~Zhang, M.~Cisse, Y.~N. Dauphin, and D.~Lopez-Paz}, {\em mixup: Beyond
  empirical risk minimization}, arXiv preprint arXiv:1710.09412,  (2017).

\bibitem{zhang2019fixup}
{\sc H.~Zhang, Y.~N. Dauphin, and T.~Ma}, {\em Fixup initialization: Residual
  learning without normalization}, arXiv preprint arXiv:1901.09321,  (2019).

\bibitem{Zhang2019SelfAttentionGA}
{\sc H.~Zhang, I.~J. Goodfellow, D.~N. Metaxas, and A.~Odena}, {\em
  Self-attention generative adversarial networks}, ArXiv, abs/1805.08318
  (2019).

\bibitem{DBLP:journals/corr/ZhangY17aa}
{\sc Y.~Zhang and Q.~Yang}, {\em A survey on multi-task learning}, CoRR,
  abs/1707.08114 (2017).

\bibitem{zhao2019recommending}
{\sc Z.~Zhao, L.~Hong, L.~Wei, J.~Chen, A.~Nath, S.~Andrews, A.~Kumthekar,
  M.~Sathiamoorthy, X.~Yi, and E.~Chi}, {\em Recommending what video to watch
  next: a multitask ranking system}, in Proceedings of the 13th ACM Conference
  on Recommender Systems, 2019, pp.~43--51.

\bibitem{zoph2019learning}
{\sc B.~Zoph, E.~D. Cubuk, G.~Ghiasi, T.-Y. Lin, J.~Shlens, and Q.~V. Le}, {\em
  Learning data augmentation strategies for object detection}, arXiv preprint
  arXiv:1906.11172,  (2019).

\end{thebibliography}
