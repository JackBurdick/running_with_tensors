\begin{thebibliography}{10}

\bibitem{cloudHW_amazon_aws}
{\em Amazon aws. [{O}nline]}.
\newblock \url{https://aws.amazon.com/}.
\newblock Accessed: 2018-06-21.

\bibitem{convnet_js}
{\em Convnetjs. [{O}nline]}.
\newblock \url{http://cs.stanford.edu/people/karpathy/convnetjs/}.
\newblock Accessed: 2018-06-21.

\bibitem{deeplearnjs}
{\em deeplearn.js. [{O}nline]}.
\newblock \url{https://deeplearnjs.org/}.
\newblock Accessed: 2018-06-21.

\bibitem{cloudHW_floydhub}
{\em Floydhub. [{O}nline]}.
\newblock \url{https://www.floydhub.com/}.
\newblock Accessed: 2018-06-21.

\bibitem{cloudHW_google_cloud}
{\em Google cloud. [{O}nline]}.
\newblock \url{https://cloud.google.com/gpu/}.
\newblock Accessed: 2018-06-21.

\bibitem{cloudHW_micro_azure}
{\em Microsoft azure. [{O}nline]}.
\newblock
  \url{https://azure.microsoft.com/en-us/pricing/details/virtual-machines/series/}.
\newblock Accessed: 2018-06-21.

\bibitem{tf_playground}
{\em A neural network playground. [{O}nline]}.
\newblock \url{http://playground.tensorflow.org}.
\newblock Accessed: 2018-06-21.

\bibitem{cloudHW_nvidia_cloud}
{\em Nvidia gpu cloud. [{O}nline]}.
\newblock \url{https://www.nvidia.com/en-us/gpu-cloud/}.
\newblock Accessed: 2018-06-21.

\bibitem{abadi2016tensorflow_device_placement}
{\sc M.~Abadi, A.~Agarwal, P.~Barham, E.~Brevdo, Z.~Chen, C.~Citro, G.~S.
  Corrado, A.~Davis, J.~Dean, M.~Devin, et~al.}, {\em Tensorflow: Large-scale
  machine learning on heterogeneous distributed systems}, arXiv preprint
  arXiv:1603.04467,  (2016).

\bibitem{alber2018backprop}
{\sc M.~Alber, I.~Bello, B.~Zoph, P.-J. Kindermans, P.~Ramachandran, and
  Q.~Le}, {\em Backprop evolution}, arXiv preprint arXiv:1808.02822,  (2018).

\bibitem{baydin2017online}
{\sc A.~G. Baydin, R.~Cornish, D.~M. Rubio, M.~Schmidt, and F.~Wood}, {\em
  Online learning rate adaptation with hypergradient descent}, arXiv preprint
  arXiv:1703.04782,  (2017).

\bibitem{cubuk2018autoaugment}
{\sc E.~D. Cubuk, B.~Zoph, D.~Mane, V.~Vasudevan, and Q.~V. Le}, {\em
  Autoaugment: Learning augmentation policies from data}, arXiv preprint
  arXiv:1805.09501,  (2018).

\bibitem{devries2017improved}
{\sc T.~DeVries and G.~W. Taylor}, {\em Improved regularization of
  convolutional neural networks with cutout}, arXiv preprint arXiv:1708.04552,
  (2017).

\bibitem{dozat2016incorporating}
{\sc T.~Dozat}, {\em Incorporating nesterov momentum into adam},  (2016).

\bibitem{duchi2011adaptive}
{\sc J.~Duchi, E.~Hazan, and Y.~Singer}, {\em Adaptive subgradient methods for
  online learning and stochastic optimization}, Journal of Machine Learning
  Research, 12 (2011), pp.~2121--2159.

\bibitem{glorot2010understanding}
{\sc X.~Glorot and Y.~Bengio}, {\em Understanding the difficulty of training
  deep feedforward neural networks}, in Proceedings of the thirteenth
  international conference on artificial intelligence and statistics, 2010,
  pp.~249--256.

\bibitem{griewank2008evaluating}
{\sc A.~Griewank and A.~Walther}, {\em Evaluating derivatives: principles and
  techniques of algorithmic differentiation}, vol.~105, Siam, 2008.

\bibitem{guo2016entity}
{\sc C.~Guo and F.~Berkhahn}, {\em Entity embeddings of categorical variables},
  arXiv preprint arXiv:1604.06737,  (2016).

\bibitem{ha2018world}
{\sc D.~Ha and J.~Schmidhuber}, {\em World models}, arXiv preprint
  arXiv:1803.10122,  (2018).

\bibitem{hansen1990neural}
{\sc L.~K. Hansen and P.~Salamon}, {\em Neural network ensembles}, IEEE
  Transactions on Pattern Analysis \& Machine Intelligence,  (1990),
  pp.~993--1001.

\bibitem{he2015delving}
{\sc K.~He, X.~Zhang, S.~Ren, and J.~Sun}, {\em Delving deep into rectifiers:
  Surpassing human-level performance on imagenet classification}, in
  Proceedings of the IEEE international conference on computer vision, 2015,
  pp.~1026--1034.

\bibitem{hinton2015distilling}
{\sc G.~Hinton, O.~Vinyals, and J.~Dean}, {\em Distilling the knowledge in a
  neural network}, arXiv preprint arXiv:1503.02531,  (2015).

\bibitem{ho2019population}
{\sc D.~Ho, E.~Liang, I.~Stoica, P.~Abbeel, and X.~Chen}, {\em Population based
  augmentation: Efficient learning of augmentation policy schedules}, arXiv
  preprint arXiv:1905.05393,  (2019).

\bibitem{hoffer2017train}
{\sc E.~Hoffer, I.~Hubara, and D.~Soudry}, {\em Train longer, generalize
  better: closing the generalization gap in large batch training of neural
  networks}, in Advances in Neural Information Processing Systems, 2017,
  pp.~1729--1739.

\bibitem{hornik1991approximation}
{\sc K.~Hornik}, {\em Approximation capabilities of multilayer feedforward
  networks}, Neural networks, 4 (1991), pp.~251--257.

\bibitem{huang2017snapshot}
{\sc G.~Huang, Y.~Li, G.~Pleiss, Z.~Liu, J.~E. Hopcroft, and K.~Q. Weinberger},
  {\em Snapshot ensembles: Train 1, get m for free}, arXiv preprint
  arXiv:1704.00109,  (2017).

\bibitem{inoue2018data}
{\sc H.~Inoue}, {\em Data augmentation by pairing samples for images
  classification}, arXiv preprint arXiv:1801.02929,  (2018).

\bibitem{kingma2014adam}
{\sc D.~P. Kingma and J.~Ba}, {\em Adam: A method for stochastic optimization},
  arXiv preprint arXiv:1412.6980,  (2014).

\bibitem{kochenderfer2019algorithms}
{\sc M.~J. Kochenderfer and T.~A. Wheeler}, {\em Algorithms for Optimization},
  Mit Press, 2019.

\bibitem{lecun2012efficient}
{\sc Y.~A. LeCun, L.~Bottou, G.~B. Orr, and K.-R. M{\"u}ller}, {\em Efficient
  backprop}, in Neural networks: Tricks of the trade, Springer, 2012,
  pp.~9--48.

\bibitem{lemley2017smart}
{\sc J.~Lemley, S.~Bazrafkan, and P.~Corcoran}, {\em Smart augmentation
  learning an optimal data augmentation strategy}, Ieee Access, 5 (2017),
  pp.~5858--5869.

\bibitem{lim2019fast}
{\sc S.~Lim, I.~Kim, T.~Kim, C.~Kim, and S.~Kim}, {\em Fast autoaugment}, arXiv
  preprint arXiv:1905.00397,  (2019).

\bibitem{linnainmaa1970representation}
{\sc S.~Linnainmaa}, {\em The representation of the cumulative rounding error
  of an algorithm as a taylor expansion of the local rounding errors}, Master's
  Thesis (in Finnish), Univ. Helsinki,  (1970), pp.~6--7.

\bibitem{loshchilov2016sgdr}
{\sc I.~Loshchilov and F.~Hutter}, {\em Sgdr: stochastic gradient descent with
  restarts}, Learning, 10 (2016), p.~3.

\bibitem{masters2018revisiting}
{\sc D.~Masters and C.~Luschi}, {\em Revisiting small batch training for deep
  neural networks}, arXiv preprint arXiv:1804.07612,  (2018).

\bibitem{miyato2018virtual}
{\sc T.~Miyato, S.-i. Maeda, M.~Koyama, and S.~Ishii}, {\em Virtual adversarial
  training: a regularization method for supervised and semi-supervised
  learning}, IEEE transactions on pattern analysis and machine intelligence, 41
  (2018), pp.~1979--1993.

\bibitem{papernot2018deep}
{\sc N.~Papernot and P.~McDaniel}, {\em Deep k-nearest neighbors: Towards
  confident, interpretable and robust deep learning}, arXiv preprint
  arXiv:1803.04765,  (2018).

\bibitem{pereyra2017regularizing}
{\sc G.~Pereyra, G.~Tucker, J.~Chorowski, {\L}.~Kaiser, and G.~Hinton}, {\em
  Regularizing neural networks by penalizing confident output distributions},
  arXiv preprint arXiv:1701.06548,  (2017).

\bibitem{qian1999momentum}
{\sc N.~Qian}, {\em On the momentum term in gradient descent learning
  algorithms}, Neural networks, 12 (1999), pp.~145--151.

\bibitem{reddi2019convergence}
{\sc S.~J. Reddi, S.~Kale, and S.~Kumar}, {\em On the convergence of adam and
  beyond}, arXiv preprint arXiv:1904.09237,  (2019).

\bibitem{reed2014training}
{\sc S.~Reed, H.~Lee, D.~Anguelov, C.~Szegedy, D.~Erhan, and A.~Rabinovich},
  {\em Training deep neural networks on noisy labels with bootstrapping}, arXiv
  preprint arXiv:1412.6596,  (2014).

\bibitem{ruderman2018learned}
{\sc A.~Ruderman, N.~Rabinowitz, A.~S. Morcos, and D.~Zoran}, {\em Learned
  deformation stability in convolutional neural networks}, arXiv preprint
  arXiv:1804.04438,  (2018).

\bibitem{rumelhart1988learning}
{\sc D.~E. Rumelhart, G.~E. Hinton, R.~J. Williams, et~al.}, {\em Learning
  representations by back-propagating errors}, Cognitive modeling, 5 (1988),
  p.~1.

\bibitem{salimans2016weight}
{\sc T.~Salimans and D.~P. Kingma}, {\em Weight normalization: A simple
  reparameterization to accelerate training of deep neural networks}, in
  Advances in Neural Information Processing Systems, 2016, pp.~901--909.

\bibitem{sennrich2015improving}
{\sc R.~Sennrich, B.~Haddow, and A.~Birch}, {\em Improving neural machine
  translation models with monolingual data}, arXiv preprint arXiv:1511.06709,
  (2015).

\bibitem{shrivastava2017learning}
{\sc A.~Shrivastava, T.~Pfister, O.~Tuzel, J.~Susskind, W.~Wang, and R.~Webb},
  {\em Learning from simulated and unsupervised images through adversarial
  training}, in Proceedings of the IEEE conference on computer vision and
  pattern recognition, 2017, pp.~2107--2116.

\bibitem{simonyan2014very}
{\sc K.~Simonyan and A.~Zisserman}, {\em Very deep convolutional networks for
  large-scale image recognition}, arXiv preprint arXiv:1409.1556,  (2014).

\bibitem{smith2017cyclical}
{\sc L.~N. Smith}, {\em Cyclical learning rates for training neural networks},
  in Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on,
  IEEE, 2017, pp.~464--472.

\bibitem{smith2018disciplined}
\leavevmode\vrule height 2pt depth -1.6pt width 23pt, {\em A disciplined
  approach to neural network hyper-parameters: Part 1--learning rate, batch
  size, momentum, and weight decay}, arXiv preprint arXiv:1803.09820,  (2018).

\bibitem{smith2017don}
{\sc S.~L. Smith, P.-J. Kindermans, and Q.~V. Le}, {\em Don't decay the
  learning rate, increase the batch size}, arXiv preprint arXiv:1711.00489,
  (2017).

\bibitem{springenberg2014striving}
{\sc J.~T. Springenberg, A.~Dosovitskiy, T.~Brox, and M.~Riedmiller}, {\em
  Striving for simplicity: The all convolutional net}, arXiv preprint
  arXiv:1412.6806,  (2014).

\bibitem{szegedy2016rethinking}
{\sc C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna}, {\em
  Rethinking the inception architecture for computer vision}, in Proceedings of
  the IEEE conference on computer vision and pattern recognition, 2016,
  pp.~2818--2826.

\bibitem{tran2017bayesian}
{\sc T.~Tran, T.~Pham, G.~Carneiro, L.~Palmer, and I.~Reid}, {\em A bayesian
  data augmentation approach for learning deep models}, in Advances in neural
  information processing systems, 2017, pp.~2797--2806.

\bibitem{wilson2003general}
{\sc D.~R. Wilson and T.~R. Martinez}, {\em The general inefficiency of batch
  training for gradient descent learning}, Neural Networks, 16 (2003),
  pp.~1429--1451.

\bibitem{wolpert1997no}
{\sc D.~H. Wolpert, W.~G. Macready, et~al.}, {\em No free lunch theorems for
  optimization}, IEEE transactions on evolutionary computation, 1 (1997),
  pp.~67--82.

\bibitem{xie2016disturblabel}
{\sc L.~Xie, J.~Wang, Z.~Wei, M.~Wang, and Q.~Tian}, {\em Disturblabel:
  Regularizing cnn on the loss layer}, in Proceedings of the IEEE Conference on
  Computer Vision and Pattern Recognition, 2016, pp.~4753--4762.

\bibitem{xie2019unsupervised}
{\sc Q.~Xie, Z.~Dai, E.~Hovy, M.-T. Luong, and Q.~V. Le}, {\em Unsupervised
  data augmentation}, arXiv preprint arXiv:1904.12848,  (2019).

\bibitem{xie2017aggregated}
{\sc S.~Xie, R.~Girshick, P.~Doll{\'a}r, Z.~Tu, and K.~He}, {\em Aggregated
  residual transformations for deep neural networks}, in Proceedings of the
  IEEE conference on computer vision and pattern recognition, 2017,
  pp.~1492--1500.

\bibitem{yun2019cutmix}
{\sc S.~Yun, D.~Han, S.~J. Oh, S.~Chun, J.~Choe, and Y.~Yoo}, {\em Cutmix:
  Regularization strategy to train strong classifiers with localizable
  features}, arXiv preprint arXiv:1905.04899,  (2019).

\bibitem{zeiler2012adadelta}
{\sc M.~D. Zeiler}, {\em Adadelta: an adaptive learning rate method}, arXiv
  preprint arXiv:1212.5701,  (2012).

\bibitem{zhang2017mixup}
{\sc H.~Zhang, M.~Cisse, Y.~N. Dauphin, and D.~Lopez-Paz}, {\em mixup: Beyond
  empirical risk minimization}, arXiv preprint arXiv:1710.09412,  (2017).

\bibitem{zhang2019fixup}
{\sc H.~Zhang, Y.~N. Dauphin, and T.~Ma}, {\em Fixup initialization: Residual
  learning without normalization}, arXiv preprint arXiv:1901.09321,  (2019).

\bibitem{zhao2019recommending}
{\sc Z.~Zhao, L.~Hong, L.~Wei, J.~Chen, A.~Nath, S.~Andrews, A.~Kumthekar,
  M.~Sathiamoorthy, X.~Yi, and E.~Chi}, {\em Recommending what video to watch
  next: a multitask ranking system}, in Proceedings of the 13th ACM Conference
  on Recommender Systems, 2019, pp.~43--51.

\bibitem{zoph2019learning}
{\sc B.~Zoph, E.~D. Cubuk, G.~Ghiasi, T.-Y. Lin, J.~Shlens, and Q.~V. Le}, {\em
  Learning data augmentation strategies for object detection}, arXiv preprint
  arXiv:1906.11172,  (2019).

\end{thebibliography}
