{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Gradients\n",
    "\n",
    "### Resources:\n",
    "Papers\n",
    "> [Decoupled Neural Interfaces using Synthetic Gradients, Max Jaderberg et al., 2016](https://arxiv.org/abs/1608.05343)\n",
    "> [Understanding Synthetic Gradients and Decoupled Neural Interfaces, Wojciech Marian Czarnecki et al., 2017](https://arxiv.org/abs/1703.00522)\n",
    "\n",
    "Youtube\n",
    "> [Synthetic Gradients Tutorial by Aurélien Géron](https://youtu.be/1z_Gv98-mkQ)\n",
    "\n",
    "Github\n",
    "> [github; jupyter notebook by Nitarshan Rajkumar](https://github.com/nitarshan/decoupled-neural-interfaces/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: (3, 5, 4, 'final', 0)\n",
      "TensorFlow: 1.4.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this is a custom cell that contains the common imports I personally \n",
    "# use these may/may not be necessary for the following examples\n",
    "\n",
    "# DL framework\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# common packages\n",
    "import numpy as np\n",
    "import os # handling file i/o\n",
    "import sys\n",
    "import math\n",
    "import time # timing epochs\n",
    "import random\n",
    "\n",
    "# for ordered dict when building layer components\n",
    "import collections\n",
    "\n",
    "# plotting pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import colors # making colors consistent\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable # colorbar helper\n",
    "\n",
    "\n",
    "# from imageio import imread # read image from disk\n",
    "# + data augmentation\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "\n",
    "\n",
    "import pickle # manually saving best params\n",
    "from sklearn.utils import shuffle # shuffling data batches\n",
    "from tqdm import tqdm # display training progress bar\n",
    "\n",
    "# const\n",
    "SEED = 42\n",
    "\n",
    "# Helper to make the output consistent\n",
    "def reset_graph(seed=SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# helper to create dirs if they don't already exist\n",
    "def maybe_create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print(\"{} created\".format(dir_path))\n",
    "    else:\n",
    "        print(\"{} already exists\".format(dir_path))\n",
    "    \n",
    "def make_standard_dirs(saver=True, best_params=True, tf_logs=True):\n",
    "    # `saver/` will hold tf saver files\n",
    "    maybe_create_dir(\"saver\")\n",
    "    # `best_params/` will hold a serialized version of the best params\n",
    "    # I like to keep this as a backup in case I run into issues with\n",
    "    # the saver files\n",
    "    maybe_create_dir(\"best_params\")\n",
    "    # `tf_logs/` will hold the logs that will be visable in tensorboard\n",
    "    maybe_create_dir(\"tf_logs\")\n",
    "\n",
    "    \n",
    "# set tf log level to supress messages, unless an error\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Important Version information\n",
    "print(\"Python: {}\".format(sys.version_info[:]))\n",
    "print('TensorFlow: {}'.format(tf.__version__))\n",
    "\n",
    "# Check if using GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "    \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saver already exists\n",
      "best_params already exists\n",
      "tf_logs already exists\n"
     ]
    }
   ],
   "source": [
    "make_standard_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean all logs\n",
    "## WARNING! You likely don't want to do this (but if you do, this is a convenient call)\n",
    "# !rm -r -f ./tf_logs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these two functions (get_model_params and restore_model_params) are \n",
    "# ad[a|o]pted from; \n",
    "# https://github.com/ageron/handson-ml/blob/master/11_deep_learning.ipynb\n",
    "def get_model_params():\n",
    "    global_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    return {global_vars.op.name: value for global_vars, value in \n",
    "            zip(global_vars, tf.get_default_session().run(global_vars))}\n",
    "\n",
    "def restore_model_params(model_params, g, sess):\n",
    "    gvar_names = list(model_params.keys())\n",
    "    assign_ops = {gvar_name: g.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                  for gvar_name in gvar_names}\n",
    "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "# these two functions are used to manually save the best\n",
    "# model params to disk\n",
    "def save_obj(obj, name):\n",
    "    with open('best_params/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('best_params/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `!conda install -c conda-forge tqdm`\n",
    "# from tqdm import tqdm # Used to display training progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t10k-images-idx3-ubyte.gz\n",
      "t10k-labels-idx1-ubyte.gz\n",
      "train-images-idx3-ubyte.gz\n",
      "train-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "ROOT_DATA = \"../../ROOT_DATA/\"\n",
    "DATA_DIR = \"mnist_data\"\n",
    "\n",
    "MNIST_TRAINING_PATH = os.path.join(ROOT_DATA, DATA_DIR)\n",
    "# ensure we have the correct directory\n",
    "for _, _, files in os.walk(MNIST_TRAINING_PATH):\n",
    "    files = sorted(files)\n",
    "    for filename in files:\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../ROOT_DATA/mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../ROOT_DATA/mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../ROOT_DATA/mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../ROOT_DATA/mnist_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "MNIST = input_data.read_data_sets(MNIST_TRAINING_PATH, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image dimensions (GLOBAL) - [MG_WIDTH x IMG_HEIGHT, CHANNELS]\n",
    "#SQUARE_DIM = 299\n",
    "SQUARE_DIM = 28\n",
    "if SQUARE_DIM:\n",
    "    IMG_WIDTH = SQUARE_DIM\n",
    "    IMG_HEIGHT = SQUARE_DIM\n",
    "#CHANNELS = 3\n",
    "CHANNELS = 1\n",
    "    \n",
    "# ROOT_DIR = \"../../dataset/record_holder\"\n",
    "# # ensure we have the correct directory\n",
    "# for _, _, files in os.walk(ROOT_DIR):\n",
    "#     files = sorted(files)\n",
    "#     for filename in files:\n",
    "#         print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read tf records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL_SET_TYPE = None\n",
    "\n",
    "# def _parse_function(example_proto):\n",
    "#     global GLOBAL_SET_TYPE\n",
    "#     labelName = str(GLOBAL_SET_TYPE) + '/label'\n",
    "#     featureName = str(GLOBAL_SET_TYPE) + '/image'\n",
    "#     feature = {featureName: tf.FixedLenFeature([], tf.string),\n",
    "#                labelName: tf.FixedLenFeature([], tf.int64)}\n",
    "    \n",
    "#     # decode\n",
    "#     parsed_features = tf.parse_single_example(example_proto, features=feature)\n",
    "    \n",
    "#     # convert image data from string to number\n",
    "#     image = tf.decode_raw(parsed_features[featureName], tf.float32)\n",
    "#     image = tf.reshape(image, [IMG_WIDTH, IMG_HEIGHT, CHANNELS])\n",
    "#     label = tf.cast(parsed_features[labelName], tf.int64)\n",
    "    \n",
    "#     # [do any preprocessing here]\n",
    "    \n",
    "#     return image, label\n",
    "\n",
    "# def return_batched_iter(setType, data_params, sess):\n",
    "#     global GLOBAL_SET_TYPE\n",
    "#     GLOBAL_SET_TYPE = setType\n",
    "    \n",
    "#     filenames_ph = tf.placeholder(tf.string, shape=[None])\n",
    "\n",
    "#     dataset = tf.data.TFRecordDataset(filenames_ph)\n",
    "#     dataset = dataset.map(_parse_function)  # Parse the record into tensors.\n",
    "#     dataset = dataset.shuffle(buffer_size=data_params['buffer_size'])\n",
    "#     dataset = dataset.batch(data_params['batch_size'])\n",
    "#     dataset = dataset.repeat(data_params['n_epochs'])\n",
    "    \n",
    "#     iterator = dataset.make_initializable_iterator()\n",
    "    \n",
    "#     tfrecords_file_name = str(GLOBAL_SET_TYPE) + '.tfrecords'\n",
    "#     tfrecord_file_path = os.path.join(FINAL_DIR, tfrecords_file_name)\n",
    "    \n",
    "#     # initialize\n",
    "#     sess.run(iterator.initializer, feed_dict={filenames_ph: [tfrecord_file_path]})\n",
    "    \n",
    "#     return iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hyper_params():\n",
    "    data_params = {}\n",
    "    data_params['n_epochs'] = 20\n",
    "    data_params['batch_size'] = 512\n",
    "    data_params['buffer_size'] = 128 # for shuffling\n",
    "\n",
    "    data_params['init_lr'] = 1e-5\n",
    "    #data_params['lr_div'] = 10\n",
    "    #lr_low = int(data_params['n_epochs'] * 0.6)\n",
    "    #lr_high = int(data_params['n_epochs'] * 0.8)\n",
    "    #data_params['lr_div_steps'] = set([lr_low, lr_high])\n",
    "\n",
    "    data_params['update_prob'] = 0.2 # Probability of updating a decoupled layer\n",
    "    \n",
    "    return data_params\n",
    "\n",
    "validation_checkpoint = 1 # How often (epochs) to validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = create_hyper_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers for creating layers\n",
    "def dense_layer(inputs, units, name, output=False):\n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.layers.dense(inputs, units, name=\"fc\")\n",
    "        if not output:\n",
    "            x = tf.layers.batch_normalization(x, name=\"bn\")\n",
    "            x = tf.nn.relu(x, name=\"activation_relu\")\n",
    "    return x\n",
    "\n",
    "def sg_module(inputs, units, name, label):\n",
    "    with tf.variable_scope(name):\n",
    "        inputs_c = tf.concat([inputs, label], 1)\n",
    "        x = tf.layers.dense(inputs_c, units, name=\"fc\", kernel_initializer=tf.zeros_initializer())\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_conv_layer(inputs, units, kern_shape, name, stride_len, output=False):\n",
    "    with tf.variable_scope(name):\n",
    "        x = tf.layers.conv2d(inputs, filters=units, kernel_size=kern_shape, \n",
    "                             padding='SAME', strides=stride_len,\n",
    "                             name=\"conv\")\n",
    "        if not output:\n",
    "            x = tf.nn.relu(x, name=\"activation_relu\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_conv_sg_module(inputs, units, kern_shape, name, label, stride_len):\n",
    "    with tf.variable_scope(name):\n",
    "        # TODO: really unsure about this concat here.\n",
    "        # using in attempt to adapt: https://github.com/vyraun/DNI-tensorflow/blob/master/utils.py\n",
    "        out_shape = inputs.get_shape().as_list()\n",
    "        label_shape = label.get_shape().as_list()\n",
    "        #label_tile = tf.reshape(tf.tile(label, [1,out_shape[1]*out_shape[2]]), \n",
    "         #                       [out_shape[0], out_shape[1], out_shape[2], label_shape[1]])\n",
    "        #inputs_c = tf.concat(3, [inputs, label_tile])\n",
    "        #inputs_c = tf.concat(3, [inputs, label])\n",
    "        x = tf.layers.conv2d(inputs, filters=units, kernel_size=kern_shape, \n",
    "                             padding='SAME', strides=stride_len,\n",
    "                             name=\"conv\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "syntgrad_sess = tf.Session()\n",
    "backprop_sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network architecture\n",
    "n_outputs = 10\n",
    "with tf.variable_scope(\"architecture\"):\n",
    "    # inputs\n",
    "    with tf.variable_scope(\"inputs\"):\n",
    "        #X = tf.placeholder(tf.float32, shape=[None, IMG_HEIGHT, IMG_WIDTH, CHANNELS], name=\"X\")\n",
    "        # labels\n",
    "        X = tf.placeholder(tf.float32, shape=(None, 784), name=\"data\") # Input\n",
    "        X_reshaped = tf.reshape(X, shape=[-1, IMG_HEIGHT, IMG_WIDTH, CHANNELS])\n",
    "        y = tf.placeholder(tf.float32, shape=(None, n_outputs), name=\"labels\") # Target\n",
    "        \n",
    "    # Inference Layers\n",
    "    # conv\n",
    "    h_1 = create_conv_layer(X_reshaped, units=12, kern_shape=3, name=\"layer_01\", stride_len=1)\n",
    "    h_2 = create_conv_layer(h_1, units=24, kern_shape=3, name=\"layer_02\", stride_len=1)\n",
    "    h_3 = create_conv_layer(h_2, units=36, kern_shape=3, name=\"layer_03\", stride_len=2)\n",
    "    # pooling\n",
    "    h_4 = tf.layers.max_pooling2d(h_3, pool_size=[2,2], strides=2, name=\"max_pool_01\")\n",
    "    last_shape = int(np.prod(h_4.get_shape()[1:]))\n",
    "    pool_flat = tf.reshape(h_4, shape=[-1, last_shape])\n",
    "    \n",
    "    # fc\n",
    "    h_5 = dense_layer(pool_flat, 64, \"layer_05\")\n",
    "    logits = dense_layer(h_5, n_outputs, name=\"layer_06\", output=True)\n",
    "    \n",
    "    # Synthetic Gradient Layers\n",
    "    sg_1 = create_conv_sg_module(h_1, units=12, kern_shape=3, name=\"sg_02\", label=y, stride_len=1)\n",
    "    sg_2 = create_conv_sg_module(h_2, units=24, kern_shape=3, name=\"sg_03\", label=y, stride_len=1)\n",
    "    sg_4 = create_conv_sg_module(h_3, units=36, kern_shape=3, name=\"sg_05\", label=y, stride_len=2)\n",
    "    sg_5 = sg_module(pool_flat, 64, \"sg_06\", y)\n",
    "    \n",
    "# collections of trainable variables in each block\n",
    "layer_vars = [tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer_01/\"),\n",
    "              tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer_02/\"),\n",
    "              tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer_03/\"),\n",
    "              None,\n",
    "              tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer_05/\"),\n",
    "              tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/layer_06/\")]\n",
    "sg_vars = [None,\n",
    "           tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/sg_02/\"),\n",
    "           tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/sg_03/\"),\n",
    "           None,\n",
    "           tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/sg_05/\"),\n",
    "           tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"architecture/sg_06/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize layer and the sythetic gradient module\n",
    "def train_layer_n(n, h_m, h_n, sg_m, class_loss, d_n=None):\n",
    "    with tf.variable_scope(\"layer_0\"+str(n)):\n",
    "        layer_grads = tf.gradients(h_n, [h_m]+layer_vars[n-1], d_n)\n",
    "        layer_gv = list(zip(layer_grads[1:], layer_vars[n-1]))\n",
    "        layer_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).apply_gradients(layer_gv)\n",
    "    with tf.variable_scope(\"sg_0\"+str(n)):\n",
    "        d_m = layer_grads[0]\n",
    "        sg_loss = tf.divide(tf.losses.mean_squared_error(sg_m, d_m), class_loss)\n",
    "        sg_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(sg_loss, var_list=sg_vars[n-1])\n",
    "    return layer_opt, sg_opt\n",
    "\n",
    "# Ops: training\n",
    "with tf.variable_scope(\"train\"):\n",
    "    with tf.variable_scope(\"learning_rate\"):\n",
    "        learning_rate = tf.Variable(data_params['init_lr'], dtype=tf.float32, name=\"lr\")\n",
    "        #reduce_lr = tf.assign(learning_Rate, learning_rate/lr_div, name=\"lr_decrease\")\n",
    "        \n",
    "    #pred_loss = tf.losses.softmax_cross_entropy(onehot_labels=y, logits=logits, scope=\"prediction_loss\")\n",
    "    with tf.variable_scope(\"prediction_loss\"):\n",
    "        #pred_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y)\n",
    "        pred_loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "        batch_loss = tf.reduce_mean(pred_loss)\n",
    "    \n",
    "    # Optimizers when using synthetic gradients\n",
    "    with tf.variable_scope(\"synthetic\"):\n",
    "        layer6_opt, sg6_opt = train_layer_n(6, h_5, pred_loss, sg_5, pred_loss)\n",
    "        layer5_opt, sg5_opt = train_layer_n(5, h_4, h_5, sg_4, pred_loss, sg_5)\n",
    "        # none\n",
    "        layer3_opt, sg3_opt = train_layer_n(3, h_2, h_3, sg_2, pred_loss, h_3)\n",
    "        layer2_opt, sg2_opt = train_layer_n(2, h_1, h_2, sg_1, pred_loss, sg_2)\n",
    "        with tf.variable_scope(\"layer_01\"):\n",
    "            layer1_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(h_1, var_list=layer_vars[0], grad_loss=sg_1)\n",
    "        \n",
    "    with tf.variable_scope(\"backprop\"):\n",
    "        backprop_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(pred_loss)\n",
    "        \n",
    "init_global = tf.global_variables_initializer()\n",
    "init_local = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ops: training metrics\n",
    "with tf.name_scope(\"metrics\") as scope:\n",
    "    with tf.name_scope(\"train_metrics\") as scope:\n",
    "        preds = tf.nn.softmax(logits, name=\"prediction\")\n",
    "        train_y_true_cls = tf.argmax(y,1)\n",
    "        train_y_pred_cls = tf.argmax(preds,1)\n",
    "\n",
    "        train_correct_prediction = tf.equal(train_y_pred_cls, train_y_true_cls, name=\"correct_predictions\")\n",
    "        train_batch_acc = tf.reduce_mean(tf.cast(train_correct_prediction, tf.float32))\n",
    "\n",
    "        train_auc, train_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "        train_acc, train_acc_update = tf.metrics.accuracy(labels=train_y_true_cls, predictions=train_y_pred_cls)\n",
    "\n",
    "        train_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "        train_acc_reset_op = tf.variables_initializer(train_acc_vars, name=\"train_acc_reset_op\")\n",
    "\n",
    "        #for node in (y_, preds, train_y_true_cls, train_y_pred_cls, correct_prediction, train_batch_acc):\n",
    "                #g.add_to_collection(\"label_nodes\", node)\n",
    "\n",
    "    # Ops: validation metrics\n",
    "    with tf.name_scope(\"validation_metrics\") as scope:\n",
    "        preds = tf.nn.softmax(logits, name=\"prediction\")\n",
    "        val_y_true_cls = tf.argmax(y,1)\n",
    "        val_y_pred_cls = tf.argmax(preds,1)        \n",
    "\n",
    "        val_correct_prediction = tf.equal(val_y_pred_cls, val_y_true_cls)\n",
    "        val_batch_acc = tf.reduce_mean(tf.cast(val_correct_prediction, tf.float32))\n",
    "\n",
    "        val_auc, val_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "        val_acc, val_acc_update = tf.metrics.accuracy(labels=val_y_true_cls, predictions=val_y_pred_cls)\n",
    "\n",
    "        val_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "        val_acc_reset_op = tf.variables_initializer(val_acc_vars, name=\"val_acc_reset_op\")\n",
    "\n",
    "    # Ops: test metrics\n",
    "    with tf.name_scope(\"test_metrics\") as scope:    \n",
    "        preds = tf.nn.softmax(logits, name=\"prediction\")\n",
    "        test_y_true_cls = tf.argmax(y,1)\n",
    "        test_y_pred_cls = tf.argmax(preds,1)\n",
    "\n",
    "        test_auc, test_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "\n",
    "        test_acc, test_acc_update = tf.metrics.accuracy(labels=test_y_true_cls, predictions=test_y_pred_cls)\n",
    "        test_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "        test_acc_reset_op = tf.variables_initializer(test_acc_vars, name=\"test_acc_reset_op\")\n",
    "\n",
    "    # =============================================== loss \n",
    "    with tf.name_scope(\"train_loss_eval\") as scope:\n",
    "        train_mean_loss, train_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "        train_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "        train_loss_reset_op = tf.variables_initializer(train_loss_vars, name=\"train_loss_reset_op\")\n",
    "\n",
    "    with tf.name_scope(\"val_loss_eval\") as scope:\n",
    "        val_mean_loss, val_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "        val_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "        val_loss_reset_op = tf.variables_initializer(val_loss_vars, name=\"val_loss_reset_op\")\n",
    "\n",
    "    with tf.name_scope(\"test_loss_eval\") as scope:\n",
    "        test_mean_loss, test_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "        test_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "        test_loss_reset_op = tf.variables_initializer(test_loss_vars, name=\"test_loss_rest_op\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====combine operations\n",
    "# ===== epoch, train\n",
    "with tf.name_scope(\"tensorboard_writer\") as scope:\n",
    "    epoch_train_loss_scalar = tf.summary.scalar('train_epoch_loss', train_mean_loss)\n",
    "    epoch_train_acc_scalar = tf.summary.scalar('train_epoch_acc', train_acc)\n",
    "    epoch_train_auc_scalar = tf.summary.scalar('train_epoch_auc', train_auc)\n",
    "    epoch_train_write_op = tf.summary.merge([epoch_train_loss_scalar, epoch_train_acc_scalar, epoch_train_auc_scalar], name=\"epoch_train_write_op\")\n",
    "\n",
    "    # ===== epoch, validation\n",
    "    epoch_validation_loss_scalar = tf.summary.scalar('validation_epoch_loss', val_mean_loss)\n",
    "    epoch_validation_acc_scalar = tf.summary.scalar('validation_epoch_acc', val_acc)\n",
    "    epoch_validation_auc_scalar = tf.summary.scalar('validation_epoch_auc', val_auc)\n",
    "    epoch_validation_write_op = tf.summary.merge([epoch_validation_loss_scalar, epoch_validation_acc_scalar, epoch_validation_auc_scalar], name=\"epoch_validation_write_op\")\n",
    "\n",
    "    # ====== batch, train\n",
    "    train_batch_loss_scalar = tf.summary.scalar('train_batch_loss', batch_loss)\n",
    "    train_batch_acc_scalar = tf.summary.scalar('train_batch_acc', train_batch_acc)\n",
    "    train_batch_write_op = tf.summary.merge([train_batch_loss_scalar, train_batch_acc_scalar], name=\"train_batch_write_op\")\n",
    "\n",
    "    # ====== checkpoint, validation\n",
    "    checkpoint_validation_loss_scalar = tf.summary.scalar('validation_batch_loss', batch_loss)\n",
    "    checkpoint_validation_acc_scalar = tf.summary.scalar('validation_batch_acc', val_batch_acc)\n",
    "    checkpoint_validation_write_op = tf.summary.merge([checkpoint_validation_loss_scalar, checkpoint_validation_acc_scalar], name=\"checkpoint_valdiation_write_op\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop (locked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:25<00:00,  4.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# backprop\n",
    "with backprop_sess.as_default():\n",
    "    backprop_train_path = os.path.join(\"tf_logs\",\"backprop\",\"train\")\n",
    "    backprop_train_writer = tf.summary.FileWriter(backprop_train_path)\n",
    "    backprop_validation_path = os.path.join(\"tf_logs\",\"backprop\",\"validation\")\n",
    "    backprop_validation_writer = tf.summary.FileWriter(backprop_validation_path)\n",
    "    \n",
    "    backprop_sess.run([init_global,init_local])\n",
    "    \n",
    "    for e in tqdm(range(1,data_params['n_epochs']+1)):\n",
    "        backprop_sess.run([val_acc_reset_op,val_loss_reset_op,train_acc_reset_op,train_loss_reset_op])\n",
    "        \n",
    "        n_batches = int(MNIST.train.num_examples/data_params['batch_size'])\n",
    "        for i in range(1,n_batches+1):\n",
    "            data, target = MNIST.train.next_batch(data_params['batch_size'])\n",
    "            backprop_sess.run([backprop_opt, train_auc_update, train_acc_update, train_mean_loss_update], feed_dict={X:data, y:target})\n",
    "        \n",
    "        # write average for epoch\n",
    "        summary = backprop_sess.run(epoch_train_write_op)    \n",
    "        backprop_train_writer.add_summary(summary, e)\n",
    "        backprop_train_writer.flush()\n",
    "        \n",
    "        # run validation\n",
    "        n_batches = int(MNIST.validation.num_examples/data_params['batch_size'])\n",
    "        for i in range(1,n_batches+1):\n",
    "            Xb, yb = MNIST.validation.next_batch(data_params['batch_size'])\n",
    "            backprop_sess.run([val_auc_update, val_acc_update, val_mean_loss_update], feed_dict={X:Xb, y:yb})\n",
    "        \n",
    "        #summary = backprop_sess.run([summary_op], feed_dict={X:Xb, y:yb})[0]\n",
    "        summary = backprop_sess.run(epoch_validation_write_op) \n",
    "        backprop_validation_writer.add_summary(summary, e)\n",
    "        backprop_validation_writer.flush()\n",
    "        \n",
    "    # close writers\n",
    "    backprop_train_writer.close()\n",
    "    backprop_validation_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:36<00:00,  1.82s/it]\n"
     ]
    }
   ],
   "source": [
    "with syntgrad_sess.as_default():\n",
    "    sg_train_path = os.path.join(\"tf_logs\",\"sg\",\"train\")\n",
    "    sg_train_writer = tf.summary.FileWriter(sg_train_path, syntgrad_sess.graph)\n",
    "    sg_validation_path = os.path.join(\"tf_logs\",\"sg\",\"validation\")\n",
    "    sg_validation_writer = tf.summary.FileWriter(sg_validation_path)\n",
    "    \n",
    "    syntgrad_sess.run([init_global,init_local])\n",
    "    \n",
    "    for e in tqdm(range(1,data_params['n_epochs']+1)):\n",
    "        syntgrad_sess.run([val_acc_reset_op,val_loss_reset_op,train_acc_reset_op,train_loss_reset_op])\n",
    "        \n",
    "        n_batches = int(MNIST.train.num_examples/data_params['batch_size'])\n",
    "        for i in range(1,n_batches+1):\n",
    "            data, target = MNIST.train.next_batch(data_params['batch_size'])\n",
    "            # The layers here could be independently updated (data parallism) - device placement\n",
    "            # > stochastic updates are possible\n",
    "            if random.random() <= data_params['update_prob']:\n",
    "                syntgrad_sess.run([layer1_opt], feed_dict={X:data, y:target})\n",
    "            if random.random() <= data_params['update_prob']:\n",
    "                syntgrad_sess.run([layer2_opt, sg2_opt], feed_dict={X:data, y:target})\n",
    "            if random.random() <= data_params['update_prob']:\n",
    "                syntgrad_sess.run([layer3_opt, sg3_opt], feed_dict={X:data, y:target})\n",
    "            if random.random() <= data_params['update_prob']:\n",
    "                syntgrad_sess.run([layer5_opt, sg5_opt], feed_dict={X:data, y:target})\n",
    "            if random.random() <= data_params['update_prob']:\n",
    "                syntgrad_sess.run([layer6_opt, sg6_opt, train_auc_update, train_acc_update, train_mean_loss_update], \n",
    "                                  feed_dict={X:data, y:target})\n",
    "                \n",
    "        # write average for epoch\n",
    "        summary = syntgrad_sess.run(epoch_train_write_op)    \n",
    "        sg_train_writer.add_summary(summary, e)\n",
    "        sg_train_writer.flush()\n",
    "            \n",
    "        # validation\n",
    "        n_batches = int(MNIST.validation.num_examples/data_params['batch_size'])\n",
    "        for i in range(1,n_batches+1):\n",
    "            Xb, yb = MNIST.validation.next_batch(data_params['batch_size'])\n",
    "            syntgrad_sess.run([val_auc_update, val_acc_update, val_mean_loss_update], feed_dict={X:Xb, y:yb})\n",
    "\n",
    "        summary = syntgrad_sess.run(epoch_validation_write_op) \n",
    "        sg_validation_writer.add_summary(summary, e)\n",
    "        sg_validation_writer.flush()\n",
    "        \n",
    "    sg_train_writer.close()\n",
    "    sg_validation_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 42.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 99.130% acc: 90.234% loss: 0.34111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test using backprop\n",
    "# batching isn't working \"perfectly\".. we seem to iterate over a slightly different test\n",
    "# set each time (even if the batch size is a multiple of the num_examples)..\n",
    "with backprop_sess.as_default():\n",
    "    backprop_sess.run([test_acc_reset_op, test_loss_reset_op])\n",
    "    \n",
    "    n_batches = int(MNIST.test.num_examples/data_params['batch_size'])\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        Xb, yb = MNIST.test.next_batch(data_params['batch_size'])\n",
    "        batch_accuracy, batch_loss, batch_auc = backprop_sess.run([test_acc_update, test_mean_loss_update, test_auc_update], \n",
    "                                                                  feed_dict={X:Xb,y:yb})\n",
    "    # print\n",
    "    final_test_acc, final_test_loss, final_test_auc = backprop_sess.run([test_acc, test_mean_loss, test_auc])\n",
    "    print(\"test auc: {:.3f}% acc: {:.3f}% loss: {:.5f}\".format(final_test_auc*100, \n",
    "                                                              final_test_acc*100,\n",
    "                                                              final_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 44.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test auc: 74.709% acc: 35.310% loss: 2.26109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now use synthetic grad\n",
    "with syntgrad_sess.as_default():\n",
    "    syntgrad_sess.run([test_acc_reset_op, test_loss_reset_op])\n",
    "    \n",
    "    n_batches = int(MNIST.test.num_examples/data_params['batch_size'])\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        Xb, yb = MNIST.test.next_batch(data_params['batch_size'])\n",
    "        batch_accuracy, batch_loss, batch_auc = syntgrad_sess.run([test_acc_update, test_mean_loss_update, test_auc_update], \n",
    "                                                                  feed_dict={X:Xb,y:yb})\n",
    "    # print\n",
    "    final_test_acc, final_test_loss, final_test_auc = syntgrad_sess.run([test_acc, test_mean_loss, test_auc])\n",
    "    print(\"test auc: {:.3f}% acc: {:.3f}% loss: {:.5f}\".format(final_test_auc*100, \n",
    "                                                              final_test_acc*100,\n",
    "                                                              final_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "backprop_sess.close()\n",
    "syntgrad_sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_edge",
   "language": "python",
   "name": "tf_edge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
