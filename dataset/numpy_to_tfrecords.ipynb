{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: (3, 5, 4, 'final', 0)\n",
      "TensorFlow: 1.4.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this is a custom cell that contains the common imports I personally \n",
    "# use these may/may not be necessary for the following examples\n",
    "\n",
    "# DL framework\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# common packages\n",
    "import numpy as np\n",
    "import os # handling file i/o\n",
    "import sys\n",
    "import math\n",
    "import time # timing epochs\n",
    "\n",
    "# for ordered dict when building layer components\n",
    "import collections\n",
    "\n",
    "# plotting pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import colors # making colors consistent\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable # colorbar helper\n",
    "\n",
    "# read image\n",
    "### from imageio import imread\n",
    "# + data augmentation\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "\n",
    "# used for manually saving best params\n",
    "import pickle\n",
    "\n",
    "# for shuffling data batches\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# const\n",
    "SEED = 42\n",
    "\n",
    "# Helper to make the output consistent\n",
    "def reset_graph(seed=SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# helper to create dirs if they don't already exist\n",
    "def maybe_create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print(\"{} created\".format(dir_path))\n",
    "    else:\n",
    "        print(\"{} already exists\".format(dir_path))\n",
    "    \n",
    "# set tf log level to supress messages, unless an error\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Important Version information\n",
    "print(\"Python: {}\".format(sys.version_info[:]))\n",
    "print('TensorFlow: {}'.format(tf.__version__))\n",
    "\n",
    "# Check if using GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "    \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./record_holder/lesion/balanced/ already exists\n"
     ]
    }
   ],
   "source": [
    "# `/record_holder` will (hopefully) contain our tf_records file\n",
    "# by the end of this notebook\n",
    "FINAL_DIR = \"./record_holder/lesion/balanced/\"\n",
    "maybe_create_dir(FINAL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.npy\n",
      "X_train.npy\n",
      "X_val.npy\n",
      "y_test.npy\n",
      "y_train.npy\n",
      "y_val.npy\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = \"./numpy/sigmoid/lesion/224/\"\n",
    "\n",
    "for _, _, files in os.walk(ROOT_DIR):\n",
    "    files = sorted(files)\n",
    "    for filename in files:\n",
    "        print(filename)\n",
    "        \n",
    "X_test = np.load(os.path.join(ROOT_DIR, files[0]))\n",
    "X_train = np.load(os.path.join(ROOT_DIR, files[1]))\n",
    "X_val = np.load(os.path.join(ROOT_DIR, files[2]))\n",
    "y_test = np.load(os.path.join(ROOT_DIR, files[3]))\n",
    "y_train = np.load(os.path.join(ROOT_DIR, files[4]))\n",
    "y_val = np.load(os.path.join(ROOT_DIR, files[5]))\n",
    "\n",
    "# reset_graph()\n",
    "# X_test_ph =  tf.placeholder(X_test.dtype, X_test.shape)\n",
    "# X_train_ph = tf.placeholder(X_train.dtype, X_train.shape)\n",
    "# X_val_ph = tf.placeholder(X_val.dtype, X_val.shape)\n",
    "# y_test_ph = tf.placeholder(y_test.dtype, y_test.shape)\n",
    "# y_train_ph = tf.placeholder(y_train.dtype, y_train.shape)\n",
    "# y_val_ph = tf.placeholder(y_val.dtype, y_val.shape)\n",
    "\n",
    "# def create_dataset_obj(X, y):\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "#     return dataset\n",
    "\n",
    "# tr_dataset = create_dataset_obj(X_train_ph, \n",
    "#                                 X_train_ph)\n",
    "# val_dataset = create_dataset_obj(X_val_ph, \n",
    "#                                  y_val_ph)\n",
    "# test_dataset = create_dataset_obj(X_test_ph, \n",
    "#                                   y_test_ph)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_tfrecords(features, lables, setType):\n",
    "    tfrecords_file_name = str(setType) + '.tfrecords'\n",
    "    writer = tf.python_io.TFRecordWriter(os.path.join(FINAL_DIR, tfrecords_file_name))\n",
    "    \n",
    "    labelName = str(setType) + '/label'\n",
    "    featureName = str(setType) + '/image'\n",
    "    \n",
    "    # TODO: assert same length\n",
    "    for i in range(len(features)):\n",
    "        label = lables[i]\n",
    "        img = features[i]\n",
    "    \n",
    "        # create features\n",
    "        feature = {labelName: _int64_feature(label),\n",
    "                   featureName: _bytes_feature(tf.compat.as_bytes(img.tostring()))}\n",
    "        \n",
    "        # create example protocol buffer\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        \n",
    "        writer.write(example.SerializeToString())\n",
    "        \n",
    "        if i % 25 == 0:\n",
    "            print(\"{} {} written\".format(i, setType))\n",
    "        \n",
    "    writer.close()\n",
    "    sys.stdout.flush()\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 test written\n",
      "25 test written\n",
      "50 test written\n",
      "75 test written\n",
      "100 test written\n",
      "125 test written\n",
      "150 test written\n",
      "175 test written\n",
      "200 test written\n",
      "225 test written\n",
      "250 test written\n",
      "275 test written\n",
      "300 test written\n",
      "325 test written\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "numpy_to_tfrecords(X_test, y_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 val written\n",
      "25 val written\n",
      "50 val written\n",
      "75 val written\n",
      "100 val written\n",
      "125 val written\n",
      "150 val written\n",
      "175 val written\n",
      "200 val written\n",
      "225 val written\n",
      "250 val written\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "numpy_to_tfrecords(X_val, y_val, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train written\n",
      "25 train written\n",
      "50 train written\n",
      "75 train written\n",
      "100 train written\n",
      "125 train written\n",
      "150 train written\n",
      "175 train written\n",
      "200 train written\n",
      "225 train written\n",
      "250 train written\n",
      "275 train written\n",
      "300 train written\n",
      "325 train written\n",
      "350 train written\n",
      "375 train written\n",
      "400 train written\n",
      "425 train written\n",
      "450 train written\n",
      "475 train written\n",
      "500 train written\n",
      "525 train written\n",
      "550 train written\n",
      "575 train written\n",
      "600 train written\n",
      "625 train written\n",
      "650 train written\n",
      "675 train written\n",
      "700 train written\n",
      "725 train written\n",
      "750 train written\n",
      "775 train written\n",
      "800 train written\n",
      "825 train written\n",
      "850 train written\n",
      "875 train written\n",
      "900 train written\n",
      "925 train written\n",
      "950 train written\n",
      "975 train written\n",
      "1000 train written\n",
      "1025 train written\n",
      "1050 train written\n",
      "1075 train written\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "numpy_to_tfrecords(X_train, y_train, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE!\n",
    "This is likely the *wrong* way to approach this problem.. I'm currently trying to find a better way to appraoch reading the tf records/having a reusable `_parse_function` that does not use a GLOBAL var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_SET_TYPE = None\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    global GLOBAL_SET_TYPE\n",
    "    labelName = str(GLOBAL_SET_TYPE) + '/label'\n",
    "    featureName = str(GLOBAL_SET_TYPE) + '/image'\n",
    "    feature = {featureName: tf.FixedLenFeature([], tf.string),\n",
    "               labelName: tf.FixedLenFeature([], tf.int64)}\n",
    "    \n",
    "    # decode\n",
    "    parsed_features = tf.parse_single_example(example_proto, features=feature)\n",
    "    \n",
    "    # convert image data from string to number\n",
    "    image = tf.decode_raw(parsed_features[featureName], tf.float32)\n",
    "    image = tf.reshape(image, [224, 224, 3])\n",
    "    label = tf.cast(parsed_features[labelName], tf.int64)\n",
    "    \n",
    "    # [do any preprocessing here]\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_batched_iter(setType, data_params, sess):\n",
    "    global GLOBAL_SET_TYPE\n",
    "    GLOBAL_SET_TYPE = setType\n",
    "    \n",
    "    filenames_ph = tf.placeholder(tf.string, shape=[None])\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames_ph)\n",
    "    dataset = dataset.map(_parse_function)  # Parse the record into tensors.\n",
    "    dataset = dataset.shuffle(buffer_size=data_params['buffer_size'])\n",
    "    dataset = dataset.batch(data_params['batch_size'])\n",
    "    dataset = dataset.repeat(data_params['n_epochs'])\n",
    "    \n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    \n",
    "    tfrecords_file_name = str(GLOBAL_SET_TYPE) + '.tfrecords'\n",
    "    tfrecord_file_path = os.path.join(FINAL_DIR, tfrecords_file_name)\n",
    "    \n",
    "    # initialize\n",
    "    sess.run(iterator.initializer, feed_dict={filenames_ph: [tfrecord_file_path]})\n",
    "    \n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 0\n",
      "e: 1\n",
      "e: 2\n",
      "e: 3\n",
      "e: 4\n",
      "done with 'training'\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    data_params = {}\n",
    "    data_params['n_epochs'] = 5\n",
    "    data_params['batch_size'] = 16\n",
    "    data_params['buffer_size'] = 128\n",
    "    \n",
    "    # training\n",
    "    tr_iter = return_batched_iter('train', data_params, sess)\n",
    "    next_tr_element = tr_iter.get_next()\n",
    "    \n",
    "    # validation\n",
    "    val_iter = return_batched_iter('val', data_params, sess)\n",
    "    next_val_element = val_iter.get_next()\n",
    "    \n",
    "    for e in range(data_params['n_epochs']):\n",
    "        print(\"e: {}\".format(e))\n",
    "        \n",
    "        # training\n",
    "        while True:\n",
    "            try:\n",
    "                _ = sess.run(next_tr_element)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "        # validation (after training on entire training set, in this case)\n",
    "        while True:\n",
    "            try:\n",
    "                _ = sess.run(next_val_element)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "    \n",
    "    print(\"done with \\'training\\'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 1 => [1]\n",
      "i: 2 => [0]\n",
      "i: 3 => [1]\n",
      "i: 4 => [1]\n",
      "i: 5 => [1]\n",
      "i: 6 => [1]\n",
      "i: 7 => [1]\n",
      "i: 8 => [1]\n",
      "i: 9 => [0]\n",
      "i: 10 => [0]\n",
      "i: 11 => [1]\n",
      "i: 12 => [0]\n",
      "i: 13 => [0]\n",
      "i: 14 => [0]\n",
      "i: 15 => [0]\n",
      "i: 16 => [0]\n",
      "i: 17 => [0]\n",
      "i: 18 => [0]\n",
      "i: 19 => [1]\n",
      "i: 20 => [0]\n",
      "i: 21 => [0]\n",
      "i: 22 => [1]\n",
      "i: 23 => [1]\n",
      "i: 24 => [0]\n",
      "i: 25 => [0]\n",
      "i: 26 => [0]\n",
      "i: 27 => [0]\n",
      "i: 28 => [0]\n",
      "i: 29 => [1]\n",
      "i: 30 => [1]\n",
      "i: 31 => [0]\n",
      "i: 32 => [0]\n",
      "i: 33 => [1]\n",
      "i: 34 => [0]\n",
      "i: 35 => [1]\n",
      "i: 36 => [0]\n",
      "i: 37 => [1]\n",
      "i: 38 => [1]\n",
      "i: 39 => [0]\n",
      "i: 40 => [1]\n",
      "i: 41 => [0]\n",
      "i: 42 => [0]\n",
      "i: 43 => [0]\n",
      "i: 44 => [1]\n",
      "i: 45 => [0]\n",
      "i: 46 => [1]\n",
      "i: 47 => [1]\n",
      "i: 48 => [1]\n",
      "i: 49 => [0]\n",
      "i: 50 => [0]\n",
      "i: 51 => [0]\n",
      "i: 52 => [1]\n",
      "i: 53 => [0]\n",
      "i: 54 => [0]\n",
      "i: 55 => [0]\n",
      "i: 56 => [0]\n",
      "i: 57 => [0]\n",
      "i: 58 => [0]\n",
      "i: 59 => [0]\n",
      "i: 60 => [0]\n",
      "i: 61 => [1]\n",
      "i: 62 => [1]\n",
      "i: 63 => [0]\n",
      "i: 64 => [1]\n",
      "i: 65 => [1]\n",
      "i: 66 => [1]\n",
      "i: 67 => [1]\n",
      "i: 68 => [1]\n",
      "i: 69 => [0]\n",
      "i: 70 => [1]\n",
      "i: 71 => [0]\n",
      "i: 72 => [0]\n",
      "i: 73 => [1]\n",
      "i: 74 => [1]\n",
      "i: 75 => [0]\n",
      "i: 76 => [0]\n",
      "i: 77 => [0]\n",
      "i: 78 => [1]\n",
      "i: 79 => [1]\n",
      "i: 80 => [1]\n",
      "i: 81 => [1]\n",
      "i: 82 => [0]\n",
      "i: 83 => [1]\n",
      "i: 84 => [0]\n",
      "i: 85 => [0]\n",
      "i: 86 => [0]\n",
      "i: 87 => [1]\n",
      "i: 88 => [0]\n",
      "i: 89 => [1]\n",
      "i: 90 => [0]\n",
      "i: 91 => [1]\n",
      "i: 92 => [1]\n",
      "i: 93 => [0]\n",
      "i: 94 => [1]\n",
      "i: 95 => [0]\n",
      "i: 96 => [0]\n",
      "i: 97 => [0]\n",
      "i: 98 => [1]\n",
      "i: 99 => [0]\n",
      "i: 100 => [0]\n",
      "i: 101 => [1]\n",
      "i: 102 => [0]\n",
      "i: 103 => [1]\n",
      "i: 104 => [1]\n",
      "i: 105 => [1]\n",
      "i: 106 => [0]\n",
      "i: 107 => [1]\n",
      "i: 108 => [1]\n",
      "i: 109 => [0]\n",
      "i: 110 => [1]\n",
      "i: 111 => [1]\n",
      "i: 112 => [0]\n",
      "i: 113 => [0]\n",
      "i: 114 => [0]\n",
      "i: 115 => [1]\n",
      "i: 116 => [1]\n",
      "i: 117 => [1]\n",
      "i: 118 => [0]\n",
      "i: 119 => [1]\n",
      "i: 120 => [0]\n",
      "i: 121 => [1]\n",
      "i: 122 => [0]\n",
      "i: 123 => [1]\n",
      "i: 124 => [0]\n",
      "i: 125 => [1]\n",
      "i: 126 => [0]\n",
      "i: 127 => [1]\n",
      "i: 128 => [1]\n",
      "i: 129 => [0]\n",
      "i: 130 => [0]\n",
      "i: 131 => [1]\n",
      "i: 132 => [1]\n",
      "i: 133 => [1]\n",
      "i: 134 => [1]\n",
      "i: 135 => [0]\n",
      "i: 136 => [0]\n",
      "i: 137 => [0]\n",
      "i: 138 => [1]\n",
      "i: 139 => [0]\n",
      "i: 140 => [0]\n",
      "i: 141 => [1]\n",
      "i: 142 => [1]\n",
      "i: 143 => [1]\n",
      "i: 144 => [0]\n",
      "i: 145 => [0]\n",
      "i: 146 => [1]\n",
      "i: 147 => [1]\n",
      "i: 148 => [1]\n",
      "i: 149 => [1]\n",
      "i: 150 => [0]\n",
      "i: 151 => [0]\n",
      "i: 152 => [0]\n",
      "i: 153 => [1]\n",
      "i: 154 => [0]\n",
      "i: 155 => [0]\n",
      "i: 156 => [1]\n",
      "i: 157 => [0]\n",
      "i: 158 => [0]\n",
      "i: 159 => [1]\n",
      "i: 160 => [0]\n",
      "i: 161 => [1]\n",
      "i: 162 => [0]\n",
      "i: 163 => [1]\n",
      "i: 164 => [0]\n",
      "i: 165 => [1]\n",
      "i: 166 => [0]\n",
      "i: 167 => [0]\n",
      "i: 168 => [1]\n",
      "i: 169 => [0]\n",
      "i: 170 => [1]\n",
      "i: 171 => [0]\n",
      "i: 172 => [0]\n",
      "i: 173 => [1]\n",
      "i: 174 => [0]\n",
      "i: 175 => [1]\n",
      "i: 176 => [1]\n",
      "i: 177 => [0]\n",
      "i: 178 => [1]\n",
      "i: 179 => [1]\n",
      "i: 180 => [1]\n",
      "i: 181 => [0]\n",
      "i: 182 => [0]\n",
      "i: 183 => [1]\n",
      "i: 184 => [1]\n",
      "i: 185 => [1]\n",
      "i: 186 => [0]\n",
      "i: 187 => [0]\n",
      "i: 188 => [1]\n",
      "i: 189 => [0]\n",
      "i: 190 => [1]\n",
      "i: 191 => [0]\n",
      "i: 192 => [0]\n",
      "i: 193 => [0]\n",
      "i: 194 => [0]\n",
      "i: 195 => [1]\n",
      "i: 196 => [0]\n",
      "i: 197 => [0]\n",
      "i: 198 => [0]\n",
      "i: 199 => [0]\n",
      "i: 200 => [0]\n",
      "i: 201 => [1]\n",
      "i: 202 => [1]\n",
      "i: 203 => [0]\n",
      "i: 204 => [0]\n",
      "i: 205 => [1]\n",
      "i: 206 => [1]\n",
      "i: 207 => [0]\n",
      "i: 208 => [1]\n",
      "i: 209 => [1]\n",
      "i: 210 => [0]\n",
      "i: 211 => [1]\n",
      "i: 212 => [0]\n",
      "i: 213 => [1]\n",
      "i: 214 => [1]\n",
      "i: 215 => [1]\n",
      "i: 216 => [0]\n",
      "i: 217 => [1]\n",
      "i: 218 => [0]\n",
      "i: 219 => [0]\n",
      "i: 220 => [1]\n",
      "i: 221 => [1]\n",
      "i: 222 => [0]\n",
      "i: 223 => [0]\n",
      "i: 224 => [1]\n",
      "i: 225 => [0]\n",
      "i: 226 => [0]\n",
      "i: 227 => [1]\n",
      "i: 228 => [0]\n",
      "i: 229 => [0]\n",
      "i: 230 => [1]\n",
      "i: 231 => [1]\n",
      "i: 232 => [0]\n",
      "i: 233 => [1]\n",
      "i: 234 => [1]\n",
      "i: 235 => [0]\n",
      "i: 236 => [1]\n",
      "i: 237 => [1]\n",
      "i: 238 => [1]\n",
      "i: 239 => [0]\n",
      "i: 240 => [1]\n",
      "i: 241 => [0]\n",
      "i: 242 => [1]\n",
      "i: 243 => [0]\n",
      "i: 244 => [1]\n",
      "i: 245 => [0]\n",
      "i: 246 => [0]\n",
      "i: 247 => [0]\n",
      "i: 248 => [1]\n",
      "i: 249 => [1]\n",
      "i: 250 => [1]\n",
      "i: 251 => [1]\n",
      "i: 252 => [1]\n",
      "i: 253 => [0]\n",
      "i: 254 => [1]\n",
      "i: 255 => [1]\n",
      "i: 256 => [0]\n",
      "i: 257 => [1]\n",
      "i: 258 => [1]\n",
      "i: 259 => [1]\n",
      "i: 260 => [0]\n",
      "i: 261 => [1]\n",
      "i: 262 => [1]\n",
      "i: 263 => [1]\n",
      "i: 264 => [1]\n",
      "i: 265 => [0]\n",
      "i: 266 => [0]\n",
      "i: 267 => [0]\n",
      "i: 268 => [0]\n",
      "i: 269 => [0]\n",
      "i: 270 => [0]\n",
      "i: 271 => [0]\n",
      "i: 272 => [1]\n",
      "i: 273 => [1]\n",
      "i: 274 => [1]\n",
      "i: 275 => [0]\n",
      "i: 276 => [1]\n",
      "i: 277 => [1]\n",
      "i: 278 => [0]\n",
      "i: 279 => [1]\n",
      "i: 280 => [0]\n",
      "i: 281 => [0]\n",
      "i: 282 => [0]\n",
      "i: 283 => [1]\n",
      "i: 284 => [1]\n",
      "i: 285 => [0]\n",
      "i: 286 => [1]\n",
      "i: 287 => [1]\n",
      "i: 288 => [0]\n",
      "i: 289 => [0]\n",
      "i: 290 => [1]\n",
      "i: 291 => [1]\n",
      "i: 292 => [0]\n",
      "i: 293 => [1]\n",
      "i: 294 => [0]\n",
      "i: 295 => [1]\n",
      "i: 296 => [1]\n",
      "i: 297 => [1]\n",
      "i: 298 => [0]\n",
      "i: 299 => [1]\n",
      "i: 300 => [0]\n",
      "i: 301 => [1]\n",
      "i: 302 => [0]\n",
      "i: 303 => [1]\n",
      "i: 304 => [1]\n",
      "i: 305 => [1]\n",
      "i: 306 => [1]\n",
      "i: 307 => [0]\n",
      "i: 308 => [1]\n",
      "i: 309 => [0]\n",
      "i: 310 => [1]\n",
      "i: 311 => [1]\n",
      "i: 312 => [0]\n",
      "i: 313 => [0]\n",
      "i: 314 => [1]\n",
      "i: 315 => [0]\n",
      "i: 316 => [1]\n",
      "i: 317 => [0]\n",
      "i: 318 => [1]\n",
      "i: 319 => [1]\n",
      "i: 320 => [0]\n",
      "i: 321 => [1]\n",
      "i: 322 => [0]\n",
      "i: 323 => [1]\n",
      "i: 324 => [0]\n",
      "i: 325 => [1]\n",
      "i: 326 => [0]\n",
      "i: 327 => [1]\n",
      "i: 328 => [0]\n",
      "i: 329 => [1]\n",
      "i: 330 => [0]\n",
      "i: 331 => [1]\n",
      "i: 332 => [0]\n",
      "i: 333 => [0]\n",
      "i: 334 => [0]\n",
      "i: 335 => [0]\n",
      "i: 336 => [1]\n",
      "i: 337 => [0]\n",
      "i: 338 => [1]\n",
      "i: 339 => [1]\n",
      "i: 340 => [0]\n",
      "i: 341 => [0]\n",
      "i: 342 => [1]\n",
      "i: 343 => [0]\n",
      "i: 344 => [1]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    data_params = {}\n",
    "    data_params['n_epochs'] = 1\n",
    "    data_params['batch_size'] = 1\n",
    "    data_params['buffer_size'] = 1 # no shuffling\n",
    "    \n",
    "    test_iter = return_batched_iter('test', data_params, sess)\n",
    "    next_test_element = test_iter.get_next()\n",
    "    \n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            i += 1\n",
    "            print(\"i: {} => {}\".format(i, sess.run(next_test_element)[1]))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_edge",
   "language": "python",
   "name": "tf_edge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
