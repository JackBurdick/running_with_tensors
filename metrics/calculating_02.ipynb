{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics: Calculating_02: tf.metrics() intro\n",
    "\n",
    "\n",
    "<font color=red>TODO: add links and information on the series here </font>\n",
    "\n",
    "#### series information\n",
    "- previous: calculating accuracy [video]() [notebook]()\n",
    "- next:  [video]() [notebook]()\n",
    "\n",
    "#### Related series\n",
    "- metrics [video]() [notebook]()\n",
    "\n",
    "#### Resources:\n",
    " - [official tensorflow documentation](https://www.tensorflow.org/api_docs/python/tf/metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Helper to make the output consistent\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "# supress most messages (display only error messages)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# reset graph\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.int32, [10])\n",
    "y = tf.placeholder(tf.int32, [10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tf.metrics()\n",
    "Using;\n",
    "```\n",
    "tf.variable_scope()\n",
    "tf...get_variables()\n",
    "tf.variables_initializer()\n",
    "```\n",
    "\n",
    "<font color=red>TODO: add links and information </font>\n",
    "\n",
    "tf.variable_scope crates a scope ------- , then, using tf...get_variables() we create an opperation to initialize (and reset) the specfied variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"accuracy_metrics\") as scope:\n",
    "    acc, acc_op = tf.metrics.accuracy(labels=y, predictions=x)\n",
    "    m_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "    acc_reset_op = tf.variables_initializer(m_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include previous method for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate ratio of correct predictions\n",
    "correct_prediction = tf.equal(x, y)\n",
    "batch_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fake data\n",
    "y_pred_cls = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "y_true_cls = np.array([0, 0, 1, 0, 0, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize global and local variables\n",
    "<font color=red>TODO: add links and information on the series here </font>\n",
    "```\n",
    "tf.local_variables_initializer()\n",
    "```\n",
    "We need to use a local initializer to initialize the metrics in tf.metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initializers, global and local\n",
    "init_global = tf.global_variables_initializer()\n",
    "init_local = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the session and global and local vars\n",
    "# note: we will want to close the session at the end `sess.close()`\n",
    "sess = tf.Session()\n",
    "init_global.run(session=sess)\n",
    "init_local.run(session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_acc: 40.00% acc: 0.00%, acc_op: 40.00%\n"
     ]
    }
   ],
   "source": [
    "v = sess.run([batch_acc, acc, acc_op], feed_dict={x: y_pred_cls,\n",
    "                                                  y: y_true_cls})\n",
    "print(\"batch_acc: {:.2f}% acc: {:.2f}%, acc_op: {:.2f}%\".format(v[0]*100, v[1]*100, v[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the results\n",
    "```\n",
    "batch acc: 40.00%\n",
    "acc: 0.00%\n",
    "acc_op: 40.00%\n",
    "```\n",
    "The manual calculation of accuracy (as shown in the previous notebook|video) `batch_acc` produces results as expected.  The `acc` result may not be expected though as it returns **0%** when we know the accuracy should be **40%** and the `acc_op` returns the expected **40%**.\n",
    "\n",
    "What happens if we run these operations again with different data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_acc: 80.00% acc: 40.00%, acc_op: 60.00%\n"
     ]
    }
   ],
   "source": [
    "# 80% are correct (matching)\n",
    "y_pred_cls = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1])\n",
    "y_true_cls = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "v = sess.run([batch_acc, acc, acc_op], feed_dict={x: y_pred_cls,\n",
    "                                                  y: y_true_cls})\n",
    "print(\"batch_acc: {:.2f}% acc: {:.2f}%, acc_op: {:.2f}%\".format(v[0]*100, v[1]*100, v[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Um.....\n",
    "```\n",
    "batch acc: 60.00%\n",
    "acc: 40.00%\n",
    "acc_op: 50.00%\n",
    "```\n",
    "What just happened?\n",
    "\n",
    "This might make sense to you -- if so, great! If not, I bet you can figure it out pretty quickly.\n",
    "\n",
    "We expected our value to be **60%** and that is what our `batch_acc` returned.  But acc returned **40%**, which, if you remember was our previous data's accuracy and our `acc_op` returned **50%**.\n",
    "\n",
    "If you read the documentation -- which, I'm sure you did, right? ... right? (here's a [link](https://www.tensorflow.org/api_docs/python/tf/metrics/accuracy) again just in case) -- then you'll find that `tf.metrics.accuracy()` is pretty clever under the hood. The function creates and updates two local variables (`total` and `count`) that are used to calculate the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_acc: 80.00% acc: 60.00%, acc_op: 66.67%\n"
     ]
    }
   ],
   "source": [
    "v = sess.run([batch_acc, acc, acc_op], feed_dict={x: y_pred_cls,\n",
    "                                                  y: y_true_cls})\n",
    "print(\"batch_acc: {:.2f}% acc: {:.2f}%, acc_op: {:.2f}%\".format(v[0]*100, v[1]*100, v[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "batch acc: 80.00%\n",
    "acc: 60.00%\n",
    "acc_op: 66.67%\n",
    "```\n",
    "\n",
    "We expected **80%** and that's what `batch_acc` returned. `acc` is returning the previous `accuracy` value **60.00%** and `acc_op` is returning **66.67%**.\n",
    "\n",
    "\n",
    "### What's all this 66.67% about?\n",
    "so let's think about what data we've fed into our session.\n",
    "1. 4/10\n",
    "1. 8/10\n",
    "1. 8/10\n",
    "\n",
    "<font color=red>TODO: add equation </font>\n",
    "\n",
    "What is the running accuracy of these values? (4+8+8)/(10+10+10) = 20/30 = **66.67%**\n",
    "\n",
    "So `acc_op` is returning the current running accuracy for us.  Great, but now what is `acc` doing?  It's still returning the previous running accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_acc: 80.00% acc_op: 70.00%, acc: 66.67%\n"
     ]
    }
   ],
   "source": [
    "# What if I run the opperations in a different order?\n",
    "v = sess.run([batch_acc, acc_op, acc], feed_dict={x: y_pred_cls,\n",
    "                                                  y: y_true_cls})\n",
    "print(\"batch_acc: {:.2f}% acc_op: {:.2f}%, acc: {:.2f}%\".format(v[0]*100, v[1]*100, v[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing intersesting -- we're still seeing the same trend in results.\n",
    "\n",
    "What if I run them in the different order than before, but instead, run them one at a time?\n",
    "\n",
    "_note: I'm using a different call here. rather than call sess.run() and pass the operation, I'm `eval()`uating the operation with the `session=`to our `sess`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "print(acc.eval(feed_dict={x: y_pred_cls, y: y_true_cls}, session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "print(acc_op.eval(feed_dict={x: y_pred_cls, y: y_true_cls}, session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "print(acc.eval(feed_dict={x: y_pred_cls, y: y_true_cls}, session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "print(acc.eval(feed_dict={x: y_pred_cls, y: y_true_cls}, session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "print(acc.eval(feed_dict={x: y_pred_cls, y: y_true_cls}, session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.733333\n"
     ]
    }
   ],
   "source": [
    "print(acc_op.eval(feed_dict={x: y_pred_cls, y: y_true_cls}, session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.742857\n"
     ]
    }
   ],
   "source": [
    "print(acc_op.eval(feed_dict={x: y_pred_cls, y: y_true_cls}, session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.742857\n"
     ]
    }
   ],
   "source": [
    "print(acc.eval(feed_dict={x: y_pred_cls, y: y_true_cls}, session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.742857\n"
     ]
    }
   ],
   "source": [
    "print(acc.eval(feed_dict={x: y_pred_cls, y: y_true_cls}, session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See it yet?\n",
    "Should I run a couple more? Only kidding, I'm done.\n",
    "\n",
    "The point is this, the `acc_op` is updating the local variables and then returning the metric and `acc` is only displaying them.  Pretty cool, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# close the session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take home\n",
    "\n",
    "In our exmples, and as well see later;\n",
    "- `tf.metrics().xxxx()` keeps track of the overall metric value, not only the current batch\n",
    "- local vars (`total` and `count`) need to be initialized\n",
    "    - `tf.local_variables_initializer()`\n",
    "- `acc` returned the current metric value\n",
    "- `acc_op` updated the local variables then returned the value\n",
    "\n",
    "\n",
    "## Next steps\n",
    "- checkout the related [exercises]()\n",
    "- additional common tf.metrics() [video]()|[notebook]()\n",
    "- how to better control the metrics [video]()|[notebook]()\n",
    "\n",
    "<font color=red>TODO: Insert image of accuracy calculation here </font>\n",
    "\n",
    "Next, we'll [use and discuss more functions]() from the tf.metrics metrics package and then we'll look at how to better [control these metrics]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documentation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-69132aba9eed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#### Precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mdocumentation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#### Recall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdocumentation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'documentation' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
