{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: (3, 5, 4, 'final', 0)\n",
      "TensorFlow: 1.4.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# DL framework\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# common packages\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# timing epochs\n",
    "import time\n",
    "\n",
    "# read image\n",
    "from scipy.misc import imread\n",
    "\n",
    "# for ordered dict when building layer components\n",
    "import collections\n",
    "\n",
    "# plotting pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# for data augmentation\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "\n",
    "# for shuffling data batches\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# const\n",
    "SEED = 42\n",
    "\n",
    "# Helper to make the output consistent\n",
    "def reset_graph(seed=SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    \n",
    "# set log level to supress messages, unless an error\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Version information\n",
    "print(\"Python: {}\".format(sys.version_info[:]))\n",
    "print('TensorFlow: {}'.format(tf.__version__))\n",
    "\n",
    "# Check if using GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU found')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "    \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for saving the best params\n",
    "# this shouldn't be necessary but I'm having trouble using the saver to save\n",
    "# with the best params\n",
    "import pickle\n",
    "def save_obj(obj, name):\n",
    "    with open('best_params/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('best_params/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQUARE_DIM = 224\n",
    "if SQUARE_DIM:\n",
    "    IMG_WIDTH = SQUARE_DIM\n",
    "    IMG_HEIGHT = SQUARE_DIM\n",
    "    \n",
    "CHANNELS = 3\n",
    "\n",
    "# utility plotting function\n",
    "def plot_color_image(image):\n",
    "    plt.imshow(image.astype(np.uint8),interpolation=\"nearest\")\n",
    "    plt.title(\"{}x{}\".format(image.shape[1], image.shape[0]))\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "# load dataset into memory\n",
    "X_tr = np.load('../numpy/sigmoid/' + str(SQUARE_DIM) + '/X_train.npy')\n",
    "y_tr = np.load('../numpy/sigmoid/' + str(SQUARE_DIM) + '/y_train.npy')\n",
    "\n",
    "X_val = np.load('../numpy/sigmoid/' + str(SQUARE_DIM) + '/X_val.npy')\n",
    "y_val = np.load('../numpy/sigmoid/' + str(SQUARE_DIM) + '/y_val.npy')\n",
    "\n",
    "X_test = np.load('../numpy/sigmoid/' + str(SQUARE_DIM) + '/X_test.npy')\n",
    "y_test = np.load('../numpy/sigmoid/' + str(SQUARE_DIM) + '/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # validation\n",
    "X_val /= 255\n",
    "X_val -= 0.5\n",
    "X_val *= 2\n",
    "\n",
    "\n",
    "# # test\n",
    "X_test /= 255\n",
    "X_test -= 0.5\n",
    "X_test *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is adapted from https://github.com/ageron/handson-ml/blob/master/13_convolutional_neural_networks.ipynb\n",
    "def val_augment_image(image, lrf_p=0.0, upf_p=0.0, roll_num=0):\n",
    "        \n",
    "    n_image = image\n",
    "\n",
    "    # Flip the image horizontally with upf_p% probability:\n",
    "    if np.random.rand() < lrf_p:\n",
    "        n_image = np.fliplr(n_image)\n",
    "\n",
    "    # Flip the image vertically with lrf_p% probability:\n",
    "    if np.random.rand() < upf_p:\n",
    "        n_image = np.flipud(n_image)\n",
    "        \n",
    "    # 50% chance roll image horiz\n",
    "    if np.random.rand() < 0.5:\n",
    "        val = int(np.random.rand() * roll_num)\n",
    "        # 50% left or right\n",
    "        if np.random.rand() < 0.5:\n",
    "            n_image = np.roll(n_image, val, axis=1)\n",
    "        else:\n",
    "            n_image = np.roll(n_image, -val, axis=1)\n",
    "        \n",
    "    # 50% chance roll image vert\n",
    "    if np.random.rand() < 0.5:\n",
    "        val = int(np.random.rand() * roll_num)\n",
    "        # 50% left or right\n",
    "        if np.random.rand() < 0.5:\n",
    "            n_image = np.roll(n_image, val, axis=0)\n",
    "        else:\n",
    "            n_image = np.roll(n_image, -val, axis=0)\n",
    "\n",
    "    # ensure float32\n",
    "    n_image = n_image.astype(np.float32)\n",
    "\n",
    "    return n_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is adapted from https://github.com/ageron/handson-ml/blob/master/13_convolutional_neural_networks.ipynb\n",
    "def augment_image(image, lrf_p=0.0, upf_p=0.0, r_degree=0.0, max_zoom=0.0, roll_num=0):\n",
    "        \n",
    "    n_image = image\n",
    "    \n",
    "    \n",
    "    # Flip the image horizontally with upf_p% probability:\n",
    "    if np.random.rand() < lrf_p:\n",
    "        n_image = np.fliplr(n_image)\n",
    "\n",
    "    # Flip the image vertically with lrf_p% probability:\n",
    "    if np.random.rand() < upf_p:\n",
    "        n_image = np.flipud(n_image)\n",
    "\n",
    "    # 50% chance roll image horiz\n",
    "    if np.random.rand() < 0.5:\n",
    "        val = int(np.random.rand() * roll_num)\n",
    "        # 50% left or right\n",
    "        if np.random.rand() < 0.5:\n",
    "            n_image = np.roll(n_image, val, axis=1)\n",
    "        else:\n",
    "            n_image = np.roll(n_image, -val, axis=1)\n",
    "        \n",
    "    # 50% chance roll image vert\n",
    "    if np.random.rand() < 0.5:\n",
    "        val = int(np.random.rand() * roll_num)\n",
    "        # 50% left or right\n",
    "        if np.random.rand() < 0.5:\n",
    "            n_image = np.roll(n_image, val, axis=0)\n",
    "        else:\n",
    "            n_image = np.roll(n_image, -val, axis=0)\n",
    "\n",
    "    # normalize on the fly\n",
    "    # this is inefficient, but we need to use this here since the resize converts\n",
    "    # back from uint8\n",
    "    n_image = n_image.astype(np.float32)\n",
    "\n",
    "    #print(\"pre\", n_image[0][0])\n",
    "    n_image /= 255\n",
    "    n_image -= 0.5\n",
    "    n_image *= 2\n",
    "    \n",
    "    if r_degree > 0.0:\n",
    "        r_degree *= np.random.rand()\n",
    "        # randomly choose rotation direction\n",
    "        r_dir = (1 if np.random.rand() > 0.5 else -1)\n",
    "        n_image = ndimage.interpolation.rotate(n_image, r_dir*r_degree, reshape=False)\n",
    "    \n",
    "\n",
    "    return n_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not my code -- this is adapted from hands on ML\n",
    "def get_model_params():\n",
    "    gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    return {gvar.op.name: value for gvar, value in zip(gvars, tf.get_default_session().run(gvars))}\n",
    "\n",
    "def restore_model_params(model_params, g, sess):\n",
    "    gvar_names = list(model_params.keys())\n",
    "    assign_ops = {gvar_name: g.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                  for gvar_name in gvar_names}\n",
    "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "    sess.run(assign_ops, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "n_outputs = 1\n",
    "\n",
    "# l2_regularizer = tf.contrib.layers.l2_regularizer(0.01)\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "def build_vgg(ARCH_ID, NAME_STR):\n",
    "\n",
    "    reset_graph()\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "\n",
    "        with tf.name_scope(\"hyper_params\"):\n",
    "            learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "        with tf.name_scope(\"inputs\"):\n",
    "            # data\n",
    "            X = tf.placeholder(tf.float32, shape=[None, IMG_HEIGHT, IMG_WIDTH, CHANNELS], name=\"X\")\n",
    "\n",
    "            # labels\n",
    "            y_raw = tf.placeholder(tf.int64, shape=[None, n_outputs], name=\"y_input\")\n",
    "            y_ = tf.cast(y_raw, tf.float32)\n",
    "\n",
    "            # for training/evaluating\n",
    "            training = tf.placeholder_with_default(False, shape=[], name=\"training\")\n",
    "\n",
    "        \n",
    "        with tf.name_scope(\"gan\"):\n",
    "            # 224x224\n",
    "            conv1_1 = tf.layers.conv2d(inputs=X, filters=64, kernel_size=3,\n",
    "                                   strides=2, padding=\"SAME\",\n",
    "                                   activation=tf.nn.selu, \n",
    "                                   kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                   bias_initializer=tf.zeros_initializer(), trainable=True,\n",
    "                                   name=\"discriminator/Gconv_1\")\n",
    "\n",
    "            # 112x112\n",
    "            conv1_2 = tf.layers.conv2d(conv1_1, 128, kernel_size=3, strides=2, padding='same', \n",
    "                                   activation=tf.nn.selu,\n",
    "                                   kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                   bias_initializer=tf.zeros_initializer(), trainable=True,\n",
    "                                   name=\"discriminator/Gconv_2\")\n",
    "\n",
    "            # 56x56\n",
    "            conv2_1 = tf.layers.conv2d(conv1_2, 256, kernel_size=3, strides=2, padding='same', \n",
    "                                               activation=tf.nn.selu,\n",
    "                                               kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                               bias_initializer=tf.zeros_initializer(), trainable=True,\n",
    "                                               name=\"discriminator/Gconv_3\")\n",
    "            \n",
    "#             \n",
    "\n",
    "            # 28x28x256\n",
    "            conv2_2 = tf.layers.conv2d(conv2_1, 512, kernel_size=3, strides=2, padding='same', \n",
    "                                   activation=tf.nn.selu,\n",
    "                                   kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                   bias_initializer=tf.zeros_initializer(), trainable=True,\n",
    "                                   name=\"discriminator/Gconv_4\")\n",
    "#             \n",
    "            # 14x14x512\n",
    "            conv2_3 = tf.layers.conv2d(conv2_2, 768, kernel_size=3, strides=1, padding='same', \n",
    "                                   activation=tf.nn.selu,\n",
    "                                   kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                   bias_initializer=tf.zeros_initializer(), trainable=True,\n",
    "                                   name=\"discriminator/my_conv\")\n",
    "            conv2_2 = tf.layers.max_pooling2d(inputs=conv2_3, pool_size=[2,2], strides=2, name=\"max_pool_myc\")\n",
    "            #7x7x768\n",
    "        \n",
    "            ###### 0\n",
    "            if NAME_STR == \"discrim_class_0c_0f\" or NAME_STR == \"discrim_class_0c_4f\":\n",
    "                last_shape = int(np.prod(conv2_2.get_shape()[1:]))\n",
    "                pool_flat = tf.reshape(conv2_2, shape=[-1, last_shape])\n",
    "        \n",
    "        \n",
    "        \n",
    "            #-------------------------------------------------------------------------------------------------------------\n",
    "            ##### A\n",
    "\n",
    "            elif NAME_STR == \"discrim_class_1c_0f\" or NAME_STR == \"discrim_class_1c_4f\":\n",
    "                # 14x14\n",
    "                conv_new_01 = tf.layers.conv2d(conv2_2, 1024, kernel_size=7, strides=1, padding='same', \n",
    "                                           activation=tf.nn.selu,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                           bias_initializer=tf.zeros_initializer(), trainable=True,\n",
    "                                           name=\"new_01\")\n",
    "                last_shape = int(np.prod(conv_new_01.get_shape()[1:]))\n",
    "                pool_flat = tf.reshape(conv_new_01, shape=[-1, last_shape])\n",
    "            \n",
    "            #-------------------------------------------------------------------------------------------------------------\n",
    "            ##### B\n",
    "            elif NAME_STR == \"discrim_class_2c_0f\" or NAME_STR == \"discrim_class_2c_4f\":\n",
    "                # 14x14\n",
    "                conv_new_01 = tf.layers.conv2d(conv2_2, 1024, kernel_size=3, strides=1, padding='same', \n",
    "                                           activation=tf.nn.selu,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                           bias_initializer=tf.zeros_initializer(), trainable=True,\n",
    "                                           name=\"new_01\")\n",
    "\n",
    "                conv2_2_pool = tf.layers.max_pooling2d(inputs=conv_new_01, padding='same', pool_size=[2,2], strides=2, name=\"max_pool_02\")\n",
    "\n",
    "                # 7x7\n",
    "                conv_new_02 = tf.layers.conv2d(conv2_2_pool, 2048, kernel_size=3, strides=1, \n",
    "                                           activation=tf.nn.selu,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                           bias_initializer=tf.zeros_initializer(), trainable=True,\n",
    "                                           name=\"new_02\")\n",
    "                last_shape = int(np.prod(conv_new_02.get_shape()[1:]))\n",
    "                pool_flat = tf.reshape(conv_new_02, shape=[-1, last_shape])\n",
    "            \n",
    "            # ------------------------------------------------------------------------------------------ [END]\n",
    "            else:\n",
    "                print(\"shoot - somethings wrong\")\n",
    "\n",
    "            # reshape\n",
    "            print(\"last_shape: \", last_shape)\n",
    "            pool_flat_drop = tf.layers.dropout(pool_flat, 0.33, training=training, name=\"entry_drop\")\n",
    "        \n",
    "\n",
    "#         with tf.name_scope(\"fc1\"):\n",
    "#             fc1 = tf.layers.dense(pool_flat_drop, 512, kernel_initializer=he_init, activation=tf.nn.selu, name=\"fc1\")\n",
    "#             fc1_drop = tf.layers.dropout(fc1, 0.33, training=training, name=\"fc1_drop\")\n",
    "\n",
    "#         with tf.name_scope(\"fc2\"):\n",
    "#             fc2 = tf.layers.dense(fc1_drop, 256, kernel_initializer=he_init, activation=tf.nn.selu, name=\"fc2\")\n",
    "#             fc2 = tf.layers.dropout(fc2, 0.5, training=training, name=\"fc1_drop\")\n",
    "\n",
    "#         with tf.name_scope(\"fc2\"):\n",
    "            fc1 = tf.layers.dense(pool_flat_drop, 64, kernel_initializer=he_init, activation=tf.nn.selu, name=\"fc1\")\n",
    "\n",
    "            \n",
    "        with tf.name_scope(\"output_layer\"):\n",
    "            logits_ = tf.layers.dense(fc1, n_outputs, name=\"sigmoid_output\")\n",
    "            preds = tf.sigmoid(logits_, name=\"preds\")\n",
    "\n",
    "        # ================================================ cost\n",
    "        with tf.name_scope(\"cross_entropy\"):\n",
    "            xentropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_, labels=y_)\n",
    "            batch_loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "        with tf.name_scope(\"train\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                               beta1=0.9,\n",
    "                                               beta2=0.999,\n",
    "                                               epsilon=1e-08,\n",
    "                                               use_locking=False,\n",
    "                                               name='Adam')\n",
    "            \n",
    "            all_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "            if NAME_STR == \"discrim_class_0c_0f\" or NAME_STR == \"discrim_class_1c_0f\" or NAME_STR == \"discrim_class_2c_0f\":\n",
    "                train_vars = all_vars\n",
    "                print(\"training all the vars\")\n",
    "            else:\n",
    "                freeze_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"discriminator/Gconv_[1234]\")\n",
    "                train_vars = [var for var in all_vars if var not in freeze_vars]\n",
    "                print(\"num vars:\", len(all_vars), \"= frozen(\", len(freeze_vars), \")\", \"+ train(\", len(train_vars), \")\")\n",
    "\n",
    "            training_op = optimizer.minimize(batch_loss, var_list=train_vars, name=\"training_op\")\n",
    "\n",
    "\n",
    "        # =============================================== metrics\n",
    "        with tf.name_scope(\"train_metrics\") as scope:    \n",
    "            # train_y_true_cls = tf.cast(y_, tf.bool)\n",
    "            train_y_true_cls = tf.greater_equal(y_, 0.5)\n",
    "            train_y_pred_cls = tf.greater_equal(preds, 0.5)\n",
    "\n",
    "            correct_prediction = tf.equal(train_y_pred_cls, train_y_true_cls)\n",
    "            train_batch_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            \n",
    "            train_auc, train_auc_update = tf.metrics.auc(labels=y_, predictions=preds)\n",
    "\n",
    "            train_tf_acc, train_tf_acc_op = tf.metrics.accuracy(labels=train_y_true_cls, predictions=train_y_pred_cls)\n",
    "            train_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "            train_acc_reset_op = tf.variables_initializer(train_acc_vars, name=\"train_acc_reset_op\")\n",
    "            \n",
    "        for node in (y_, preds, train_y_true_cls, train_y_pred_cls, correct_prediction, train_batch_acc):\n",
    "            g.add_to_collection(\"label_nodes\", node)\n",
    "\n",
    "        with tf.name_scope(\"validation_metrics\") as scope:    \n",
    "            val_y_true_cls = tf.greater_equal(y_, 0.5)\n",
    "            val_y_pred_cls = tf.greater_equal(preds, 0.5)\n",
    "\n",
    "            val_correct_prediction = tf.equal(val_y_pred_cls, val_y_true_cls)\n",
    "            val_batch_acc = tf.reduce_mean(tf.cast(val_correct_prediction, tf.float32))\n",
    "            \n",
    "            val_auc, val_auc_update = tf.metrics.auc(labels=y_, predictions=preds)\n",
    "\n",
    "            val_tf_acc, val_tf_acc_op = tf.metrics.accuracy(labels=val_y_true_cls, predictions=val_y_pred_cls)\n",
    "            val_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "            val_acc_reset_op = tf.variables_initializer(val_acc_vars, name=\"val_acc_reset_op\")\n",
    "\n",
    "        with tf.name_scope(\"test_metrics\") as scope:    \n",
    "            test_y_true_cls = tf.greater_equal(y_, 0.5)\n",
    "            test_y_pred_cls = tf.greater_equal(preds, 0.5)\n",
    "            \n",
    "            test_auc, test_auc_update = tf.metrics.auc(labels=y_, predictions=preds)\n",
    "            \n",
    "            test_tf_acc, test_tf_acc_op = tf.metrics.accuracy(labels=test_y_true_cls, predictions=test_y_pred_cls)\n",
    "            test_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "            test_acc_reset_op = tf.variables_initializer(test_acc_vars, name=\"test_acc_reset_op\")\n",
    "\n",
    "        # =============================================== loss \n",
    "        with tf.name_scope(\"train_loss_eval\") as scope:\n",
    "            train_mean_loss, train_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "            train_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "            train_loss_reset_op = tf.variables_initializer(train_loss_vars, name=\"train_loss_reset_op\")\n",
    "\n",
    "        with tf.name_scope(\"val_loss_eval\") as scope:\n",
    "            val_mean_loss, val_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "            val_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "            val_loss_reset_op = tf.variables_initializer(val_loss_vars, name=\"val_loss_reset_op\")\n",
    "\n",
    "        with tf.name_scope(\"test_loss_eval\") as scope:\n",
    "            test_mean_loss, test_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "            test_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "            test_loss_reset_op = tf.variables_initializer(test_loss_vars, name=\"test_loss_rest_op\")\n",
    "\n",
    "        # ================================================ initialize and save\n",
    "        with tf.name_scope(\"save_session\"):\n",
    "            init_global = tf.global_variables_initializer()\n",
    "            init_local = tf.local_variables_initializer()\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "        # ==================================== combine operations\n",
    "        # ===== epoch, train\n",
    "        epoch_train_loss_scalar = tf.summary.scalar('train_epoch_loss', train_mean_loss)\n",
    "#         epoch_train_acc_scalar = tf.summary.scalar('train_epoch_acc', train_tf_acc)\n",
    "        epoch_train_auc_scalar = tf.summary.scalar('train_epoch_auc', train_auc)\n",
    "        epoch_train_write_op = tf.summary.merge([epoch_train_loss_scalar, epoch_train_auc_scalar], name=\"epoch_train_write_op\")\n",
    "\n",
    "        # ===== epoch, validation\n",
    "        epoch_validation_loss_scalar = tf.summary.scalar('validation_epoch_loss', val_mean_loss)\n",
    "#         epoch_validation_acc_scalar = tf.summary.scalar('validation_epoch_acc', val_tf_acc)\n",
    "        epoch_validation_auc_scalar = tf.summary.scalar('validation_epoch_auc', val_auc)\n",
    "        epoch_validation_write_op = tf.summary.merge([epoch_validation_loss_scalar, epoch_validation_auc_scalar], name=\"epoch_validation_write_op\")\n",
    "\n",
    "        # ====== batch, train\n",
    "        train_batch_loss_scalar = tf.summary.scalar('train_batch_loss', batch_loss)\n",
    "        train_batch_acc_scalar = tf.summary.scalar('train_batch_acc', train_batch_acc)\n",
    "        train_batch_write_op = tf.summary.merge([train_batch_loss_scalar, train_batch_acc_scalar], name=\"train_batch_write_op\")\n",
    "\n",
    "        # ====== checkpoint, validation\n",
    "        checkpoint_validation_loss_scalar = tf.summary.scalar('validation_batch_loss', batch_loss)\n",
    "        checkpoint_validation_acc_scalar = tf.summary.scalar('validation_batch_acc', val_batch_acc)\n",
    "        checkpoint_validation_write_op = tf.summary.merge([checkpoint_validation_loss_scalar, checkpoint_validation_acc_scalar], name=\"checkpoint_valdiation_write_op\")\n",
    "        \n",
    "        # write operations\n",
    "        for node in (epoch_train_write_op, epoch_validation_write_op, train_batch_write_op, checkpoint_validation_write_op):\n",
    "            g.add_to_collection(\"write_ops\", node)\n",
    "            \n",
    "        # saver/init\n",
    "        for node in (init_global, init_local):\n",
    "            g.add_to_collection(\"save_init\", node)\n",
    "            \n",
    "        # acc metrics\n",
    "        for node in (train_tf_acc, train_tf_acc_op, train_acc_reset_op, \n",
    "                     train_auc, train_auc_update,\n",
    "                     val_tf_acc, val_tf_acc_op, val_acc_reset_op,\n",
    "                     val_auc, val_auc_update,\n",
    "                     test_tf_acc, test_tf_acc_op, test_acc_reset_op,\n",
    "                     test_auc, test_auc_update):\n",
    "            g.add_to_collection(\"acc_metrics\", node)\n",
    "        \n",
    "        # loss metrics\n",
    "        for node in (train_mean_loss, train_mean_loss_update, train_loss_reset_op, \n",
    "                     val_mean_loss, val_mean_loss_update, val_loss_reset_op, \n",
    "                     test_mean_loss, test_mean_loss_update, test_loss_reset_op):\n",
    "            g.add_to_collection(\"loss_metrics\", node)\n",
    "            \n",
    "        # test metrics\n",
    "        for node in (train_y_true_cls, train_y_pred_cls):\n",
    "            g.add_to_collection(\"test_metrics\", node)\n",
    "            \n",
    "        # main operations\n",
    "        for node in (training_op, X, y_raw, training, learning_rate):\n",
    "            g.add_to_collection(\"main_ops\", node)\n",
    "\n",
    "        print(\"model built.\")\n",
    "\n",
    "    return g, saver\n",
    "\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "def run_that_model(NAME_STR):\n",
    "    global X_tr\n",
    "    global y_tr\n",
    "    global X_val\n",
    "    global y_val\n",
    "    global X_test\n",
    "    global y_test\n",
    "    global batch_size\n",
    "    print(\"batch_size: \", batch_size)\n",
    "    NUM_EPOCHS = 50\n",
    "    \n",
    "    # lr_choice = [1e-5, 1e-4, 1e-3]\n",
    "    # LEARNING_RATE = np.random.choice(lr_choice).tolist()\n",
    "#     LEARNING_RATE = 0.000001\n",
    "    LEARNING_RATE = 1e-6\n",
    "    # -----------\n",
    "    #####################################\n",
    "    # ----------- augmentation\n",
    "    ##### training\n",
    "    AUG_BOOL = True\n",
    "    LRF_P = 0.5\n",
    "    UPF_p = 0.5\n",
    "    MAX_ZOOM = 0.0\n",
    "    ROLL_NUM = 25\n",
    "    ROT_DEG = 0.0\n",
    "    \n",
    "    ##### validation\n",
    "    VAL_AUG_BOOL = True\n",
    "    VAL_LRF_P = 0.3\n",
    "    VAL_UPF_P = 0.3\n",
    "    VAL_ROLL_NUM = 20\n",
    "    ######################################\n",
    "\n",
    "    # --- save best params\n",
    "    best_model_params = None\n",
    "    best_loss_val = np.infty\n",
    "\n",
    "    # ------ for \"early-ish\" stopping\n",
    "    # check_interval = 15\n",
    "    checks_since_last_progress = 0\n",
    "    # max_checks_without_progress = 20\n",
    "\n",
    "    # Tensorboard\n",
    "    now = datetime.now().strftime(\"%d%b%Y_%H%M%S\")\n",
    "    root_logdir = \"tf_logs/final_discrim_class/\" + NAME_STR + \"_3\"\n",
    "    logdir = \"{}/{}/\".format(root_logdir, now)\n",
    "\n",
    "    g, saver = build_vgg(now, NAME_STR)\n",
    "    \n",
    "    \n",
    "    epoch_train_write_op, epoch_validation_write_op, train_batch_write_op, checkpoint_validation_write_op = g.get_collection(\"write_ops\")\n",
    "    init_global, init_local = g.get_collection(\"save_init\")\n",
    "    train_tf_acc, train_tf_acc_op, train_acc_reset_op, train_auc, train_auc_update, val_tf_acc, val_tf_acc_op, val_acc_reset_op, val_auc, val_auc_update, test_tf_acc, test_tf_acc_op, test_acc_reset_op, test_auc, test_auc_update = g.get_collection(\"acc_metrics\")\n",
    "    train_mean_loss, train_mean_loss_update, train_loss_reset_op, val_mean_loss, val_mean_loss_update, val_loss_reset_op, test_mean_loss, test_mean_loss_update, test_loss_reset_op = g.get_collection(\"loss_metrics\")\n",
    "    training_op, X, y_raw, training, learning_rate = g.get_collection(\"main_ops\")\n",
    "    #prepared_image_op, input_image = g.get_collection(\"preprocess\")\n",
    "    step = 0\n",
    "    with tf.Session(graph=g) as sess:\n",
    "        init_global.run()\n",
    "        init_local.run()\n",
    "        \n",
    "        if NAME_STR == \"discrim_class_0c_0f\" or NAME_STR == \"discrim_class_1c_0f\" or NAME_STR == \"discrim_class_2c_0f\":\n",
    "            print(\"not loading vars\")\n",
    "        else:\n",
    "            const_lookup = 0\n",
    "            max_lookup = 5\n",
    "            conv1_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                   scope=\"discriminator/Gconv_1\")\n",
    "            conv1_init = sess.run(conv1_vars)\n",
    "            print(conv1_init[const_lookup][const_lookup][const_lookup][const_lookup][:max_lookup])\n",
    "            reuse_vars = g.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                               scope=\"discriminator/Gconv_[1234]\") \n",
    "            reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "            restore_saver = tf.train.Saver(reuse_vars_dict)\n",
    "            restore_saver.restore(sess, \"../GAN/saver/GAN_Model_3.ckpt\")\n",
    "            conv1_restore = sess.run(conv1_vars)\n",
    "            print(\"loaded {} vars\".format(len(reuse_vars_dict)))\n",
    "            print(conv1_restore[const_lookup][const_lookup][const_lookup][const_lookup][:max_lookup])\n",
    "        \n",
    "        \n",
    "        \n",
    "        sess.run(val_acc_reset_op)\n",
    "        sess.run(val_loss_reset_op)\n",
    "        sess.run(train_acc_reset_op)\n",
    "        sess.run(train_loss_reset_op)\n",
    "\n",
    "        # tensorboard writer\n",
    "        #batch_train_writer = tf.summary.FileWriter(os.path.join(logdir, \"cnn/batch/train/\"))\n",
    "        #batch_val_writer = tf.summary.FileWriter(os.path.join(logdir, \"cnn/batch/val/\"))\n",
    "        epoch_train_writer = tf.summary.FileWriter(os.path.join(logdir, \"cnn/epoch/train/\"))\n",
    "        epoch_val_writer = tf.summary.FileWriter(os.path.join(logdir, \"cnn/epoch/val/\"))\n",
    "        epoch_train_writer.add_graph(sess.graph)\n",
    "\n",
    "        low_i = 0\n",
    "        high_i = 0\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            step += 1\n",
    "            EPOCH_START_TIME = time.time()\n",
    "\n",
    "            # Shuffle the training set (data and label)\n",
    "            X_tr, y_tr = shuffle(X_tr, y_tr, random_state=42)\n",
    "\n",
    "            mini_step = 0\n",
    "            for iteration in range(len(X_tr) // batch_size):\n",
    "                mini_step += 1\n",
    "\n",
    "                low_i = iteration*batch_size\n",
    "                high_i = (iteration+1)*batch_size\n",
    "                X_batch_raw = X_tr[low_i:high_i]     \n",
    "                y_batch = y_tr[low_i:high_i]\n",
    "#                 print(mini_step, \": \", \"[\",low_i,\",\", high_i,\"]\",\"of\",len(X_batch_raw))\n",
    "\n",
    "                # augment batch\n",
    "                # TODO: look into factoring class size\n",
    "                X_batch_aug = np.zeros((X_batch_raw.shape[0], IMG_WIDTH, IMG_HEIGHT, 3), dtype='float32')\n",
    "                ind = 0\n",
    "                for img in X_batch_raw:\n",
    "                    if AUG_BOOL:\n",
    "#                         X_batch_aug[ind] = sess.run(prepared_image_op, feed_dict={input_image: img})\n",
    "                        X_batch_aug[ind] = augment_image(np.copy(img), lrf_p=LRF_P, \n",
    "                                                         upf_p=UPF_p, r_degree=ROT_DEG, \n",
    "                                                         roll_num=ROLL_NUM)\n",
    "                    else:\n",
    "                        X_batch_aug[ind] = np.copy(img)\n",
    "                    ind += 1\n",
    "\n",
    "                # shuffle batch after augmentation\n",
    "                #X_tr, y_tr = shuffle(X_batch_aug, y_batch, random_state=42)\n",
    "\n",
    "\n",
    "                feed_train = {X: X_batch_aug, \n",
    "                              y_raw: y_batch, \n",
    "                              training: True,\n",
    "                              learning_rate: LEARNING_RATE}\n",
    "\n",
    "                # training operation (back prop+)\n",
    "                sess.run(training_op, feed_dict=feed_train)\n",
    "                #step += 1\n",
    "\n",
    "                # update training metrics, acc and loss, (evaluation on current batch)\n",
    "                sess.run([train_tf_acc_op, train_auc_update, train_mean_loss_update], feed_dict={X: X_batch_aug, \n",
    "                                                                                                 y_raw: y_batch,\n",
    "                                                                                                 training: False})\n",
    "\n",
    "                # ================================================================================[validation]\n",
    "                if mini_step % 8 == 0:\n",
    "                         # ----- validation\n",
    "                    # run validation 3x w/ (lesser) augmentation\n",
    "                    for i in range(4):\n",
    "#                         X_val, y_val = shuffle(X_val, y_val, random_state=42)\n",
    "                        lower_i = 0\n",
    "                        higher_i = 0\n",
    "                        for vitter in range(len(X_val) // batch_size):\n",
    "                            lower_i = vitter*batch_size\n",
    "                            higher_i = (vitter+1)*batch_size\n",
    "                            X_val_batch = X_val[lower_i:higher_i]\n",
    "                            y_val_batch = y_val[lower_i:higher_i]\n",
    "\n",
    "                            # augment\n",
    "                            X_val_batch_aug = np.zeros((X_val_batch.shape[0], IMG_WIDTH, IMG_HEIGHT, 3), dtype='float32')\n",
    "                            ind = 0\n",
    "                            for img in X_val_batch:\n",
    "                                if VAL_AUG_BOOL:\n",
    "                                    # no normalization happens here\n",
    "                                    X_val_batch_aug[ind] = val_augment_image(np.copy(img), \n",
    "                                                                             lrf_p=VAL_LRF_P, \n",
    "                                                                             upf_p=VAL_UPF_P, \n",
    "                                                                             roll_num=VAL_ROLL_NUM)\n",
    "                                else:\n",
    "                                    X_val_batch_aug[ind] = np.copy(img)\n",
    "                                ind += 1\n",
    "\n",
    "                            sess.run([val_tf_acc_op, val_auc_update, val_mean_loss_update], \n",
    "                                     feed_dict={X: X_val_batch_aug, y_raw: y_val_batch, training: False})\n",
    "\n",
    "                        X_val_batch = X_val[higher_i:]\n",
    "                        y_val_batch = y_val[higher_i:]\n",
    "                        # not augmenting final validation batch *(which will be different each round\n",
    "                        # becuase of the shuffle)\n",
    "                        if len(X_batch_raw) > 0:\n",
    "                            sess.run([val_tf_acc_op, val_auc_update, val_mean_loss_update], \n",
    "                                     feed_dict={X: X_val_batch, y_raw: y_val_batch, training: False})\n",
    "            # ===============================================================================[validation end]\n",
    "            \n",
    "            # xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Train on remaining batch, if it exists \n",
    "            # metrics aren't calculated since the batch size could be really small\n",
    "            X_batch_raw = X_tr[high_i:]\n",
    "            y_batch = y_tr[high_i:]\n",
    "\n",
    "            if len(X_batch_raw) > 0:\n",
    "#                 print(len(X_batch_raw))\n",
    "                X_batch_augx = np.zeros((X_batch_raw.shape[0], IMG_WIDTH, IMG_HEIGHT, 3), dtype='float32')\n",
    "                ind = 0\n",
    "                for img in X_batch_raw:\n",
    "                    if AUG_BOOL:\n",
    "#                         X_batch_aug[ind] = sess.run(prepared_image_op, feed_dict={input_image: img})\n",
    "                        X_batch_augx[ind] = augment_image(np.copy(img), lrf_p=LRF_P, \n",
    "                                                          upf_p=UPF_p, r_degree=ROT_DEG, \n",
    "                                                          roll_num=ROLL_NUM)\n",
    "                    else:\n",
    "                        X_batch_augx[ind] = np.copy(img)\n",
    "                    ind += 1\n",
    "\n",
    "                feed_train = {X: X_batch_augx, \n",
    "                              y_raw: y_batch, \n",
    "                              training: True,\n",
    "                              learning_rate: LEARNING_RATE}\n",
    "\n",
    "                sess.run(training_op, feed_dict=feed_train)\n",
    "                #step += 1\n",
    "            # xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "\n",
    "            # ========================================================= Epoch metrics (log and print)\n",
    "\n",
    "            # ----- training\n",
    "            epoch_acc_print, epoch_auc_print, epoch_loss_print = sess.run([train_tf_acc, train_auc, train_mean_loss])\n",
    "\n",
    "            epoch_train_summary = sess.run(epoch_train_write_op)\n",
    "            epoch_train_writer.add_summary(epoch_train_summary, step)\n",
    "            epoch_train_writer.flush()\n",
    "\n",
    "            epoch_val_acc_print, epoch_val_auc_print, epoch_val_loss_print = sess.run([val_tf_acc, val_auc, val_mean_loss])\n",
    "\n",
    "            epoch_val_summary = sess.run(epoch_validation_write_op)\n",
    "            epoch_val_writer.add_summary(epoch_val_summary, step)\n",
    "            epoch_val_writer.flush()\n",
    "\n",
    "            print(\"E:{}tr: auc {:.2f}%|acc {:.2f}%|{:.4f} || val: auc {:.2f}%|acc {:.2f}%|{:.4f}\".format(epoch,\n",
    "                                                                                             epoch_auc_print*100,              \n",
    "                                                                                             epoch_acc_print*100, \n",
    "                                                                                             epoch_loss_print,\n",
    "                                                                                             epoch_val_auc_print*100,\n",
    "                                                                                             epoch_val_acc_print*100, \n",
    "                                                                                             epoch_val_loss_print))\n",
    "            #plot_color_image(X_batch_aug[0])\n",
    "            #plt.imshow(X_batch_aug[0].astype(np.uint8),interpolation=\"nearest\")\n",
    "            #plt.title(\"{}x{}\".format(image.shape[1], image.shape[0]))\n",
    "#             show_output(sess, 9, X_batch_aug, 3, \"RGB\")\n",
    "#             plt.axis(\"off\")\n",
    "            # ==========================================================================================\n",
    "\n",
    "            # save 'best' model (lowest epoch avg for validation loss)\n",
    "            if epoch_val_loss_print < best_loss_val:\n",
    "                best_loss_val = epoch_val_loss_print\n",
    "                checks_since_last_progress = 0\n",
    "                best_model_params = get_model_params()\n",
    "                print(\"Best params saved| val loss: {:.4f}\".format(best_loss_val))\n",
    "                \n",
    "            else:\n",
    "                checks_since_last_progress += 1\n",
    "            \n",
    "            if checks_since_last_progress >= 150:\n",
    "                print(\"Early Stopping\")\n",
    "                break\n",
    "\n",
    "            # estimate time remaining\n",
    "            EPOCH_END_TIME = time.time()\n",
    "            EPOCH_ELAPSED_TIME = EPOCH_END_TIME - EPOCH_START_TIME\n",
    "            EST_TIME_REMAINING = (((EPOCH_ELAPSED_TIME) * (NUM_EPOCHS - epoch)) / 60)\n",
    "            print(\"E time {:.1f} secs, Est.Time Remain: {:.1f} mins\".format(EPOCH_ELAPSED_TIME, EST_TIME_REMAINING))\n",
    "            print(\"-----------------------------\")\n",
    "\n",
    "            sess.run(val_acc_reset_op)\n",
    "            sess.run(val_loss_reset_op)\n",
    "            sess.run(train_acc_reset_op)\n",
    "            sess.run(train_loss_reset_op)\n",
    "\n",
    "        # done with training writers\n",
    "        #batch_train_writer.close()\n",
    "        #batch_val_writer.close()\n",
    "        epoch_train_writer.close()\n",
    "        epoch_val_writer.close()\n",
    "\n",
    "        # save session so we can re-enter at a later time\n",
    "        # restore best params\n",
    "        restore_model_params(best_model_params, g, sess)\n",
    "        print(\"best params restored\")\n",
    "        \n",
    "        saver_path = \"./corrected_saver/\" + NAME_STR + \"_\" + str(SQUARE_DIM) + \"_\" + \"FINAL\" + \"_448\"\n",
    "        save_path = saver.save(sess, saver_path)\n",
    "        save_obj(best_model_params, NAME_STR)\n",
    "        print(\"best params saved\")\n",
    "        \n",
    "        return g, best_model_params, saver\n",
    "\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size:  16\n",
      "last_shape:  8192\n",
      "num vars: 18 = frozen( 6 ) + train( 12 )\n",
      "model built.\n",
      "[ 0.04795146  0.06462063  0.04433362 -0.07806579  0.06958175]\n",
      "INFO:tensorflow:Restoring parameters from ../GAN/saver/GAN_Model_3.ckpt\n",
      "loaded 6 vars\n",
      "[-0.04778407  0.03699316  0.0329456   0.02024674 -0.04461705]\n",
      "E:0tr: auc 58.56%|acc 56.08%|0.6849 || val: auc 57.54%|acc 56.12%|0.6882\n",
      "Best params saved| val loss: 0.6882\n",
      "E time 27.4 secs, Est.Time Remain: 22.9 mins\n",
      "-----------------------------\n",
      "E:1tr: auc 60.54%|acc 58.08%|0.6771 || val: auc 60.77%|acc 59.21%|0.6794\n",
      "Best params saved| val loss: 0.6794\n",
      "E time 26.5 secs, Est.Time Remain: 21.7 mins\n",
      "-----------------------------\n",
      "E:2tr: auc 60.21%|acc 56.50%|0.6764 || val: auc 61.96%|acc 60.85%|0.6750\n",
      "Best params saved| val loss: 0.6750\n",
      "E time 26.6 secs, Est.Time Remain: 21.2 mins\n",
      "-----------------------------\n",
      "E:3tr: auc 62.74%|acc 59.42%|0.6687 || val: auc 62.99%|acc 61.60%|0.6708\n",
      "Best params saved| val loss: 0.6708\n",
      "E time 26.3 secs, Est.Time Remain: 20.6 mins\n",
      "-----------------------------\n",
      "E:4tr: auc 62.47%|acc 59.08%|0.6677 || val: auc 63.53%|acc 62.01%|0.6688\n",
      "Best params saved| val loss: 0.6688\n",
      "E time 27.2 secs, Est.Time Remain: 20.9 mins\n",
      "-----------------------------\n",
      "E:5tr: auc 62.92%|acc 59.25%|0.6673 || val: auc 64.26%|acc 62.32%|0.6663\n",
      "Best params saved| val loss: 0.6663\n",
      "E time 26.8 secs, Est.Time Remain: 20.1 mins\n",
      "-----------------------------\n",
      "E:6tr: auc 62.95%|acc 58.58%|0.6656 || val: auc 64.55%|acc 63.08%|0.6642\n",
      "Best params saved| val loss: 0.6642\n",
      "E time 26.7 secs, Est.Time Remain: 19.5 mins\n",
      "-----------------------------\n",
      "E:7tr: auc 63.72%|acc 60.00%|0.6628 || val: auc 64.42%|acc 62.83%|0.6636\n",
      "Best params saved| val loss: 0.6636\n",
      "E time 27.3 secs, Est.Time Remain: 19.6 mins\n",
      "-----------------------------\n",
      "E:8tr: auc 64.29%|acc 60.08%|0.6618 || val: auc 65.56%|acc 63.46%|0.6597\n",
      "Best params saved| val loss: 0.6597\n",
      "E time 27.0 secs, Est.Time Remain: 18.9 mins\n",
      "-----------------------------\n",
      "E:9tr: auc 64.49%|acc 59.67%|0.6600 || val: auc 66.30%|acc 64.09%|0.6565\n",
      "Best params saved| val loss: 0.6565\n",
      "E time 27.1 secs, Est.Time Remain: 18.5 mins\n",
      "-----------------------------\n",
      "E:10tr: auc 64.78%|acc 60.92%|0.6592 || val: auc 65.76%|acc 62.81%|0.6579\n",
      "E time 26.7 secs, Est.Time Remain: 17.8 mins\n",
      "-----------------------------\n",
      "E:11tr: auc 64.58%|acc 60.17%|0.6584 || val: auc 66.16%|acc 63.84%|0.6562\n",
      "Best params saved| val loss: 0.6562\n",
      "E time 27.0 secs, Est.Time Remain: 17.6 mins\n",
      "-----------------------------\n",
      "E:12tr: auc 65.81%|acc 60.58%|0.6548 || val: auc 66.84%|acc 63.92%|0.6534\n",
      "Best params saved| val loss: 0.6534\n",
      "E time 26.5 secs, Est.Time Remain: 16.8 mins\n",
      "-----------------------------\n",
      "E:13tr: auc 64.91%|acc 59.67%|0.6560 || val: auc 66.73%|acc 63.57%|0.6533\n",
      "Best params saved| val loss: 0.6533\n",
      "E time 27.0 secs, Est.Time Remain: 16.6 mins\n",
      "-----------------------------\n",
      "E:14tr: auc 65.67%|acc 60.50%|0.6534 || val: auc 67.19%|acc 64.15%|0.6512\n",
      "Best params saved| val loss: 0.6512\n",
      "E time 27.1 secs, Est.Time Remain: 16.3 mins\n",
      "-----------------------------\n",
      "E:15tr: auc 65.61%|acc 60.75%|0.6535 || val: auc 67.34%|acc 63.86%|0.6496\n",
      "Best params saved| val loss: 0.6496\n",
      "E time 26.5 secs, Est.Time Remain: 15.5 mins\n",
      "-----------------------------\n",
      "E:16tr: auc 66.42%|acc 61.42%|0.6501 || val: auc 67.93%|acc 64.06%|0.6473\n",
      "Best params saved| val loss: 0.6473\n",
      "E time 27.1 secs, Est.Time Remain: 15.4 mins\n",
      "-----------------------------\n",
      "E:17tr: auc 66.28%|acc 60.83%|0.6497 || val: auc 67.11%|acc 63.29%|0.6493\n",
      "E time 26.2 secs, Est.Time Remain: 14.4 mins\n",
      "-----------------------------\n",
      "E:18tr: auc 66.43%|acc 61.42%|0.6502 || val: auc 68.26%|acc 64.55%|0.6450\n",
      "Best params saved| val loss: 0.6450\n",
      "E time 27.1 secs, Est.Time Remain: 14.5 mins\n",
      "-----------------------------\n",
      "E:19tr: auc 66.35%|acc 61.50%|0.6494 || val: auc 67.72%|acc 64.05%|0.6465\n",
      "E time 27.3 secs, Est.Time Remain: 14.1 mins\n",
      "-----------------------------\n",
      "E:20tr: auc 65.60%|acc 60.25%|0.6512 || val: auc 68.39%|acc 64.29%|0.6433\n",
      "Best params saved| val loss: 0.6433\n",
      "E time 27.2 secs, Est.Time Remain: 13.6 mins\n",
      "-----------------------------\n",
      "E:21tr: auc 66.47%|acc 60.83%|0.6490 || val: auc 68.68%|acc 63.92%|0.6429\n",
      "Best params saved| val loss: 0.6429\n",
      "E time 27.4 secs, Est.Time Remain: 13.2 mins\n",
      "-----------------------------\n",
      "E:22tr: auc 66.82%|acc 61.75%|0.6461 || val: auc 68.01%|acc 63.88%|0.6439\n",
      "E time 27.0 secs, Est.Time Remain: 12.6 mins\n",
      "-----------------------------\n",
      "E:23tr: auc 67.67%|acc 62.58%|0.6441 || val: auc 68.23%|acc 63.60%|0.6428\n",
      "Best params saved| val loss: 0.6428\n",
      "E time 27.0 secs, Est.Time Remain: 12.1 mins\n",
      "-----------------------------\n",
      "E:24tr: auc 66.47%|acc 60.75%|0.6484 || val: auc 68.31%|acc 64.08%|0.6421\n",
      "Best params saved| val loss: 0.6421\n",
      "E time 26.6 secs, Est.Time Remain: 11.5 mins\n",
      "-----------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f332c372d405>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mname_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"discrim_class_2c_4f\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_that_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNAME_STR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mEND_TIME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-8514d7f3c675>\u001b[0m in \u001b[0;36mrun_that_model\u001b[0;34m(NAME_STR)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0;31m# training operation (back prop+)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m                 \u001b[0;31m#step += 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_edge/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_edge/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_edge/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_edge/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_edge/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "g = None\n",
    "bmp = None\n",
    "saver = None\n",
    "sess = None\n",
    "\n",
    "# run graph\n",
    "START_TIME = time.time()\n",
    "# 0\n",
    "# name_str = \"discrim_class_0c_0f\"\n",
    "# name_str = \"discrim_class_0c_4f\"\n",
    "# 1\n",
    "# name_str = \"discrim_class_1c_0f\"\n",
    "# name_str = \"discrim_class_1c_4f\"\n",
    "# 2\n",
    "# name_str = \"discrim_class_2c_0f\"\n",
    "name_str = \"discrim_class_2c_4f\"\n",
    "\n",
    "g, bmp, saver = run_that_model(NAME_STR=name_str)\n",
    "END_TIME = time.time()\n",
    "\n",
    "# time remaining metrics\n",
    "print(\"Time: {:.1f}min\".format((END_TIME - START_TIME)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_shape:  8192\n",
      "training all the vars\n",
      "model built.\n",
      "INFO:tensorflow:Restoring parameters from ./corrected_saver/discrim_class_2c_0f_224_FINAL\n",
      "BMP!!!!!\n",
      ">>>>>>>>>> test auc: 66.831% acc: 59.14 loss: 0.63048\n",
      "[[64 71]\n",
      " [34 88]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "NAME_STR = name_str\n",
    "g, saver = build_vgg(\"now\", NAME_STR)\n",
    "\n",
    "saver_path = \"./corrected_saver/\" + NAME_STR + \"_\" + str(SQUARE_DIM) + \"_\" + \"FINAL\"\n",
    "sess = tf.Session(graph=g)\n",
    "saver.restore(sess, saver_path)\n",
    "\n",
    "training_op, X, y_raw, training, learning_rate = g.get_collection(\"main_ops\")\n",
    "train_tf_acc, train_tf_acc_op, train_acc_reset_op, train_auc, train_auc_update, val_tf_acc, val_tf_acc_op, val_acc_reset_op, val_auc, val_auc_update, test_tf_acc, test_tf_acc_op, test_acc_reset_op, test_auc, test_auc_update = g.get_collection(\"acc_metrics\")\n",
    "train_y_true_cls, train_y_pred_cls = g.get_collection(\"test_metrics\")\n",
    "train_mean_loss, train_mean_loss_update, train_loss_reset_op, val_mean_loss, val_mean_loss_update, val_loss_reset_op, test_mean_loss, test_mean_loss_update, test_loss_reset_op = g.get_collection(\"loss_metrics\")\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "# todo: there is a better way to implement the confusion matrix\n",
    "# see here; https://stackoverflow.com/questions/41617463/tensorflow-confusion-matrix-in-tensorboard#42857070\n",
    "\n",
    "with sess:\n",
    "    confusion_mat = tf.Variable( tf.zeros([num_classes,num_classes], dtype=tf.int32 ), name='confusion')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # restore to the best parameters to calculate test metrics\n",
    "    BMPs = load_obj(NAME_STR)\n",
    "    if BMPs:\n",
    "        print(\"BMP!!!!!\")\n",
    "        restore_model_params(BMPs, g, sess)\n",
    "\n",
    "    sess.run(test_acc_reset_op)\n",
    "    sess.run(test_loss_reset_op)\n",
    "    \n",
    "    for iteration in range(len(X_test) // batch_size):\n",
    "        low_i = iteration*batch_size\n",
    "        high_i = (iteration+1)*batch_size\n",
    "        X_test_batch = X_test[low_i:high_i]\n",
    "        y_test_batch = y_test[low_i:high_i]\n",
    "        sess.run([test_tf_acc_op, test_mean_loss_update, test_auc_update], \n",
    "                 feed_dict={X: X_test_batch, y_raw: y_test_batch, training: False})\n",
    "        jj = train_y_true_cls.eval(feed_dict={y_raw: y_test_batch})\n",
    "        batch_conf_matrix = tf.confusion_matrix(labels = train_y_true_cls.eval(feed_dict={y_raw: y_test_batch}).reshape(-1),\n",
    "                                                predictions = train_y_pred_cls.eval(feed_dict={X: X_test_batch}).reshape(-1),\n",
    "                                                num_classes=num_classes)\n",
    "        sess.run(confusion_mat.assign(confusion_mat.eval() + batch_conf_matrix.eval()))\n",
    "    \n",
    "    X_test_batch = X_test[high_i:]\n",
    "    y_test_batch = y_test[high_i:]\n",
    "    if len(X_test_batch) > 0:\n",
    "        sess.run([test_tf_acc_op, test_mean_loss_update, test_auc_update], \n",
    "                 feed_dict={X: X_test_batch, y_raw: y_test_batch, training: False})\n",
    "        batch_conf_matrix = tf.confusion_matrix(labels = train_y_true_cls.eval(feed_dict={y_raw: y_test_batch}).reshape(-1),\n",
    "                                                predictions = train_y_pred_cls.eval(feed_dict={X: X_test_batch}).reshape(-1),\n",
    "                                                num_classes=num_classes)\n",
    "        sess.run(confusion_mat.assign(confusion_mat.eval() + batch_conf_matrix.eval()))\n",
    "    final_test_acc, final_test_loss, final_test_auc = sess.run([test_tf_acc, test_mean_loss, test_auc])\n",
    "\n",
    "    print(\">>>>>>>>>> test auc: {:.3f}% acc: {:.2f} loss: {:.5f}\".format(final_test_auc*100, \n",
    "                                                                         final_test_acc*100,\n",
    "                                                                         final_test_loss))\n",
    "\n",
    "    print(confusion_mat.eval())\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num = 0\n",
    "# image = X_tr[num]\n",
    "# # with tf.Session() as sess:\n",
    "# #     image = sess.run(prepared_image_op, feed_dict={input_image: image})\n",
    "# image = augment_image(image=np.copy(image), lrf_p=0.5, max_zoom=0.4, r_degree=0.0, upf_p=0.0)\n",
    "# print(image[100][100])\n",
    "# label = y_tr[num]\n",
    "# print(label)\n",
    "# plt.imshow(image.astype(np.uint8),interpolation=\"nearest\")\n",
    "# plt.title(\"{}x{}\".format(image.shape[1], image.shape[0]))\n",
    "# plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
