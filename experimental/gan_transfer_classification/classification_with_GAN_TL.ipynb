{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: (3, 5, 4, 'final', 0)\n",
      "TensorFlow: 1.4.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this is a custom cell that contains the common imports I personally \n",
    "# use these may/may not be necessary for the following examples\n",
    "\n",
    "# DL framework\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# common packages\n",
    "import numpy as np\n",
    "import os # handling file i/o\n",
    "import sys\n",
    "import math\n",
    "import time # timing epochs\n",
    "import random\n",
    "\n",
    "# for ordered dict when building layer components\n",
    "import collections\n",
    "\n",
    "# plotting pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import colors # making colors consistent\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable # colorbar helper\n",
    "\n",
    "\n",
    "# from imageio import imread # read image from disk\n",
    "# + data augmentation\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "\n",
    "\n",
    "import pickle # manually saving best params\n",
    "from sklearn.utils import shuffle # shuffling data batches\n",
    "from tqdm import tqdm # display training progress bar\n",
    "\n",
    "# const\n",
    "SEED = 42\n",
    "\n",
    "# Helper to make the output consistent\n",
    "def reset_graph(seed=SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# helper to create dirs if they don't already exist\n",
    "def maybe_create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print(\"{} created\".format(dir_path))\n",
    "    else:\n",
    "        print(\"{} already exists\".format(dir_path))\n",
    "    \n",
    "def make_standard_dirs(saver=True, best_params=True, tf_logs=True):\n",
    "    # `saver/` will hold tf saver files\n",
    "    maybe_create_dir(\"saver\")\n",
    "    # `best_params/` will hold a serialized version of the best params\n",
    "    # I like to keep this as a backup in case I run into issues with\n",
    "    # the saver files\n",
    "    maybe_create_dir(\"best_params\")\n",
    "    # `tf_logs/` will hold the logs that will be visable in tensorboard\n",
    "    maybe_create_dir(\"tf_logs\")\n",
    "\n",
    "    \n",
    "# set tf log level to supress messages, unless an error\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Important Version information\n",
    "print(\"Python: {}\".format(sys.version_info[:]))\n",
    "print('TensorFlow: {}'.format(tf.__version__))\n",
    "\n",
    "# Check if using GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "    \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saver already exists\n",
      "best_params already exists\n",
      "tf_logs already exists\n"
     ]
    }
   ],
   "source": [
    "make_standard_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.tfrecords\n",
      "train.tfrecords\n",
      "val.tfrecords\n"
     ]
    }
   ],
   "source": [
    "FINAL_DIR = \"../../dataset/record_holder/lesion/balanced/\"\n",
    "for _, _, files in os.walk(FINAL_DIR):\n",
    "    files = sorted(files)\n",
    "    for filename in files:\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these two functions (get_model_params and restore_model_params) are \n",
    "# ad[a|o]pted from; \n",
    "# https://github.com/ageron/handson-ml/blob/master/11_deep_learning.ipynb\n",
    "def get_model_params():\n",
    "    global_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    return {global_vars.op.name: value for global_vars, value in \n",
    "            zip(global_vars, tf.get_default_session().run(global_vars))}\n",
    "\n",
    "def restore_model_params(model_params, g, sess):\n",
    "    gvar_names = list(model_params.keys())\n",
    "    assign_ops = {gvar_name: g.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                  for gvar_name in gvar_names}\n",
    "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "# these two functions are used to manually save the best\n",
    "# model params to disk\n",
    "def save_obj(obj, name):\n",
    "    with open('./best_params/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('./best_params/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_SET_TYPE = None\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    global GLOBAL_SET_TYPE\n",
    "    labelName = str(GLOBAL_SET_TYPE) + '/label'\n",
    "    featureName = str(GLOBAL_SET_TYPE) + '/image'\n",
    "    feature = {featureName: tf.FixedLenFeature([], tf.string),\n",
    "               labelName: tf.FixedLenFeature([], tf.int64)}\n",
    "    \n",
    "    # decode\n",
    "    parsed_features = tf.parse_single_example(example_proto, features=feature)\n",
    "    \n",
    "    # convert image data from string to number\n",
    "    image = tf.decode_raw(parsed_features[featureName], tf.float32)\n",
    "    image = tf.reshape(image, [224, 224, 3])\n",
    "    \n",
    "    label = tf.cast(parsed_features[labelName], tf.int64)\n",
    "    \n",
    "    image /= 255\n",
    "    image -= 0.5\n",
    "    image *= 2\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # validation\n",
    "# X_val /= 255\n",
    "# X_val -= 0.5\n",
    "# X_val *= 2\n",
    "\n",
    "\n",
    "# # # test\n",
    "# X_test /= 255\n",
    "# X_test -= 0.5\n",
    "# X_test *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_batched_iter(setType, data_params, sess):\n",
    "    global GLOBAL_SET_TYPE\n",
    "    global FINAL_DIR\n",
    "    GLOBAL_SET_TYPE = setType\n",
    "    \n",
    "    filenames_ph = tf.placeholder(tf.string, shape=[None])\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames_ph)\n",
    "    dataset = dataset.map(_parse_function)  # Parse the record into tensors.\n",
    "    if GLOBAL_SET_TYPE != 'test':\n",
    "        dataset = dataset.shuffle(buffer_size=data_params['buffer_size'])\n",
    "    #dataset = dataset.shuffle(buffer_size=1)\n",
    "    dataset = dataset.batch(data_params['batch_size'])\n",
    "    dataset = dataset.repeat(1)\n",
    "    \n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    \n",
    "    tfrecords_file_name = str(GLOBAL_SET_TYPE) + '.tfrecords'\n",
    "    tfrecord_file_path = os.path.join(FINAL_DIR, tfrecords_file_name)\n",
    "    \n",
    "    # initialize\n",
    "    sess.run(iterator.initializer, feed_dict={filenames_ph: [tfrecord_file_path]})\n",
    "    \n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hyper_params():\n",
    "    data_params = {}\n",
    "    data_params['n_epochs'] = 50\n",
    "    data_params['batch_size'] = 32\n",
    "    data_params['buffer_size'] = 128 # for shuffling\n",
    "    data_params['n_outputs'] = 1\n",
    "    data_params['num_classes'] = 2\n",
    "    \n",
    "    data_params['path_to_GAN_ckpt'] = \"./saved_gan_params/GAN_Model_3.ckpt\"\n",
    "    \n",
    "    data_params['name_str'] = \"0\"\n",
    "    data_params[\"LOADFREEZE_STR\"] = \"discriminator/Gconv_[\"+data_params[\"name_str\"]+\"]\"\n",
    "    data_params[\"BEST_PARAMS_PATH\"] = \"./best_params/\" + data_params[\"name_str\"]\n",
    "\n",
    "    data_params[\"init_lr\"] = 1e-5\n",
    "\n",
    "    return data_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this function is adapted from https://github.com/ageron/handson-ml/blob/master/13_convolutional_neural_networks.ipynb\n",
    "# def val_augment_image(image, lrf_p=0.0, upf_p=0.0, roll_num=0):\n",
    "        \n",
    "#     n_image = image\n",
    "\n",
    "#     # Flip the image horizontally with upf_p% probability:\n",
    "#     if np.random.rand() < lrf_p:\n",
    "#         n_image = np.fliplr(n_image)\n",
    "\n",
    "#     # Flip the image vertically with lrf_p% probability:\n",
    "#     if np.random.rand() < upf_p:\n",
    "#         n_image = np.flipud(n_image)\n",
    "        \n",
    "#     # 50% chance roll image horiz\n",
    "#     if np.random.rand() < 0.5:\n",
    "#         val = int(np.random.rand() * roll_num)\n",
    "#         # 50% left or right\n",
    "#         if np.random.rand() < 0.5:\n",
    "#             n_image = np.roll(n_image, val, axis=1)\n",
    "#         else:\n",
    "#             n_image = np.roll(n_image, -val, axis=1)\n",
    "        \n",
    "#     # 50% chance roll image vert\n",
    "#     if np.random.rand() < 0.5:\n",
    "#         val = int(np.random.rand() * roll_num)\n",
    "#         # 50% left or right\n",
    "#         if np.random.rand() < 0.5:\n",
    "#             n_image = np.roll(n_image, val, axis=0)\n",
    "#         else:\n",
    "#             n_image = np.roll(n_image, -val, axis=0)\n",
    "\n",
    "#     # ensure float32\n",
    "#     n_image = n_image.astype(np.float32)\n",
    "\n",
    "#     return n_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this function is adapted from https://github.com/ageron/handson-ml/blob/master/13_convolutional_neural_networks.ipynb\n",
    "def augment_image(image, lrf_p=0.0, upf_p=0.0, r_degree=0.0, max_zoom=0.0, roll_num=0):\n",
    "        \n",
    "    n_image = image\n",
    "    \n",
    "    # Flip the image horizontally with upf_p% probability:\n",
    "    if np.random.rand() < lrf_p:\n",
    "        n_image = np.fliplr(n_image)\n",
    "\n",
    "    # Flip the image vertically with lrf_p% probability:\n",
    "    if np.random.rand() < upf_p:\n",
    "        n_image = np.flipud(n_image)\n",
    "\n",
    "    # 50% chance roll image horiz\n",
    "    if np.random.rand() < 0.5:\n",
    "        val = int(np.random.rand() * roll_num)\n",
    "        # 50% left or right\n",
    "        if np.random.rand() < 0.5:\n",
    "            n_image = np.roll(n_image, val, axis=1)\n",
    "        else:\n",
    "            n_image = np.roll(n_image, -val, axis=1)\n",
    "        \n",
    "    # 50% chance roll image vert\n",
    "    if np.random.rand() < 0.5:\n",
    "        val = int(np.random.rand() * roll_num)\n",
    "        # 50% left or right\n",
    "        if np.random.rand() < 0.5:\n",
    "            n_image = np.roll(n_image, val, axis=0)\n",
    "        else:\n",
    "            n_image = np.roll(n_image, -val, axis=0)\n",
    "\n",
    "    \n",
    "#     if r_degree > 0.0:\n",
    "#         r_degree *= np.random.rand()\n",
    "#         # randomly choose rotation direction\n",
    "#         r_dir = (1 if np.random.rand() > 0.5 else -1)\n",
    "#         n_image = ndimage.interpolation.rotate(n_image, r_dir*r_degree, reshape=False)\n",
    "    \n",
    "\n",
    "    return n_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2_regularizer = tf.contrib.layers.l2_regularizer(0.01)\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(data_params):\n",
    "    \n",
    "    reset_graph()\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        IMG_HEIGHT, IMG_WIDTH, CHANNELS = 224,224,3\n",
    "\n",
    "        with tf.name_scope(\"inputs\"):\n",
    "            X = tf.placeholder(tf.float32, shape=[None, IMG_HEIGHT, IMG_WIDTH, CHANNELS], name=\"X\")\n",
    "            y_raw = tf.placeholder(tf.int64, shape=[None, data_params[\"n_outputs\"]], name=\"y_input\")\n",
    "            y = tf.cast(y_raw, tf.float32, name=\"label\")\n",
    "\n",
    "            # for training/evaluating\n",
    "            training = tf.placeholder_with_default(False, shape=[], name=\"training\")\n",
    "\n",
    "        \n",
    "        with tf.name_scope(\"gan\"):\n",
    "            # 224x224\n",
    "            conv1_1 = tf.layers.conv2d(inputs=X, filters=64, kernel_size=3,\n",
    "                                   strides=2, padding=\"SAME\",\n",
    "                                   activation=tf.nn.elu, \n",
    "                                   kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                   bias_initializer=tf.zeros_initializer(), trainable=True,\n",
    "                                   name=\"discriminator/Gconv_1\")\n",
    "\n",
    "            # 112x112\n",
    "            conv1_2 = tf.layers.conv2d(conv1_1, 128, kernel_size=3, strides=2, padding='same', \n",
    "                                   activation=tf.nn.elu,\n",
    "                                   kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                   bias_initializer=tf.zeros_initializer(), trainable=True,\n",
    "                                   name=\"discriminator/Gconv_2\")\n",
    "\n",
    "            # 56x56\n",
    "            conv2_1 = tf.layers.conv2d(conv1_2, 256, kernel_size=3, strides=2, padding='same', \n",
    "                                               activation=tf.nn.elu,\n",
    "                                               kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                               bias_initializer=tf.zeros_initializer(), trainable=True,\n",
    "                                               name=\"discriminator/Gconv_3\")\n",
    "            \n",
    "#             \n",
    "\n",
    "            # 28x28x256\n",
    "            conv2_2 = tf.layers.conv2d(conv2_1, 512, kernel_size=3, strides=2, padding='same', \n",
    "                                   activation=tf.nn.elu,\n",
    "                                   kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                   bias_initializer=tf.zeros_initializer(), trainable=True,\n",
    "                                   name=\"discriminator/Gconv_4\")\n",
    "#             \n",
    "            # 14x14x512\n",
    "            conv2_3 = tf.layers.conv2d(conv2_2, 768, kernel_size=3, strides=1, padding='same', \n",
    "                                   activation=tf.nn.elu,\n",
    "                                   kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                                   bias_initializer=tf.zeros_initializer(), trainable=True,\n",
    "                                   name=\"discriminator/my_conv\")\n",
    "            conv2_2 = tf.layers.max_pooling2d(inputs=conv2_3, pool_size=[2,2], strides=2, name=\"max_pool_myc\")\n",
    "            #7x7x768\n",
    "        \n",
    "            # reshape\n",
    "            last_shape = int(np.prod(conv2_2.get_shape()[1:]))\n",
    "            print(\"last_shape: \", last_shape)\n",
    "            pool_flat = tf.reshape(conv2_2, shape=[-1, last_shape])\n",
    "            pool_flat_drop = tf.layers.dropout(pool_flat, 0.5, training=training, name=\"entry_drop\")\n",
    "\n",
    "            fc1 = tf.layers.dense(pool_flat_drop, 256, kernel_initializer=he_init, activation=tf.nn.elu, name=\"fc1\")\n",
    "            fc1_drop = tf.layers.dropout(fc1, 0.5, training=training, name=\"fc1_drop\")\n",
    "            \n",
    "            fc2 = tf.layers.dense(fc1_drop, 64, kernel_initializer=he_init, activation=tf.nn.elu, name=\"fc2\")\n",
    "            \n",
    "            logits = tf.layers.dense(fc2, data_params[\"n_outputs\"], name=\"logits\")\n",
    "            preds = tf.sigmoid(logits, name=\"preds\")\n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "            batch_loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "        with tf.name_scope(\"train\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=data_params[\"init_lr\"],\n",
    "                                               beta1=0.9,\n",
    "                                               beta2=0.999,\n",
    "                                               epsilon=1e-08,\n",
    "                                               use_locking=False,\n",
    "                                               name='Adam')\n",
    "            \n",
    "            all_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "            freeze_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=data_params[\"LOADFREEZE_STR\"])\n",
    "            train_vars = [var for var in all_vars if var not in freeze_vars]\n",
    "            print(\"num vars:\", len(all_vars), \"= frozen(\", len(freeze_vars), \")\", \"+ train(\", len(train_vars), \")\")\n",
    "\n",
    "            training_op = optimizer.minimize(batch_loss, var_list=train_vars, name=\"training_op\")\n",
    "\n",
    "        with tf.name_scope(\"save_session\"):\n",
    "            init_global = tf.global_variables_initializer()\n",
    "            init_local = tf.local_variables_initializer()\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "        # Ops: training metrics\n",
    "        with tf.name_scope(\"metrics\"):\n",
    "            # ================================== performance\n",
    "            with tf.name_scope(\"common\"):\n",
    "                #preds = tf.nn.softmax(logits, name=\"prediction\")\n",
    "                #y_true_cls = tf.argmax(y,1)\n",
    "                #y_pred_cls = tf.argmax(preds,1)\n",
    "                y_true_cls = tf.greater_equal(y, 0.5)\n",
    "                y_pred_cls = tf.greater_equal(preds, 0.5)\n",
    "\n",
    "                correct_prediction = tf.equal(y_pred_cls, y_true_cls, name=\"correct_predictions\")\n",
    "                batch_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            with tf.name_scope(\"train_metrics\") as scope:\n",
    "                train_auc, train_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                train_acc, train_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                train_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                train_met_reset_op = tf.variables_initializer(train_acc_vars, name=\"train_met_reset_op\")\n",
    "            with tf.name_scope(\"val_metrics\") as scope:\n",
    "                val_auc, val_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                val_acc, val_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                val_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                val_met_reset_op = tf.variables_initializer(val_acc_vars, name=\"val_met_reset_op\")\n",
    "            with tf.name_scope(\"test_metrics\") as scope:\n",
    "                test_auc, test_auc_update = tf.metrics.auc(labels=y, predictions=preds)\n",
    "                test_acc, test_acc_update = tf.metrics.accuracy(labels=y_true_cls, predictions=y_pred_cls)\n",
    "                test_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                test_acc_reset_op = tf.variables_initializer(test_acc_vars, name=\"test_met_reset_op\")\n",
    "\n",
    "            # =============================================== loss \n",
    "            with tf.name_scope(\"train_loss_eval\") as scope:\n",
    "                train_mean_loss, train_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                train_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                train_loss_reset_op = tf.variables_initializer(train_loss_vars, name=\"train_loss_reset_op\")\n",
    "            with tf.name_scope(\"val_loss_eval\") as scope:\n",
    "                val_mean_loss, val_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                val_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                val_loss_reset_op = tf.variables_initializer(val_loss_vars, name=\"val_loss_reset_op\")\n",
    "            with tf.name_scope(\"test_loss_eval\")as scope:\n",
    "                test_mean_loss, test_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "                test_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "                test_loss_reset_op = tf.variables_initializer(test_loss_vars, name=\"test_loss_rest_op\")\n",
    "\n",
    "        # --- create collections\n",
    "        for node in (saver, init_global, init_local):\n",
    "            g.add_to_collection(\"save_init\", node)\n",
    "        for node in (X, y_raw, training_op):\n",
    "            g.add_to_collection(\"main_ops\", node)\n",
    "        for node in (preds, y_true_cls, y_pred_cls, correct_prediction):\n",
    "            g.add_to_collection(\"preds\", node)\n",
    "        for node in (train_auc, train_auc_update, train_acc, train_acc_update, train_met_reset_op):\n",
    "            g.add_to_collection(\"train_metrics\", node)\n",
    "        for node in (val_auc, val_auc_update, val_acc, val_acc_update, val_met_reset_op):\n",
    "            g.add_to_collection(\"val_metrics\", node)\n",
    "        for node in (test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op):\n",
    "            g.add_to_collection(\"test_metrics\", node)\n",
    "        for node in (train_mean_loss, train_mean_loss_update, train_loss_reset_op):\n",
    "            g.add_to_collection(\"train_loss\", node)\n",
    "        for node in (val_mean_loss, val_mean_loss_update, val_loss_reset_op):\n",
    "            g.add_to_collection(\"val_loss\", node)\n",
    "        for node in (test_mean_loss, test_mean_loss_update, test_loss_reset_op):\n",
    "            g.add_to_collection(\"test_loss\", node)\n",
    "        g.add_to_collection(\"logits\", logits)\n",
    "            \n",
    "        # ===================================== tensorboard\n",
    "        with tf.name_scope(\"tensorboard_writer\") as scope:\n",
    "            epoch_train_loss_scalar = tf.summary.scalar('train_epoch_loss', train_mean_loss)\n",
    "            epoch_train_acc_scalar = tf.summary.scalar('train_epoch_acc', train_acc)\n",
    "            epoch_train_auc_scalar = tf.summary.scalar('train_epoch_auc', train_auc)\n",
    "            epoch_train_write_op = tf.summary.merge([epoch_train_loss_scalar, epoch_train_acc_scalar, epoch_train_auc_scalar], name=\"epoch_train_write_op\")\n",
    "\n",
    "            # ===== epoch, validation\n",
    "            epoch_validation_loss_scalar = tf.summary.scalar('validation_epoch_loss', val_mean_loss)\n",
    "            epoch_validation_acc_scalar = tf.summary.scalar('validation_epoch_acc', val_acc)\n",
    "            epoch_validation_auc_scalar = tf.summary.scalar('validation_epoch_auc', val_auc)\n",
    "            epoch_validation_write_op = tf.summary.merge([epoch_validation_loss_scalar, epoch_validation_acc_scalar, epoch_validation_auc_scalar], name=\"epoch_validation_write_op\")\n",
    "        \n",
    "        for node in (epoch_train_write_op, epoch_validation_write_op):\n",
    "            g.add_to_collection(\"tensorboard\", node)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "def train_graph(g, data_params):\n",
    "    \n",
    "    #####################################\n",
    "    # ----------- augmentation\n",
    "    ##### training\n",
    "    AUG_BOOL = True\n",
    "    LRF_P = 0.5\n",
    "    UPF_p = 0.5\n",
    "    MAX_ZOOM = 0.0\n",
    "    ROLL_NUM = 25\n",
    "    ROT_DEG = 0.0\n",
    "    \n",
    "    ##### validation\n",
    "    VAL_AUG_BOOL = True\n",
    "    VAL_LRF_P = 0.3\n",
    "    VAL_UPF_P = 0.3\n",
    "    VAL_ROLL_NUM = 20\n",
    "    ######################################\n",
    "    \n",
    "    saver, init_global, init_local = g.get_collection(\"save_init\")\n",
    "    X, y_raw, training_op = g.get_collection(\"main_ops\")\n",
    "    preds, y_true_cls, y_pred_cls, _ = g.get_collection(\"preds\")\n",
    "    train_auc, train_auc_update, train_acc, train_acc_update, train_met_reset_op = g.get_collection(\"train_metrics\")\n",
    "    val_auc, val_auc_update, val_acc, val_acc_update, val_met_reset_op = g.get_collection(\"val_metrics\")\n",
    "    train_mean_loss, train_mean_loss_update, train_loss_reset_op = g.get_collection(\"train_loss\")\n",
    "    val_mean_loss, val_mean_loss_update, val_loss_reset_op = g.get_collection(\"val_loss\")\n",
    "    epoch_train_write_op, epoch_validation_write_op = g.get_collection(\"tensorboard\")\n",
    "#     next_tr_element, next_val_element, _ = g.get_collection(\"data_sets\")\n",
    "\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(\"tf_logs\",\"train\"))\n",
    "    val_writer = tf.summary.FileWriter(os.path.join(\"tf_logs\",\"validation\"))\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    with tf.Session(graph=g) as sess:\n",
    "        \n",
    "        sess.run([init_global, init_local])\n",
    "        \n",
    "        #####################################################################\n",
    "        ## Load GAN params\n",
    "        const_lookup = 0\n",
    "        max_lookup = 5\n",
    "        conv1_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                               scope=\"discriminator/Gconv_1\")\n",
    "        conv1_init = sess.run(conv1_vars)\n",
    "        print(conv1_init[const_lookup][const_lookup][const_lookup][const_lookup][:max_lookup])\n",
    "        \n",
    "        # Load vars according to \"LOADFREEZE_STR\"\n",
    "        reuse_vars = g.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                           scope=data_params[\"LOADFREEZE_STR\"]) \n",
    "        reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "        if data_params['name_str'] != \"0\":\n",
    "            restore_saver = tf.train.Saver(reuse_vars_dict)\n",
    "            restore_saver.restore(sess, data_params['path_to_GAN_ckpt'])\n",
    "        conv1_restore = sess.run(conv1_vars)\n",
    "        \n",
    "        # ensure vars are loaded correctly\n",
    "        print(\"loaded {} vars\".format(len(reuse_vars_dict)))\n",
    "        print(conv1_restore[const_lookup][const_lookup][const_lookup][const_lookup][:max_lookup])\n",
    "        #########################################################################\n",
    "\n",
    "        for e in tqdm(range(1,data_params['n_epochs']+1)):\n",
    "            sess.run([val_met_reset_op,val_loss_reset_op,train_met_reset_op,train_loss_reset_op])\n",
    "            # training\n",
    "            tr_iter = return_batched_iter('train', data_params, sess)\n",
    "            next_tr_element = tr_iter.get_next()\n",
    "            \n",
    "            # loop entire training set\n",
    "            while True:\n",
    "                try:\n",
    "                    data, target = sess.run(next_tr_element)\n",
    "                    data_a = [augment_image(np.copy(img), lrf_p=LRF_P, \n",
    "                                          upf_p=UPF_p, r_degree=ROT_DEG, \n",
    "                                          roll_num=ROLL_NUM) for img in data]\n",
    "                    target = np.reshape(target, (target.shape[0], 1))\n",
    "                    sess.run([training_op, train_auc_update, train_acc_update, train_mean_loss_update], \n",
    "                             feed_dict={X:data_a, y_raw:target})\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "        \n",
    "            # write average for epoch\n",
    "            summary = sess.run(epoch_train_write_op)    \n",
    "            train_writer.add_summary(summary, e)\n",
    "            train_writer.flush()\n",
    "\n",
    "            # run validation\n",
    "            val_iter = return_batched_iter('val', data_params, sess)\n",
    "            next_val_element = val_iter.get_next()\n",
    "            while True:\n",
    "                try:\n",
    "                    Xb, yb = sess.run(next_val_element)\n",
    "                    Xb_a = [augment_image(np.copy(img), lrf_p=LRF_P, \n",
    "                                          upf_p=UPF_p, r_degree=ROT_DEG, \n",
    "                                          roll_num=ROLL_NUM) for img in Xb]\n",
    "                    yb = np.reshape(yb, (yb.shape[0], 1))\n",
    "                    sess.run([val_auc_update, val_acc_update, val_mean_loss_update], \n",
    "                             feed_dict={X:Xb_a, y_raw:yb})\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "\n",
    "            # check for (and save) best validation params here\n",
    "            cur_loss, cur_acc = sess.run([val_mean_loss, val_acc])\n",
    "            if cur_loss < best_val_loss:\n",
    "                best_val_loss = cur_loss\n",
    "                best_params = get_model_params()\n",
    "                save_obj(best_params, data_params['name_str'])\n",
    "                print(\"best params saved: val acc: {:.3f}% val loss: {:.4f}\".format(cur_acc*100, cur_loss))\n",
    "\n",
    "            summary = sess.run(epoch_validation_write_op) \n",
    "            val_writer.add_summary(summary, e)\n",
    "            val_writer.flush()\n",
    "        \n",
    "        train_writer.close()\n",
    "        val_writer.close()\n",
    "    return sess\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BEST_PARAMS_PATH': './best_params/0', 'name_str': '0', 'n_epochs': 50, 'LOADFREEZE_STR': 'discriminator/Gconv_[0]', 'path_to_GAN_ckpt': './saved_gan_params/GAN_Model_3.ckpt', 'num_classes': 2, 'n_outputs': 1, 'buffer_size': 128, 'init_lr': 1e-05, 'batch_size': 32}\n",
      "discriminator/Gconv_[0]\n",
      "last_shape:  37632\n",
      "num vars: 16 = frozen( 0 ) + train( 16 )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0432491   0.09504201 -0.04746214  0.0778892  -0.07586567]\n",
      "loaded 0 vars\n",
      "[-0.0432491   0.09504201 -0.04746214  0.0778892  -0.07586567]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 1/50 [00:05<04:44,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 58.182% val loss: 0.6796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:18<03:34,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 56.727% val loss: 0.6741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 5/50 [00:23<03:29,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 56.727% val loss: 0.6691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [01:10<02:29,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 60.364% val loss: 0.6673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [01:24<02:17,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 59.636% val loss: 0.6589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [02:37<01:07,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 62.909% val loss: 0.6560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [03:01<00:45,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 57.455% val loss: 0.6558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [03:16<00:31,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 59.273% val loss: 0.6558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [03:25<00:22,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params saved: val acc: 60.727% val loss: 0.6546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:49<00:00,  4.59s/it]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "data_params = create_hyper_params()\n",
    "print(data_params)\n",
    "print(data_params[\"LOADFREEZE_STR\"])\n",
    "g = build_graph(data_params)\n",
    "sess = train_graph(g, data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     # --- save best params\n",
    "#     best_model_params = None\n",
    "\n",
    "#     # Tensorboard\n",
    "#     now = datetime.now().strftime(\"%d%b%Y_%H%M%S\")\n",
    "#     root_logdir = \"tf_logs/final_discrim_class/\" + NAME_STR + \"_3\"\n",
    "#     logdir = \"{}/{}/\".format(root_logdir, now)\n",
    "\n",
    "#     X_batch_aug[ind] = augment_image(np.copy(img), lrf_p=LRF_P, \n",
    "#                                      upf_p=UPF_p, r_degree=ROT_DEG, \n",
    "#                                      roll_num=ROLL_NUM)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_shape:  37632\n",
      "num vars: 16 = frozen( 0 ) + train( 16 )\n",
      "test auc: 70.161% acc: 64.244% loss: 0.62703\n",
      "[[ 91  81]\n",
      " [ 42 130]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "data_params = create_hyper_params()\n",
    "g2 = build_graph(data_params)\n",
    "best_params = load_obj(data_params['name_str'])\n",
    "with tf.Session(graph=g2) as sess:\n",
    "    confusion_mat = tf.Variable( tf.zeros([data_params[\"num_classes\"],data_params[\"num_classes\"]], dtype=tf.int32 ), name='confusion')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver, init_global, init_local = g2.get_collection(\"save_init\")\n",
    "    X, y_raw, training_op = g2.get_collection(\"main_ops\")\n",
    "    preds, y_true_cls, y_pred_cls, _ = g2.get_collection(\"preds\")\n",
    "    test_auc, test_auc_update, test_acc, test_acc_update, test_acc_reset_op = g2.get_collection(\"test_metrics\")\n",
    "    test_mean_loss, test_mean_loss_update, test_loss_reset_op = g2.get_collection(\"test_loss\")\n",
    "    \n",
    "    restore_model_params(model_params=best_params, g=g2, sess=sess)\n",
    "    sess.run([test_acc_reset_op, test_loss_reset_op])\n",
    "    \n",
    "    test_iter = return_batched_iter('test', data_params, sess)\n",
    "    next_test_element = test_iter.get_next()\n",
    "    while True:\n",
    "        try:\n",
    "            Xb, yb = sess.run(next_test_element)\n",
    "            yb = np.reshape(yb, (yb.shape[0], 1))\n",
    "            sess.run([test_auc_update, test_acc_update, test_mean_loss_update], feed_dict={X:Xb, y_raw:yb})\n",
    "            jj = y_true_cls.eval(feed_dict={y_raw: yb})\n",
    "            batch_conf_matrix = tf.confusion_matrix(labels = y_true_cls.eval(feed_dict={y_raw: yb}).reshape(-1),\n",
    "                                                predictions = y_pred_cls.eval(feed_dict={X: Xb}).reshape(-1),\n",
    "                                                num_classes=data_params[\"num_classes\"])\n",
    "            sess.run(confusion_mat.assign(confusion_mat.eval() + batch_conf_matrix.eval()))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break    \n",
    "    \n",
    "    # print\n",
    "    final_test_acc, final_test_loss, final_test_auc = sess.run([test_acc, test_mean_loss, test_auc])\n",
    "    print(\"test auc: {:.3f}% acc: {:.3f}% loss: {:.5f}\".format(final_test_auc*100, \n",
    "                                                              final_test_acc*100,\n",
    "                                                              final_test_loss))\n",
    "    print(confusion_mat.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0      [[ 91  81]   test auc: 70.161% acc: 64.244% loss: 0.62703\n",
    "        [ 42 130]]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_edge",
   "language": "python",
   "name": "tf_edge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
