{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental: progressively growing network\n",
    "This architecture is based on an idea I had about \"growing\" a network -- one that first learns the low level features and \"grows\" deeper in hopes to better seperate on the distinguishing features.  This file needs a lot of updating (dataset, augmentation methods - switch to tf), and internal organization, BUT the skeleton is present.\n",
    "\n",
    "Eventually, I would love to include more ideas and have the network become more \"automatic\" in that it chooses which layers should be added next -- I'd like to do this by having it evaluate itself after each iteration -- maybe on metrics like the slope of the previous layers loss / performance metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: (3, 5, 4, 'final', 0)\n",
      "TensorFlow: 1.6.0-dev20180105\n",
      "No GPU\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this is a custom cell that contains the common imports I personally \n",
    "# use these may/may not be necessary for the following examples\n",
    "\n",
    "# DL framework\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# common packages\n",
    "import numpy as np\n",
    "import os # handling file i/o\n",
    "import sys\n",
    "import math\n",
    "import time # timing epochs\n",
    "\n",
    "# for ordered dict when building layer components\n",
    "import collections\n",
    "\n",
    "# plotting pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import colors # making colors consistent\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable # colorbar helper\n",
    "\n",
    "# read image\n",
    "from imageio import imread\n",
    "# + data augmentation\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "\n",
    "# used for manually saving best params\n",
    "import pickle\n",
    "\n",
    "# for shuffling data batches\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# const\n",
    "SEED = 42\n",
    "\n",
    "# Helper to make the output consistent\n",
    "def reset_graph(seed=SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# helper to create dirs if they don't already exist\n",
    "def maybe_create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print(\"{} created\".format(dir_path))\n",
    "    else:\n",
    "        print(\"{} already exists\".format(dir_path))\n",
    "    \n",
    "def make_standard_dirs(saver=True, best_params=True, tf_logs=True):\n",
    "    # `saver/` will hold tf saver files\n",
    "    maybe_create_dir(\"saver\")\n",
    "    # `best_params/` will hold a serialized version of the best params\n",
    "    # I like to keep this as a backup in case I run into issues with\n",
    "    # the saver files\n",
    "    maybe_create_dir(\"best_params\")\n",
    "    # `tf_logs/` will hold the logs that will be visable in tensorboard\n",
    "    maybe_create_dir(\"tf_logs\")\n",
    "\n",
    "    \n",
    "# set tf log level to supress messages, unless an error\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Important Version information\n",
    "print(\"Python: {}\".format(sys.version_info[:]))\n",
    "print('TensorFlow: {}'.format(tf.__version__))\n",
    "\n",
    "# Check if using GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "    \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_standard_dirs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers for saving/loading model params to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# these two functions (get_model_params and restore_model_params) are \n",
    "# ad[a|o]pted from; \n",
    "# https://github.com/ageron/handson-ml/blob/master/11_deep_learning.ipynb\n",
    "def get_model_params():\n",
    "    global_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    return {global_vars.op.name: value for global_vars, value in \n",
    "            zip(global_vars, tf.get_default_session().run(global_vars))}\n",
    "\n",
    "def restore_model_params(model_params, g, sess):\n",
    "    gvar_names = list(model_params.keys())\n",
    "    assign_ops = {gvar_name: g.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                  for gvar_name in gvar_names}\n",
    "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "# these two functions are used to manually save the best\n",
    "# model params to disk\n",
    "def save_obj(obj, name):\n",
    "    with open('best_params/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('best_params/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: update - dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 672\n",
    "SQUARE_DIM = 224\n",
    "if SQUARE_DIM:\n",
    "    IMG_WIDTH = SQUARE_DIM\n",
    "    IMG_HEIGHT = SQUARE_DIM\n",
    "    \n",
    "CHANNELS = 3\n",
    "\n",
    "# utility plotting function\n",
    "def plot_color_image(image):\n",
    "    plt.imshow(image.astype(np.uint8),interpolation=\"nearest\")\n",
    "    plt.title(\"{}x{}\".format(image.shape[1], image.shape[0]))\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "# load dataset into memory\n",
    "X_tr = np.load('../numpy/sigmoid/' + str(SQUARE_DIM) + '/X_train.npy')\n",
    "y_tr = np.load('../numpy/sigmoid/' + str(SQUARE_DIM) + '/y_train.npy')\n",
    "\n",
    "raw_X_val = np.load('../numpy/sigmoid/' + str(SQUARE_DIM) + '/X_val.npy')\n",
    "y_val = np.load('../numpy/sigmoid/' + str(SQUARE_DIM) + '/y_val.npy')\n",
    "\n",
    "raw_X_test = np.load('../numpy/sigmoid/' + str(SQUARE_DIM) + '/X_test.npy')\n",
    "y_test = np.load('../numpy/sigmoid/' + str(SQUARE_DIM) + '/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## - vgg stats\n",
    "# meanR = 123.68\n",
    "# meanG = 116.779\n",
    "# meanB = 103.939\n",
    "\n",
    "\n",
    "# lesion information\n",
    "R_std = 40.8277369711\n",
    "G_std = 45.2704252815\n",
    "B_std = 48.3190131682\n",
    "\n",
    "meanR = 183.305\n",
    "meanG = 149.097\n",
    "meanB = 135.272\n",
    "\n",
    "\n",
    "# # scale from [0, 1]\n",
    "# X_tr = np.zeros((raw_X_tr.shape[0], IMG_WIDTH, IMG_HEIGHT, 3), dtype='float32')\n",
    "# ind = 0\n",
    "# for img in raw_X_tr:\n",
    "#     X_tr[ind][:,:,0] = np.divide((img[:,:,0] - meanR), 255)\n",
    "#     X_tr[ind][:,:,1] = np.divide((img[:,:,1] - meanG), 255)\n",
    "#     X_tr[ind][:,:,2] = np.divide((img[:,:,2] - meanB), 255)\n",
    "#     ind += 1\n",
    "# del raw_X_tr\n",
    "\n",
    "# LESION NORMALIZATION\n",
    "# # validation\n",
    "X_val = np.zeros((raw_X_val.shape[0], IMG_WIDTH, IMG_HEIGHT, 3), dtype='float32')\n",
    "ind = 0\n",
    "for img in raw_X_val:\n",
    "    #VGG ex: X_test[ind][:,:,0] =  raw_X_test[ind][:,:,0] - meanR\n",
    "    X_val[ind][:,:,0] =  np.divide((raw_X_val[ind][:,:,0] - meanR), R_std)\n",
    "    X_val[ind][:,:,1] =  np.divide((raw_X_val[ind][:,:,1] - meanG), G_std)\n",
    "    X_val[ind][:,:,2] =  np.divide((raw_X_val[ind][:,:,2] - meanB), B_std)\n",
    "    # IF VGG; convert to BGR\n",
    "#     X_val[ind] = X_val[ind][:,:,::-1]\n",
    "    ind += 1\n",
    "del raw_X_val\n",
    "\n",
    "\n",
    "# # test\n",
    "X_test = np.zeros((raw_X_test.shape[0], IMG_WIDTH, IMG_HEIGHT, 3), dtype='float32')\n",
    "ind = 0\n",
    "for img in raw_X_test:\n",
    "    X_test[ind][:,:,0] =  np.divide((raw_X_test[ind][:,:,0] - meanR), R_std)\n",
    "    X_test[ind][:,:,1] =  np.divide((raw_X_test[ind][:,:,1] - meanR), R_std)\n",
    "    X_test[ind][:,:,2] =  np.divide((raw_X_test[ind][:,:,2] - meanR), R_std)\n",
    "    # convert to BGR\n",
    "    # IF VGG; convert to BGR\n",
    "#     X_test[ind] = X_test[ind][:,:,::-1]\n",
    "    ind += 1\n",
    "del raw_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function is adapted from https://github.com/ageron/handson-ml/blob/master/13_convolutional_neural_networks.ipynb\n",
    "def val_augment_image(image, lrf_p=0.0, upf_p=0.0, roll_num=0):\n",
    "        \n",
    "    n_image = image\n",
    "\n",
    "    # Flip the image horizontally with upf_p% probability:\n",
    "    if np.random.rand() < lrf_p:\n",
    "        n_image = np.fliplr(n_image)\n",
    "\n",
    "    # Flip the image vertically with lrf_p% probability:\n",
    "    if np.random.rand() < upf_p:\n",
    "        n_image = np.flipud(n_image)\n",
    "        \n",
    "    # 50% chance roll image horiz\n",
    "    if np.random.rand() < 0.5:\n",
    "        val = int(np.random.rand() * roll_num)\n",
    "        # 50% left or right\n",
    "        if np.random.rand() < 0.5:\n",
    "            n_image = np.roll(n_image, val, axis=1)\n",
    "        else:\n",
    "            n_image = np.roll(n_image, -val, axis=1)\n",
    "        \n",
    "    # 50% chance roll image vert\n",
    "    if np.random.rand() < 0.5:\n",
    "        val = int(np.random.rand() * roll_num)\n",
    "        # 50% left or right\n",
    "        if np.random.rand() < 0.5:\n",
    "            n_image = np.roll(n_image, val, axis=0)\n",
    "        else:\n",
    "            n_image = np.roll(n_image, -val, axis=0)\n",
    "\n",
    "    # ensure float32\n",
    "    n_image = n_image.astype(np.float32)\n",
    "\n",
    "    return n_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function is adapted from https://github.com/ageron/handson-ml/blob/master/13_convolutional_neural_networks.ipynb\n",
    "def augment_image(image, lrf_p=0.0, upf_p=0.0, r_degree=0.0, max_zoom=0.0, roll_num=0):  \n",
    "    n_image = image\n",
    "    \n",
    "    # Flip the image horizontally with upf_p% probability:\n",
    "    if np.random.rand() < lrf_p:\n",
    "        n_image = np.fliplr(n_image)\n",
    "\n",
    "    # Flip the image vertically with lrf_p% probability:\n",
    "    if np.random.rand() < upf_p:\n",
    "        n_image = np.flipud(n_image)\n",
    "        \n",
    "    # 50% chance roll image horiz\n",
    "    if np.random.rand() < 0.5:\n",
    "        val = int(np.random.rand() * roll_num)\n",
    "        # 50% left or right\n",
    "        if np.random.rand() < 0.5:\n",
    "            n_image = np.roll(n_image, val, axis=1)\n",
    "        else:\n",
    "            n_image = np.roll(n_image, -val, axis=1)\n",
    "        \n",
    "    # 50% chance roll image vert\n",
    "    if np.random.rand() < 0.5:\n",
    "        val = int(np.random.rand() * roll_num)\n",
    "        # 50% left or right\n",
    "        if np.random.rand() < 0.5:\n",
    "            n_image = np.roll(n_image, val, axis=0)\n",
    "        else:\n",
    "            n_image = np.roll(n_image, -val, axis=0)\n",
    "\n",
    "    # normalize on the fly\n",
    "    # this is inefficient, but we need to use this here since the resize converts\n",
    "    # back from uint8\n",
    "    n_image = n_image.astype(np.float32)\n",
    "\n",
    "    #print(\"pre\", n_image[0][0])\n",
    "#     meanR = 123.68\n",
    "#     meanG = 116.779\n",
    "#     meanB = 103.939\n",
    "    meanR = 183.305\n",
    "    meanG = 149.097\n",
    "    meanB = 135.272\n",
    "    \n",
    "    R_std = 40.8277369711\n",
    "    G_std = 45.2704252815\n",
    "    B_std = 48.3190131682\n",
    "    \n",
    "    # if VGG, only subtract mean, don't scale\n",
    "    n_image[:,:,0] -= meanR\n",
    "    n_image[:,:,1] -= meanG\n",
    "    n_image[:,:,2] -= meanB\n",
    "    \n",
    "    n_image[:,:,0] /= R_std\n",
    "    n_image[:,:,1] /= G_std\n",
    "    n_image[:,:,2] /= B_std\n",
    "\n",
    "    # IF VGG, convert to BGR\n",
    "#     n_image = n_image[:,:,::-1]\n",
    "\n",
    "    if r_degree > 0.0:\n",
    "        r_degree *= np.random.rand()\n",
    "        # randomly choose rotation direction\n",
    "        r_dir = (1 if np.random.rand() > 0.5 else -1)\n",
    "        n_image = ndimage.interpolation.rotate(n_image, r_dir*r_degree, reshape=False)\n",
    "    \n",
    "    return n_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: update hyperparms use/organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "n_outputs = 1\n",
    "\n",
    "l2_regularizer = tf.contrib.layers.l2_regularizer(0.01)\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __build_kernel_var(NAME=None, SHAPE=None, KERNEL_INIT=None):\n",
    "    if KERNEL_INIT:\n",
    "        KERNEL_INIT_METHOD = KERNEL_INIT\n",
    "    else:\n",
    "        KERNEL_INIT_METHOD = tf.contrib.layers.xavier_initializer()\n",
    "    kernel = tf.get_variable(name=str(NAME + \"_kernel\"),\n",
    "                             shape=SHAPE,\n",
    "                             initializer=KERNEL_INIT_METHOD,\n",
    "                             dtype=tf.float32)\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __build_bias_var(NAME=None, FILTER_NUM=None, BIAS_INIT=None):\n",
    "    if BIAS_INIT:\n",
    "        BIAS_INIT_METHOD = BIAS_INIT\n",
    "    else:\n",
    "        BIAS_INIT_METHOD = tf.zeros_initializer()\n",
    "    bias_var = tf.get_variable(name=str(NAME+\"_bias\"), \n",
    "                               shape=[FILTER_NUM], \n",
    "                               initializer=BIAS_INIT_METHOD)\n",
    "    return bias_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __build_out(name=None, kernel=None, bias=None, activation_fn=None):\n",
    "    component = tf.nn.bias_add(kernel, bias)\n",
    "    if activation_fn:\n",
    "        # handle special cases of activations, if requested\n",
    "        print(\"NOTE: custom activation function requested\")\n",
    "        component_out = activation_fn(component)\n",
    "    else:\n",
    "        component_out = tf.nn.selu(component, name=str(name+\"_out\"))\n",
    "    return component_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _build_conv_layer(X_in=None, name=None, kern_shape=None, stride_len=None):\n",
    "    block_name = str(name + \"_conv2d\")\n",
    "    kernel = __build_kernel_var(NAME=block_name, \n",
    "                                SHAPE=kern_shape)\n",
    "    conv_block = tf.nn.conv2d(input=X_in, filter=kernel,\n",
    "                              strides=[1, stride_len, stride_len, 1], \n",
    "                              padding=\"SAME\", \n",
    "                              name=block_name)\n",
    "    return conv_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# used to create stacked, split, conv layers\n",
    "def _build_split_conv_layer(X_in=None, name=None, kern_size=None, \n",
    "                            H_first=True, stride_len=None, \n",
    "                            filter_num_in=None, filter_num_out=None):\n",
    "    # :kern_size: size of the kernel (long dimension), single value\n",
    "    if H_first:\n",
    "        # [long, 1] --> [1, long]\n",
    "        block_name = str(name + \"_split_HV_\" + str(kern_size))\n",
    "        shape_a = [kern_size, 1, filter_num_in, filter_num_out]\n",
    "        shape_b = [1, kern_size, filter_num_out, filter_num_out]\n",
    "        \n",
    "    else:\n",
    "        # [1, long] --> [long, 1]\n",
    "        block_name = str(name + \"_split_VH_\" + str(kern_size))\n",
    "        shape_a = [1, kern_size, filter_num_in, filter_num_out]\n",
    "        shape_b = [kern_size, 1, filter_num_out, filter_num_out]\n",
    "\n",
    "    # create the two corresponding split kernels\n",
    "    c_a_name = str(block_name + \"_a\")\n",
    "    split_kernel_a = __build_kernel_var(NAME=c_a_name, SHAPE=shape_a)\n",
    "    c_b_name = str(block_name + \"_b\")\n",
    "    split_kernel_b = __build_kernel_var(NAME=c_b_name, SHAPE=shape_b)\n",
    "    \n",
    "    # build first block\n",
    "    # ---- conv\n",
    "    conv_a = _build_conv_layer(X_in=X_in, name=c_a_name, \n",
    "                               kern_shape=shape_a, stride_len=stride_len)\n",
    "    bias_a = __build_bias_var(NAME=c_a_name, FILTER_NUM=int(filter_num_out))\n",
    "    conv_a_out = __build_out(kernel=conv_a, bias=bias_a, name=c_a_name)\n",
    "    \n",
    "    # build second block\n",
    "    conv_b = _build_conv_layer(X_in=conv_a_out, name=c_b_name,\n",
    "                               kern_shape=shape_b, stride_len=stride_len)\n",
    "    \n",
    "    return conv_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# used to create single stacked layers\n",
    "def _build_nested_conv_layer(X_in=None, name=None, kern_shape=None, stride_len=None, depth=None):\n",
    "    \n",
    "    filter_multipier = 1.5\n",
    "    nested_component_id = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\"]\n",
    "    for i in range(depth):  \n",
    "        \n",
    "        block_name = str(name + \"_conv2d_nested_\" + nested_component_id[i]) \n",
    "        if i == 0:\n",
    "            in_ph = X_in\n",
    "        else:\n",
    "            in_ph = endpoint\n",
    "            # we will want to keep the kernel size the same.\n",
    "            # the out out filter size could change\n",
    "            in_filter_num = int(in_ph.get_shape()[-1])\n",
    "            out_filter_num = int(in_filter_num*filter_multipier)\n",
    "            cur_conv_size = kern_shape[0]\n",
    "            kern_shape=[cur_conv_size, cur_conv_size, in_filter_num, out_filter_num]\n",
    "        \n",
    "              \n",
    "        endpoint = _build_conv_layer(X_in=in_ph, name=block_name, \n",
    "                                     kern_shape=kern_shape, stride_len=stride_len)\n",
    "        \n",
    "        # apply bias and activation if not end\n",
    "        if i != (depth - 1):\n",
    "            print(\"appplying mid bias and activation\")\n",
    "            conv_bias = __build_bias_var(NAME=block_name, FILTER_NUM=int(endpoint.get_shape()[-1]))\n",
    "            endpoint = __build_out(kernel=endpoint, bias=conv_bias, name=block_name)\n",
    "    \n",
    "    return endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _concatenate_layers_out(name=None, layer_list=None, full_len=None):\n",
    "    stacked_kernels = tf.concat(layer_list, axis=3)\n",
    "    bis_var = __build_bias_var(NAME=name, FILTER_NUM=full_len)\n",
    "    concat_out = __build_out(kernel=stacked_kernels, bias=bis_var, name=name)\n",
    "    return concat_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __build_feature_components_dict(component_list=None):\n",
    "    # the id is used to produce a unique id for each componet within\n",
    "    # a layer as well as prevent any variable name reuse\n",
    "    layer_id = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\"]\n",
    "    ordered_feat_desc = collections.OrderedDict()\n",
    "    for i, component in enumerate(component_list):\n",
    "        ordered_feat_desc[str(layer_id[i])] = component\n",
    "    return ordered_feat_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _build_layer_extraction(prev_endpoint=None, layer_num=None, comp_dict=None, in_filter_num=None, out_filter_num=None, name_str=None):    \n",
    "    component_list = []\n",
    "    num_conv = 0\n",
    "    num_pool = 0\n",
    "    for comp_key in comp_dict:\n",
    "        base_name = name_str + \"_\" + comp_key\n",
    "        if comp_dict[comp_key][\"type\"] == \"conv\":\n",
    "            c_size = comp_dict[comp_key][\"params\"][\"size\"]\n",
    "            stride_len = comp_dict[comp_key][\"params\"][\"stride\"]\n",
    "            if comp_dict[comp_key][\"params\"][\"shape\"] == \"square\":\n",
    "                print(\"square\")\n",
    "                component = _build_conv_layer(X_in=prev_endpoint, name=base_name, \n",
    "                                              kern_shape=[c_size,c_size,in_filter_num, out_filter_num], \n",
    "                                              stride_len=stride_len)\n",
    "\n",
    "            elif comp_dict[comp_key][\"params\"][\"shape\"] == \"split\":\n",
    "                print(\"split\")\n",
    "                component = _build_split_conv_layer(X_in=prev_endpoint, name=base_name, \n",
    "                                                    kern_size=c_size, \n",
    "                                                    H_first=True, stride_len=stride_len, \n",
    "                                                    filter_num_in=in_filter_num, filter_num_out=out_filter_num)\n",
    "            num_conv += 1\n",
    "\n",
    "            \n",
    "        elif comp_dict[comp_key][\"type\"] == \"avgpool\":\n",
    "            print(\"avg pool:\", comp_dict[comp_key])\n",
    "            component = tf.nn.avg_pool(value=prev_endpoint, \n",
    "                                       ksize=[1,3,3,1], strides=[1,1,1,1], \n",
    "                                       padding=\"SAME\", name=str(name_str+\"_avg_pool\"))\n",
    "            num_pool += 1\n",
    "        \n",
    "        elif comp_dict[comp_key][\"type\"] == \"maxpool\":\n",
    "            print(\"max pool:\", comp_dict[comp_key])\n",
    "            component = tf.nn.max_pool(value=prev_endpoint, \n",
    "                                       ksize=[1,2,2,1], strides=[1,2,2,1], \n",
    "                                       padding=\"SAME\", name=str(name_str+\"_max_pool\"))\n",
    "            num_pool += 1\n",
    "            \n",
    "        elif comp_dict[comp_key][\"type\"] == \"nested\":\n",
    "            c_size = comp_dict[comp_key][\"params\"][\"size\"]\n",
    "            stride_len = comp_dict[comp_key][\"params\"][\"stride\"]\n",
    "            nested_depth = comp_dict[comp_key][\"params\"][\"depth\"]\n",
    "            # TODO: there is an issue here in that we only increase the filter depth\n",
    "            # during the first iteration.  This likely should be interatively inreased\n",
    "            # and the filter_out_num will need to be returned and accounted for.  This\n",
    "            # strengthens the idea that when building the layers, the filter num\n",
    "            # should be calculated from the previous output and not hard coded.\n",
    "            component = _build_nested_conv_layer(X_in=prev_endpoint, name=base_name, \n",
    "                                     kern_shape=[c_size,c_size,in_filter_num, out_filter_num], \n",
    "                                     stride_len=stride_len, depth=nested_depth)\n",
    "            num_conv += 1\n",
    "        \n",
    "        component_list.append(component)\n",
    "        \n",
    "    # =================== concatenate output together\n",
    "    final_len = 0\n",
    "    final_bias = 0\n",
    "    for compx in component_list:\n",
    "        compx_bias = int(np.prod(compx.get_shape()[1:]))\n",
    "        final_bias += compx_bias\n",
    "        final_len = int(compx.get_shape()[-1])\n",
    "\n",
    "    feature_len = int(out_filter_num*(num_conv) + (in_filter_num*num_pool))\n",
    "    print(\"final_len: \", final_len)\n",
    "    print(\"final_bias: \", final_bias)\n",
    "    print(\"feature len: \", feature_len)\n",
    "    endpoint = _concatenate_layers_out(name=str(name_str+\"_concat\"), layer_list=component_list, full_len=final_len)\n",
    "\n",
    "    return endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_custom(ARCH_ID, P_LEVEL, FINAL, REG, MODEL_DEFINITION):\n",
    "\n",
    "    reset_graph()\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "\n",
    "        with tf.name_scope(\"hyper_params\"):\n",
    "            learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "        with tf.name_scope(\"inputs\"):\n",
    "            # data\n",
    "            X = tf.placeholder(tf.float32, shape=[None, IMG_HEIGHT, IMG_WIDTH, CHANNELS], name=\"X\")\n",
    "\n",
    "            # labels\n",
    "            y_raw = tf.placeholder(tf.int64, shape=[None, n_outputs], name=\"y_input\")\n",
    "            y_ = tf.cast(y_raw, tf.float32)\n",
    "\n",
    "            # for training/evaluating\n",
    "            training = tf.placeholder_with_default(False, shape=[], name=\"training\")\n",
    "\n",
    "        ######################################## Build layers\n",
    "        endpoint = None\n",
    "        l = None\n",
    "        for i, l in enumerate(MODEL_DEFINITION):\n",
    "            component_list = MODEL_DEFINITION[l]\n",
    "            layer_name = \"cat_0\" + str(i)\n",
    "            if i == 0:\n",
    "                in_chan = 3\n",
    "                filter_count = int(in_chan * 2)\n",
    "                in_ph = X\n",
    "            else:\n",
    "                in_ph = endpoint\n",
    "                \n",
    "            if l == \"E\":\n",
    "#                 component_list = MODEL_DEFINITION[l]\n",
    "                OCD = __build_feature_components_dict(component_list)\n",
    "\n",
    "            elif l == \"R\":\n",
    "                # the objective is to decrease the H and W of the current feature map\n",
    "                # i.e. [224x224xN] --> [112x112xN]\n",
    "                OCD = __build_feature_components_dict(component_list)\n",
    "                \n",
    "            elif l == \"C\":\n",
    "                # the objective is to decrease the \"length\" of the current feature map\n",
    "                # and so our filter count will be a % smaller than the current value.\n",
    "                # i.e. [HxWx512] --> [HxWx256]\n",
    "                # TODO: this value should be parameterized\n",
    "                filter_count = int(in_chan*0.6)\n",
    "                OCD = __build_feature_components_dict(component_list)\n",
    "            \n",
    "            elif l == \"N\":\n",
    "                print(\"---- nested ------\")\n",
    "                OCD = __build_feature_components_dict(component_list)\n",
    "                \n",
    "            endpoint = _build_layer_extraction(prev_endpoint=in_ph, layer_num=i, comp_dict=OCD,\n",
    "                                               in_filter_num=in_chan, out_filter_num=filter_count,\n",
    "                                               name_str=layer_name)\n",
    "            in_chan = int(np.prod(endpoint.get_shape()[-1]))\n",
    "        \n",
    "        ######################################################### [Build Layers: END]\n",
    "                        \n",
    "            \n",
    "        # Reshape for fully connected layer\n",
    "        last_shape = int(np.prod(endpoint.get_shape()[1:]))\n",
    "        endpoint = tf.reshape(endpoint, shape=[-1, last_shape])\n",
    "        \n",
    "        \n",
    "        ## add dropout regularization after the first 2 layers\n",
    "        REG = False\n",
    "        if REG:\n",
    "            reg_value = 0.33\n",
    "            print(\"adding pre-fc regularization: {}\".format(reg_value))\n",
    "            endpoint = tf.layers.dropout(endpoint, reg_value, training=training, name=\"entry_drop\")\n",
    "\n",
    "            \n",
    "        with tf.name_scope(\"output_layer\"):\n",
    "            if FINAL:\n",
    "                print(\"last shape:\", last_shape)\n",
    "                logits_ = tf.layers.dense(endpoint, 128, kernel_regularizer=l2_regularizer, \n",
    "                                          kernel_initializer=he_init, activation=tf.nn.selu, name=\"fc1\")\n",
    "                logits_ = tf.layers.dropout(endpoint, 0.2, training=training, name=\"fc1_drop\")\n",
    "                \n",
    "                logits_ = tf.layers.dense(logits_, 48, kernel_regularizer=l2_regularizer,\n",
    "                                          kernel_initializer=he_init, activation=tf.nn.selu, name=\"fc2\")\n",
    "                \n",
    "                logits_ = tf.layers.dense(logits_, 16, kernel_regularizer=l2_regularizer,\n",
    "                                          kernel_initializer=he_init, activation=tf.nn.selu, name=\"fc3\")\n",
    "                \n",
    "#                 logits_ = tf.layers.dense(logits_, 24, kernel_regularizer=l2_regularizer,\n",
    "#                                           kernel_initializer=he_init, activation=tf.nn.selu, name=\"fc4\")\n",
    "                \n",
    "#                 logits_ = tf.layers.dense(logits_, 16, kernel_regularizer=l2_regularizer, \n",
    "#                                           kernel_initializer=he_init, activation=tf.nn.selu, name=\"fc5\")\n",
    "                \n",
    "#                 logits_ = tf.layers.dense(logits_, 8, kernel_regularizer=l2_regularizer, \n",
    "#                                           kernel_initializer=he_init, activation=tf.nn.selu, name=\"fc6\")\n",
    "\n",
    "                logits_ = tf.layers.dense(logits_, n_outputs, name=\"sigmoid_output\")\n",
    "                preds = tf.sigmoid(logits_, name=\"preds\")\n",
    "            else:\n",
    "                logits_ = tf.layers.dense(endpoint, n_outputs, name=\"sigmoid_output\")\n",
    "                preds = tf.sigmoid(logits_, name=\"preds\")\n",
    "\n",
    "        # ================================================ cost\n",
    "        with tf.name_scope(\"cross_entropy\"):\n",
    "            xentropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_, labels=y_)\n",
    "            batch_loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "        with tf.name_scope(\"train\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                               beta1=0.9,\n",
    "                                               beta2=0.999,\n",
    "                                               epsilon=1e-08,\n",
    "                                               use_locking=False,\n",
    "                                               name='Adam')\n",
    "            # optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#             optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "            # training_op = optimizer.minimize(batch_loss, name=\"training_op\")\n",
    "            \n",
    "            all_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "            scope_str = None\n",
    "            if FINAL:\n",
    "                # TODO:\n",
    "                scope_str = \"cat_0[^a]\"\n",
    "                freeze_vars = g.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope_str)\n",
    "#                 freeze_vars = g.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "#                 last_bias = \"cat_0[\"+ str(P_LEVEL) +\"]_all_bias\"\n",
    "#                 last_bias_var = g.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=last_bias)\n",
    "#                 freeze_vars = [var for var in freeze_vars if var not in last_bias_var]\n",
    "            else:\n",
    "                # TODO:\n",
    "                scope_str = \"cat_0[^\" + str(P_LEVEL) + \"]\"\n",
    "                print(\"scope_str:\", scope_str)\n",
    "                freeze_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope_str)\n",
    "#                 prev_bias_str = \"cat_0[\" + str(P_LEVEL - 1) + \"]_all_bias\"\n",
    "#                 prev_bias_var = g.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=prev_bias_str)\n",
    "                # remove previous bias var from variables to freeze\n",
    "#                 freeze_vars = [var for var in freeze_vars if var not in prev_bias_var]\n",
    "                \n",
    "            train_vars = [var for var in all_vars if var not in freeze_vars]\n",
    "\n",
    "            print(\"--------------------------------\")\n",
    "            print(\"frozen: \", freeze_vars)\n",
    "            print(\"--------------------------------\")\n",
    "            print(\"train: \", train_vars)\n",
    "            print(\"--------------------------------\")\n",
    "            print(\"num vars:\", len(all_vars), \"= frozen(\", len(freeze_vars), \")\", \"+ train(\", len(train_vars), \")\")\n",
    "            print(\"=================================\")\n",
    "            \n",
    "            training_op = optimizer.minimize(batch_loss, var_list=train_vars, name=\"training_op\")\n",
    "\n",
    "\n",
    "        # =============================================== metrics\n",
    "        with tf.name_scope(\"train_metrics\") as scope:    \n",
    "            # train_y_true_cls = tf.cast(y_, tf.bool)\n",
    "            train_y_true_cls = tf.greater_equal(y_, 0.5)\n",
    "            train_y_pred_cls = tf.greater_equal(preds, 0.5)\n",
    "\n",
    "            correct_prediction = tf.equal(train_y_pred_cls, train_y_true_cls)\n",
    "            train_batch_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            \n",
    "            train_auc, train_auc_update = tf.metrics.auc(labels=y_, predictions=preds)\n",
    "\n",
    "            train_tf_acc, train_tf_acc_op = tf.metrics.accuracy(labels=train_y_true_cls, predictions=train_y_pred_cls)\n",
    "            train_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "            train_acc_reset_op = tf.variables_initializer(train_acc_vars, name=\"train_acc_reset_op\")\n",
    "            \n",
    "        for node in (y_, preds, train_y_true_cls, train_y_pred_cls, correct_prediction, train_batch_acc):\n",
    "            g.add_to_collection(\"label_nodes\", node)\n",
    "\n",
    "        with tf.name_scope(\"validation_metrics\") as scope:    \n",
    "            val_y_true_cls = tf.greater_equal(y_, 0.5)\n",
    "            val_y_pred_cls = tf.greater_equal(preds, 0.5)\n",
    "\n",
    "            val_correct_prediction = tf.equal(val_y_pred_cls, val_y_true_cls)\n",
    "            val_batch_acc = tf.reduce_mean(tf.cast(val_correct_prediction, tf.float32))\n",
    "            \n",
    "            val_auc, val_auc_update = tf.metrics.auc(labels=y_, predictions=preds)\n",
    "\n",
    "            val_tf_acc, val_tf_acc_op = tf.metrics.accuracy(labels=val_y_true_cls, predictions=val_y_pred_cls)\n",
    "            val_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "            val_acc_reset_op = tf.variables_initializer(val_acc_vars, name=\"val_acc_reset_op\")\n",
    "\n",
    "        with tf.name_scope(\"test_metrics\") as scope:    \n",
    "            test_y_true_cls = tf.greater_equal(y_, 0.5)\n",
    "            test_y_pred_cls = tf.greater_equal(preds, 0.5)\n",
    "            \n",
    "            test_auc, test_auc_update = tf.metrics.auc(labels=y_, predictions=preds)\n",
    "            \n",
    "            test_tf_acc, test_tf_acc_op = tf.metrics.accuracy(labels=test_y_true_cls, predictions=test_y_pred_cls)\n",
    "            test_acc_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "            test_acc_reset_op = tf.variables_initializer(test_acc_vars, name=\"test_acc_reset_op\")\n",
    "\n",
    "        # =============================================== loss \n",
    "        with tf.name_scope(\"train_loss_eval\") as scope:\n",
    "            train_mean_loss, train_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "            train_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "            train_loss_reset_op = tf.variables_initializer(train_loss_vars, name=\"train_loss_reset_op\")\n",
    "\n",
    "        with tf.name_scope(\"val_loss_eval\") as scope:\n",
    "            val_mean_loss, val_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "            val_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "            val_loss_reset_op = tf.variables_initializer(val_loss_vars, name=\"val_loss_reset_op\")\n",
    "\n",
    "        with tf.name_scope(\"test_loss_eval\") as scope:\n",
    "            test_mean_loss, test_mean_loss_update = tf.metrics.mean(batch_loss)\n",
    "            test_loss_vars = tf.contrib.framework.get_variables(scope, collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "            test_loss_reset_op = tf.variables_initializer(test_loss_vars, name=\"test_loss_rest_op\")\n",
    "\n",
    "        # ================================================ initialize and save\n",
    "        with tf.name_scope(\"save_session\"):\n",
    "            init_global = tf.global_variables_initializer()\n",
    "            init_local = tf.local_variables_initializer()\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "        # ==================================== combine operations\n",
    "        # ===== epoch, train\n",
    "        epoch_train_loss_scalar = tf.summary.scalar('train_epoch_loss', train_mean_loss)\n",
    "#         epoch_train_acc_scalar = tf.summary.scalar('train_epoch_acc', train_tf_acc)\n",
    "        epoch_train_auc_scalar = tf.summary.scalar('train_epoch_auc', train_auc)\n",
    "        epoch_train_write_op = tf.summary.merge([epoch_train_loss_scalar, epoch_train_auc_scalar], name=\"epoch_train_write_op\")\n",
    "\n",
    "        # ===== epoch, validation\n",
    "        epoch_validation_loss_scalar = tf.summary.scalar('validation_epoch_loss', val_mean_loss)\n",
    "#         epoch_validation_acc_scalar = tf.summary.scalar('validation_epoch_acc', val_tf_acc)\n",
    "        epoch_validation_auc_scalar = tf.summary.scalar('validation_epoch_auc', val_auc)\n",
    "        epoch_validation_write_op = tf.summary.merge([epoch_validation_loss_scalar, epoch_validation_auc_scalar], name=\"epoch_validation_write_op\")\n",
    "\n",
    "        # ====== batch, train\n",
    "        train_batch_loss_scalar = tf.summary.scalar('train_batch_loss', batch_loss)\n",
    "        train_batch_acc_scalar = tf.summary.scalar('train_batch_acc', train_batch_acc)\n",
    "        train_batch_write_op = tf.summary.merge([train_batch_loss_scalar, train_batch_acc_scalar], name=\"train_batch_write_op\")\n",
    "\n",
    "        # ====== checkpoint, validation\n",
    "        checkpoint_validation_loss_scalar = tf.summary.scalar('validation_batch_loss', batch_loss)\n",
    "        checkpoint_validation_acc_scalar = tf.summary.scalar('validation_batch_acc', val_batch_acc)\n",
    "        checkpoint_validation_write_op = tf.summary.merge([checkpoint_validation_loss_scalar, checkpoint_validation_acc_scalar], name=\"checkpoint_valdiation_write_op\")\n",
    "        \n",
    "        # write operations\n",
    "        for node in (epoch_train_write_op, epoch_validation_write_op, train_batch_write_op, checkpoint_validation_write_op):\n",
    "            g.add_to_collection(\"write_ops\", node)\n",
    "            \n",
    "        # saver/init\n",
    "        for node in (init_global, init_local):\n",
    "            g.add_to_collection(\"save_init\", node)\n",
    "            \n",
    "        # acc metrics\n",
    "        for node in (train_tf_acc, train_tf_acc_op, train_acc_reset_op, \n",
    "                     train_auc, train_auc_update,\n",
    "                     val_tf_acc, val_tf_acc_op, val_acc_reset_op,\n",
    "                     val_auc, val_auc_update,\n",
    "                     test_tf_acc, test_tf_acc_op, test_acc_reset_op,\n",
    "                     test_auc, test_auc_update):\n",
    "            g.add_to_collection(\"acc_metrics\", node)\n",
    "        \n",
    "        # loss metrics\n",
    "        for node in (train_mean_loss, train_mean_loss_update, train_loss_reset_op, \n",
    "                     val_mean_loss, val_mean_loss_update, val_loss_reset_op, \n",
    "                     test_mean_loss, test_mean_loss_update, test_loss_reset_op):\n",
    "            g.add_to_collection(\"loss_metrics\", node)\n",
    "            \n",
    "        # test metrics\n",
    "        for node in (train_y_true_cls, train_y_pred_cls):\n",
    "            g.add_to_collection(\"test_metrics\", node)\n",
    "            \n",
    "        # main operations\n",
    "        for node in (training_op, X, y_raw, training, learning_rate):\n",
    "            g.add_to_collection(\"main_ops\", node)\n",
    "\n",
    "        print(\"model built.\")\n",
    "\n",
    "    return g, saver\n",
    "\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_that_model(P_LEVEL, p_g, NUM_EPOCHS, FINAL, LEARNING_RATE, MODEL_DEF_LIST, REG_FLAG=False):\n",
    "    global X_tr\n",
    "    global y_tr\n",
    "    global X_val\n",
    "    global y_val\n",
    "    global X_test\n",
    "    global y_test\n",
    "    global batch_size\n",
    "    print(\"batch_size: \", batch_size)\n",
    "    \n",
    "    #####################################\n",
    "    # ----------- augmentation\n",
    "    ##### training\n",
    "    AUG_BOOL = True\n",
    "    LRF_P = 0.25\n",
    "    UPF_p = 0.25\n",
    "    MAX_ZOOM = 0.0\n",
    "    ROLL_NUM = 20\n",
    "    if FINAL:\n",
    "        ROT_DEG = 0.0\n",
    "    else:\n",
    "        ROT_DEG = 0.0\n",
    "    \n",
    "    ##### validation\n",
    "    VAL_AUG_BOOL = True\n",
    "    if FINAL:\n",
    "        VAL_LRF_P = 0.5\n",
    "        VAL_UPF_P = 0.5\n",
    "        VAL_ROLL_NUM = 20\n",
    "    else:\n",
    "        VAL_LRF_P = 0.25\n",
    "        VAL_UPF_P = 0.25\n",
    "        VAL_ROLL_NUM = 10\n",
    "    ######################################\n",
    "    \n",
    "    \n",
    "    # --- save best params\n",
    "    best_model_params = None\n",
    "    best_loss_val = np.infty\n",
    "\n",
    "    # ------ for \"early-ish\" stopping\n",
    "    # check_interval = 15\n",
    "    checks_since_last_progress = 0\n",
    "    # max_checks_without_progress = 20\n",
    "\n",
    "    # Tensorboard\n",
    "    now = datetime.now().strftime(\"%d%b%Y_%H%M%S\")\n",
    "    if FINAL:\n",
    "        root_logdir = \"tf_logs_prog/\" + str(P_LEVEL) + \"_dense\"\n",
    "    else:\n",
    "        root_logdir = \"tf_logs_prog/\" + str(P_LEVEL)\n",
    "    logdir = \"{}/{}/\".format(root_logdir, now)\n",
    "    \n",
    "    # get previous endpoint\n",
    "#     target_endpoint = None\n",
    "#     if p_g:\n",
    "#         target_str = \"conv_0\" + str(P_LEVEL-1)\n",
    "#         target_endpoint = p_g.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=target_str)[0]\n",
    "#         print(\"previous endpoint: \", target_endpoint)\n",
    "\n",
    "    g, saver = build_custom(now, P_LEVEL, FINAL, REG=REG_FLAG, MODEL_DEFINITION=MODEL_DEF_LIST)\n",
    "    \n",
    "    if P_LEVEL > 1:\n",
    "        # load all weights/bias that aren't on this level (all previous levels)\n",
    "        if FINAL:\n",
    "            # load all conv vars\n",
    "            # TODO:\n",
    "            scope_str = \"cat_0[^a]\"\n",
    "            reuse_vars = g.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope_str)\n",
    "#             last_bias = \"conv_0\"+ str(P_LEVEL) +\"/bias\"\n",
    "#             last_bias_var = g.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=last_bias)\n",
    "#             load_vars = [var for var in reuse_vars if var not in last_bias_var]\n",
    "            reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "            print(\"restored {} vars: {}\".format(len(reuse_vars), reuse_vars))\n",
    "            restore_saver = tf.train.Saver(reuse_vars_dict)\n",
    "        else:\n",
    "            scope_str = \"cat_0[^\" + str(P_LEVEL) + \"]\"\n",
    "            print(scope_str)\n",
    "            reuse_vars = g.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope_str)\n",
    "            reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "            print(\"restored {} vars: {}\".format(len(reuse_vars), reuse_vars))\n",
    "            restore_saver = tf.train.Saver(reuse_vars_dict)\n",
    "    \n",
    "    epoch_train_write_op, epoch_validation_write_op, train_batch_write_op, checkpoint_validation_write_op = g.get_collection(\"write_ops\")\n",
    "    init_global, init_local = g.get_collection(\"save_init\")\n",
    "    train_tf_acc, train_tf_acc_op, train_acc_reset_op, train_auc, train_auc_update, val_tf_acc, val_tf_acc_op, val_acc_reset_op, val_auc, val_auc_update, test_tf_acc, test_tf_acc_op, test_acc_reset_op, test_auc, test_auc_update = g.get_collection(\"acc_metrics\")\n",
    "    train_mean_loss, train_mean_loss_update, train_loss_reset_op, val_mean_loss, val_mean_loss_update, val_loss_reset_op, test_mean_loss, test_mean_loss_update, test_loss_reset_op = g.get_collection(\"loss_metrics\")\n",
    "    training_op, X, y_raw, training, learning_rate = g.get_collection(\"main_ops\")\n",
    "    #prepared_image_op, input_image = g.get_collection(\"preprocess\")\n",
    "    step = 0\n",
    "    with tf.Session(graph=g) as sess:\n",
    "        init_global.run()\n",
    "        init_local.run()\n",
    "        sess.run(val_acc_reset_op)\n",
    "        sess.run(val_loss_reset_op)\n",
    "        sess.run(train_acc_reset_op)\n",
    "        sess.run(train_loss_reset_op)\n",
    "\n",
    "        # tensorboard writer\n",
    "        #batch_train_writer = tf.summary.FileWriter(os.path.join(logdir, \"cnn/batch/train/\"))\n",
    "        #batch_val_writer = tf.summary.FileWriter(os.path.join(logdir, \"cnn/batch/val/\"))\n",
    "        epoch_train_writer = tf.summary.FileWriter(os.path.join(logdir, \"cnn/epoch/train/\"))\n",
    "        epoch_val_writer = tf.summary.FileWriter(os.path.join(logdir, \"cnn/epoch/val/\"))\n",
    "        epoch_train_writer.add_graph(sess.graph)\n",
    "\n",
    "        low_i = 0\n",
    "        high_i = 0\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            step += 1\n",
    "            EPOCH_START_TIME = time.time()\n",
    "\n",
    "            # Shuffle the training set (data and label)\n",
    "            X_tr, y_tr = shuffle(X_tr, y_tr, random_state=42)\n",
    "\n",
    "            mini_step = 0\n",
    "            for iteration in range(len(X_tr) // batch_size):\n",
    "                mini_step += 1\n",
    "\n",
    "                low_i = iteration*batch_size\n",
    "                high_i = (iteration+1)*batch_size\n",
    "                X_batch_raw = X_tr[low_i:high_i]     \n",
    "                y_batch = y_tr[low_i:high_i]\n",
    "#                 print(mini_step, \": \", \"[\",low_i,\",\", high_i,\"]\",\"of\",len(X_batch_raw))\n",
    "\n",
    "                # augment batch\n",
    "                # TODO: look into factoring class size\n",
    "                X_batch_aug = np.zeros((X_batch_raw.shape[0], IMG_WIDTH, IMG_HEIGHT, 3), dtype='float32')\n",
    "                ind = 0\n",
    "                for img in X_batch_raw:\n",
    "                    if AUG_BOOL:\n",
    "#                         X_batch_aug[ind] = sess.run(prepared_image_op, feed_dict={input_image: img})\n",
    "                        X_batch_aug[ind] = augment_image(np.copy(img), lrf_p=LRF_P, \n",
    "                                                         upf_p=UPF_p, r_degree=ROT_DEG, \n",
    "                                                         roll_num=ROLL_NUM)\n",
    "                    else:\n",
    "                        X_batch_aug[ind] = np.copy(img)\n",
    "                    ind += 1\n",
    "\n",
    "                # shuffle batch after augmentation\n",
    "                #X_tr, y_tr = shuffle(X_batch_aug, y_batch, random_state=42)\n",
    "\n",
    "\n",
    "                feed_train = {X: X_batch_aug, \n",
    "                              y_raw: y_batch, \n",
    "                              training: True,\n",
    "                              learning_rate: LEARNING_RATE}\n",
    "\n",
    "                # training operation (back prop+)\n",
    "                sess.run(training_op, feed_dict=feed_train)\n",
    "                #step += 1\n",
    "\n",
    "                # update training metrics, acc and loss, (evaluation on current batch)\n",
    "                sess.run([train_tf_acc_op, train_auc_update, train_mean_loss_update], feed_dict={X: X_batch_aug, \n",
    "                                                                                                 y_raw: y_batch,\n",
    "                                                                                                 training: False})\n",
    "\n",
    "                # ================================================================================[validation]\n",
    "                if mini_step % 8 == 0:\n",
    "                         # ----- validation\n",
    "                    # run validation 3x w/ (lesser) augmentation\n",
    "                    for i in range(3):\n",
    "#                         X_val, y_val = shuffle(X_val, y_val, random_state=42)\n",
    "                        lower_i = 0\n",
    "                        higher_i = 0\n",
    "                        for vitter in range(len(X_val) // batch_size):\n",
    "                            lower_i = vitter*batch_size\n",
    "                            higher_i = (vitter+1)*batch_size\n",
    "                            X_val_batch = X_val[lower_i:higher_i]\n",
    "                            y_val_batch = y_val[lower_i:higher_i]\n",
    "\n",
    "                            # augment\n",
    "                            X_val_batch_aug = np.zeros((X_val_batch.shape[0], IMG_WIDTH, IMG_HEIGHT, 3), dtype='float32')\n",
    "                            ind = 0\n",
    "                            for img in X_val_batch:\n",
    "                                if VAL_AUG_BOOL:\n",
    "                                    # no normalization happens here\n",
    "                                    X_val_batch_aug[ind] = val_augment_image(np.copy(img), \n",
    "                                                                             lrf_p=VAL_LRF_P, \n",
    "                                                                             upf_p=VAL_UPF_P, \n",
    "                                                                             roll_num=VAL_ROLL_NUM)\n",
    "                                else:\n",
    "                                    X_val_batch_aug[ind] = np.copy(img)\n",
    "                                ind += 1\n",
    "\n",
    "                            sess.run([val_tf_acc_op, val_auc_update, val_mean_loss_update], \n",
    "                                     feed_dict={X: X_val_batch_aug, y_raw: y_val_batch, training: False})\n",
    "\n",
    "                        X_val_batch = X_val[higher_i:]\n",
    "                        y_val_batch = y_val[higher_i:]\n",
    "                        # not augmenting final validation batch *(which will be different each round\n",
    "                        # becuase of the shuffle)\n",
    "                        if len(X_batch_raw) > 0:\n",
    "                            sess.run([val_tf_acc_op, val_auc_update, val_mean_loss_update], \n",
    "                                     feed_dict={X: X_val_batch, y_raw: y_val_batch, training: False})\n",
    "            # ===============================================================================[validation end]\n",
    "            \n",
    "            # xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Train on remaining batch, if it exists \n",
    "            # metrics aren't calculated since the batch size could be really small\n",
    "            X_batch_raw = X_tr[high_i:]\n",
    "            y_batch = y_tr[high_i:]\n",
    "\n",
    "            if len(X_batch_raw) > 0:\n",
    "#                 print(len(X_batch_raw))\n",
    "                X_batch_augx = np.zeros((X_batch_raw.shape[0], IMG_WIDTH, IMG_HEIGHT, 3), dtype='float32')\n",
    "                ind = 0\n",
    "                for img in X_batch_raw:\n",
    "                    if AUG_BOOL:\n",
    "#                         X_batch_aug[ind] = sess.run(prepared_image_op, feed_dict={input_image: img})\n",
    "                        X_batch_augx[ind] = augment_image(np.copy(img), lrf_p=LRF_P, \n",
    "                                                          upf_p=UPF_p, r_degree=ROT_DEG, \n",
    "                                                          roll_num=ROLL_NUM)\n",
    "                    else:\n",
    "                        X_batch_augx[ind] = np.copy(img)\n",
    "                    ind += 1\n",
    "\n",
    "                feed_train = {X: X_batch_augx, \n",
    "                              y_raw: y_batch, \n",
    "                              training: True,\n",
    "                              learning_rate: LEARNING_RATE}\n",
    "\n",
    "                sess.run(training_op, feed_dict=feed_train)\n",
    "                #step += 1\n",
    "            # xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "\n",
    "            # ========================================================= Epoch metrics (log and print)\n",
    "\n",
    "            # ----- training\n",
    "            epoch_acc_print, epoch_auc_print, epoch_loss_print = sess.run([train_tf_acc, train_auc, train_mean_loss])\n",
    "\n",
    "            epoch_train_summary = sess.run(epoch_train_write_op)\n",
    "            epoch_train_writer.add_summary(epoch_train_summary, step)\n",
    "            epoch_train_writer.flush()\n",
    "\n",
    "            epoch_val_acc_print, epoch_val_auc_print, epoch_val_loss_print = sess.run([val_tf_acc, val_auc, val_mean_loss])\n",
    "\n",
    "            epoch_val_summary = sess.run(epoch_validation_write_op)\n",
    "            epoch_val_writer.add_summary(epoch_val_summary, step)\n",
    "            epoch_val_writer.flush()\n",
    "\n",
    "            print(\"E:{}tr: auc {:.2f}%|acc {:.2f}%|{:.4f} || val: auc {:.2f}%|acc {:.2f}%|{:.4f}\".format(epoch,\n",
    "                                                                                             epoch_auc_print*100,              \n",
    "                                                                                             epoch_acc_print*100, \n",
    "                                                                                             epoch_loss_print,\n",
    "                                                                                             epoch_val_auc_print*100,\n",
    "                                                                                             epoch_val_acc_print*100, \n",
    "                                                                                             epoch_val_loss_print))\n",
    "            #plot_color_image(X_batch_aug[0])\n",
    "            #plt.imshow(X_batch_aug[0].astype(np.uint8),interpolation=\"nearest\")\n",
    "            #plt.title(\"{}x{}\".format(image.shape[1], image.shape[0]))\n",
    "#             show_output(sess, 9, X_batch_aug, 3, \"RGB\")\n",
    "#             plt.axis(\"off\")\n",
    "            # ==========================================================================================\n",
    "\n",
    "            # save 'best' model (lowest epoch avg for validation loss)\n",
    "            if epoch_val_loss_print < best_loss_val:\n",
    "                best_loss_val = epoch_val_loss_print\n",
    "                checks_since_last_progress = 0\n",
    "                best_model_params = get_model_params()\n",
    "                print(\"Best params saved| val loss: {:.4f}\".format(best_loss_val))\n",
    "                \n",
    "            else:\n",
    "                checks_since_last_progress += 1\n",
    "            \n",
    "            if checks_since_last_progress >= 150:\n",
    "                print(\"Early Stopping\")\n",
    "                break\n",
    "\n",
    "            # estimate time remaining\n",
    "            EPOCH_END_TIME = time.time()\n",
    "            EPOCH_ELAPSED_TIME = EPOCH_END_TIME - EPOCH_START_TIME\n",
    "            EST_TIME_REMAINING = (((EPOCH_ELAPSED_TIME) * (NUM_EPOCHS - epoch)) / 60)\n",
    "            print(\"E time {:.1f} secs, Est.Time Remain: {:.1f} mins\".format(EPOCH_ELAPSED_TIME, EST_TIME_REMAINING))\n",
    "            print(\"-----------------------------\")\n",
    "\n",
    "            sess.run(val_acc_reset_op)\n",
    "            sess.run(val_loss_reset_op)\n",
    "            sess.run(train_acc_reset_op)\n",
    "            sess.run(train_loss_reset_op)\n",
    "\n",
    "        # done with training writers\n",
    "        #batch_train_writer.close()\n",
    "        #batch_val_writer.close()\n",
    "        epoch_train_writer.close()\n",
    "        epoch_val_writer.close()\n",
    "\n",
    "        # save session so we can re-enter at a later time\n",
    "        if FINAL:\n",
    "            saver_path = \"./saver/pinn_\" + str(SQUARE_DIM) + \"_\" + \"FINAL\"\n",
    "        else:\n",
    "            saver_path = \"./saver/pinn_\" + str(SQUARE_DIM) + \"_\" + str(P_LEVEL)\n",
    "        save_path = saver.save(sess, saver_path)\n",
    "        return g, best_model_params, saver\n",
    "\n",
    "print(\"ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Layer by Layer\n",
    "### Layer 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MASTER_list = collections.OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# zero out params\n",
    "g = None\n",
    "bmp = None\n",
    "saver = None\n",
    "sess = None\n",
    "\n",
    "NUM_epochs = 1\n",
    "#custom_model_def = collections.Ord\n",
    "new_layer = {}\n",
    "component_list = []\n",
    "component_list.append({\"type\": \"nested\", \"params\":{\"shape\":\"square\", \"size\":3, \"stride\":1, \"depth\":2}})\n",
    "#component_list.append({\"type\": \"avgpool\"})\n",
    "new_layer[\"N\"] = component_list\n",
    "\n",
    "# run graph\n",
    "START_TIME = time.time()\n",
    "\n",
    "# pass previous graph (g) and create new graph\n",
    "P_LEVEL = 0\n",
    "print(\"P_LEVEL: \", P_LEVEL)\n",
    "\n",
    "# if bmp:\n",
    "# #         restore_model_params(bmp, g, sess)\n",
    "#     saver_path = \"./saver/pinn_\" + str(SQUARE_DIM) + \"_\" + str(P_LEVEL)\n",
    "#     sess = tf.Session(graph=g)\n",
    "#     saver.restore(sess, saver_path)\n",
    "#     restore_model_params(bmp, g, sess)\n",
    "\n",
    "g, bmp, saver = run_that_model(P_LEVEL, g, NUM_EPOCHS=NUM_epochs, FINAL=False, LEARNING_RATE=1e-7, MODEL_DEF_LIST=new_layer, REG_FLAG=False)\n",
    "\n",
    "END_TIME = time.time()\n",
    "complete = True\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_epochs = 40\n",
    "#custom_model_def = collections.Ord\n",
    "new_layer = {}\n",
    "component_list = []\n",
    "component_list.append({\"type\": \"conv\", \"params\":{\"shape\":\"square\", \"size\":1, \"stride\":1}})\n",
    "component_list.append({\"type\": \"conv\", \"params\":{\"shape\":\"square\", \"size\":3, \"stride\":1}})\n",
    "component_list.append({\"type\": \"conv\", \"params\":{\"shape\":\"split\", \"size\":5, \"stride\":1}})\n",
    "component_list.append({\"type\": \"conv\", \"params\":{\"shape\":\"split\", \"size\":7, \"stride\":1}})\n",
    "component_list.append({\"type\": \"avgpool\"})\n",
    "new_layer[\"R\"] = component_list\n",
    "\n",
    "\n",
    "custom_model_def = MASTER_list\n",
    "custom_model_def.update(new_layer)\n",
    "# run graph\n",
    "START_TIME = time.time()\n",
    "\n",
    "# pass previous graph (g) and create new graph\n",
    "P_LEVEL = 0\n",
    "print(\"P_LEVEL: \", P_LEVEL)\n",
    "\n",
    "if bmp:\n",
    "#         restore_model_params(bmp, g, sess)\n",
    "    saver_path = \"./saver/pinn_\" + str(SQUARE_DIM) + \"_\" + str(P_LEVEL - 1)\n",
    "    sess = tf.Session(graph=g)\n",
    "    saver.restore(sess, saver_path)\n",
    "    restore_model_params(bmp, g, sess)\n",
    "\n",
    "g, bmp, saver = run_that_model(P_LEVEL, g, NUM_EPOCHS=NUM_epochs, FINAL=False, LEARNING_RATE=1e-7, MODEL_DEF_LIST=custom_model_def, REG_FLAG=False)\n",
    "\n",
    "END_TIME = time.time()\n",
    "complete = True\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if complete:\n",
    "    MASTER_list.update(custom_model_def)\n",
    "    complete = False\n",
    "    P_LEVEL += 1\n",
    "else:\n",
    "    print(\"I think it was already added\")\n",
    "print(\"current list:\", MASTER_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_epochs = 40\n",
    "new_layer = {}\n",
    "\n",
    "component_list = []\n",
    "component_list.append({\"type\": \"conv\", \"params\":{\"shape\":\"square\", \"size\":1, \"stride\":1}})\n",
    "new_layer[\"C\"] = component_list\n",
    "\n",
    "custom_model_def = MASTER_list\n",
    "custom_model_def.update(new_layer)\n",
    "# run graph\n",
    "START_TIME = time.time()\n",
    "\n",
    "print(\"P_LEVEL: \", P_LEVEL)\n",
    "\n",
    "if bmp:\n",
    "    saver_path = \"./saver/pinn_\" + str(SQUARE_DIM) + \"_\" + str(P_LEVEL - 1)\n",
    "    sess = tf.Session(graph=g)\n",
    "    saver.restore(sess, saver_path)\n",
    "    restore_model_params(bmp, g, sess)\n",
    "\n",
    "g, bmp, saver = run_that_model(P_LEVEL, g, NUM_EPOCHS=NUM_epochs, FINAL=False, LEARNING_RATE=1e-7, MODEL_DEF_LIST=custom_model_def, REG_FLAG=False)\n",
    "\n",
    "END_TIME = time.time()\n",
    "complete = True\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if complete:\n",
    "    MASTER_list.update(custom_model_def)\n",
    "    complete = False\n",
    "    P_LEVEL += 1\n",
    "else:\n",
    "    print(\"I think it was already added\")\n",
    "print(\"current list:\", MASTER_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_epochs = 1\n",
    "\n",
    "new_layer = {}\n",
    "component_list = []\n",
    "component_list.append({\"type\": \"conv\", \"params\":{\"shape\":\"square\", \"size\":3, \"stride\":2}})\n",
    "component_list.append({\"type\": \"maxpool\"})\n",
    "new_layer[\"R\"] = component_list\n",
    "\n",
    "custom_model_def = MASTER_list\n",
    "custom_model_def.update(new_layer)\n",
    "# run graph\n",
    "START_TIME = time.time()\n",
    "\n",
    "print(\"P_LEVEL: \", P_LEVEL)\n",
    "\n",
    "if bmp:\n",
    "    saver_path = \"./saver/pinn_\" + str(SQUARE_DIM) + \"_\" + str(P_LEVEL - 1)\n",
    "    sess = tf.Session(graph=g)\n",
    "    saver.restore(sess, saver_path)\n",
    "    restore_model_params(bmp, g, sess)\n",
    "\n",
    "g, bmp, saver = run_that_model(P_LEVEL, g, NUM_EPOCHS=NUM_epochs, FINAL=False, LEARNING_RATE=1e-7, MODEL_DEF_LIST=custom_model_def, REG_FLAG=False)\n",
    "\n",
    "END_TIME = time.time()\n",
    "complete = True\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if complete:\n",
    "    MASTER_list.update(custom_model_def)\n",
    "    complete = False\n",
    "    P_LEVEL += 1\n",
    "else:\n",
    "    print(\"I think it was already added\")\n",
    "print(\"current list:\", MASTER_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add fc classifier\n",
    "# LAST_LEVEL = NUM_LAYERS\n",
    "# print(LAST_LEVEL)\n",
    "# if bmp:\n",
    "#     # restore params from final model\n",
    "#     saver_path = \"./saver/pinn_\" + str(SQUARE_DIM) + \"_\" + str(LAST_LEVEL)\n",
    "#     sess = tf.Session(graph=g)\n",
    "#     saver.restore(sess, saver_path)\n",
    "#     restore_model_params(bmp, g, sess)\n",
    "\n",
    "# g_f, bmp_f, saver_f = run_that_model(LAST_LEVEL, g, NUM_EPOCHS, True, LEARNING_RATE=0.0001)\n",
    "# END_TIME = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver_path = \"./saver/pinn_\" + str(SQUARE_DIM) + \"_\" + str(P_LEVEL - 1)\n",
    "sess = tf.Session(graph=g)\n",
    "saver.restore(sess, saver_path)\n",
    "\n",
    "training_op, X, y_raw, training, learning_rate = g.get_collection(\"main_ops\")\n",
    "train_tf_acc, train_tf_acc_op, train_acc_reset_op, train_auc, train_auc_update, val_tf_acc, val_tf_acc_op, val_acc_reset_op, val_auc, val_auc_update, test_tf_acc, test_tf_acc_op, test_acc_reset_op, test_auc, test_auc_update = g.get_collection(\"acc_metrics\")\n",
    "train_y_true_cls, train_y_pred_cls = g.get_collection(\"test_metrics\")\n",
    "train_mean_loss, train_mean_loss_update, train_loss_reset_op, val_mean_loss, val_mean_loss_update, val_loss_reset_op, test_mean_loss, test_mean_loss_update, test_loss_reset_op = g.get_collection(\"loss_metrics\")\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "# todo: there is a better way to implement the confusion matrix\n",
    "# see here; https://stackoverflow.com/questions/41617463/tensorflow-confusion-matrix-in-tensorboard#42857070\n",
    "\n",
    "with sess:\n",
    "    confusion_mat = tf.Variable( tf.zeros([num_classes,num_classes], dtype=tf.int32 ), name='confusion')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # restore to the best parameters to calculate test metrics\n",
    "    if bmp:\n",
    "        restore_model_params(bmp, g, sess)\n",
    "\n",
    "    sess.run(test_acc_reset_op)\n",
    "    sess.run(test_loss_reset_op)\n",
    "    \n",
    "    for iteration in range(len(X_test) // batch_size):\n",
    "        low_i = iteration*batch_size\n",
    "        high_i = (iteration+1)*batch_size\n",
    "        X_test_batch = X_test[low_i:high_i]\n",
    "        y_test_batch = y_test[low_i:high_i]\n",
    "        sess.run([test_tf_acc_op, test_mean_loss_update, test_auc_update], \n",
    "                 feed_dict={X: X_test_batch, y_raw: y_test_batch, training: False})\n",
    "        jj = train_y_true_cls.eval(feed_dict={y_raw: y_test_batch})\n",
    "        batch_conf_matrix = tf.confusion_matrix(labels = train_y_true_cls.eval(feed_dict={y_raw: y_test_batch}).reshape(-1),\n",
    "                                                predictions = train_y_pred_cls.eval(feed_dict={X: X_test_batch}).reshape(-1),\n",
    "                                                num_classes=num_classes)\n",
    "        sess.run(confusion_mat.assign(confusion_mat.eval() + batch_conf_matrix.eval()))\n",
    "    \n",
    "    X_test_batch = X_test[high_i:]\n",
    "    y_test_batch = y_test[high_i:]\n",
    "    if len(X_test_batch) > 0:\n",
    "        sess.run([test_tf_acc_op, test_mean_loss_update, test_auc_update], \n",
    "                 feed_dict={X: X_test_batch, y_raw: y_test_batch, training: False})\n",
    "        batch_conf_matrix = tf.confusion_matrix(labels = train_y_true_cls.eval(feed_dict={y_raw: y_test_batch}).reshape(-1),\n",
    "                                                predictions = train_y_pred_cls.eval(feed_dict={X: X_test_batch}).reshape(-1),\n",
    "                                                num_classes=num_classes)\n",
    "        sess.run(confusion_mat.assign(confusion_mat.eval() + batch_conf_matrix.eval()))\n",
    "    final_test_acc, final_test_loss, final_test_auc = sess.run([test_tf_acc, test_mean_loss, test_auc])\n",
    "\n",
    "    print(\">>>>>>>>>> test auc: {:.3f}% acc: {:.2f} loss: {:.5f}\".format(final_test_auc*100, \n",
    "                                                                         final_test_acc*100,\n",
    "                                                                         final_test_loss))\n",
    "\n",
    "    print(confusion_mat.eval())\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu_tf",
   "language": "python",
   "name": "cpu_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
