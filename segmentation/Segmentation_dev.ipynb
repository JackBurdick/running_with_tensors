{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segementation FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DL framework\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# common packages\n",
    "import numpy as np\n",
    "import os # handling file i/o\n",
    "import sys\n",
    "import math\n",
    "import time # timing epochs\n",
    "\n",
    "# for ordered dict when building layer components\n",
    "import collections\n",
    "\n",
    "# plotting pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable # colorbar helper\n",
    "\n",
    "# read image\n",
    "from scipy.misc import imread\n",
    "# + data augmentation\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "\n",
    "# used for manually saving best params\n",
    "import pickle\n",
    "\n",
    "# for shuffling data batches\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# const\n",
    "SEED = 42\n",
    "\n",
    "# Helper to make the output consistent\n",
    "def reset_graph(seed=SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# helper to create dirs if they don't already exist\n",
    "def maybe_create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print(\"{} createed\".format(dir_path))\n",
    "    else:\n",
    "        print(\"{} already exists\".format(dir_path))\n",
    "    \n",
    "# set log level to supress messages, unless an error\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Important Version information\n",
    "print(\"Python: {}\".format(sys.version_info[:]))\n",
    "print('TensorFlow: {}'.format(tf.__version__))\n",
    "\n",
    "# Check if using GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU found')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "    \n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `saver/` will hold tf saver files\n",
    "maybe_create_dir(\"saver\")\n",
    "# `best_params/` will hold a serialized version of the best params\n",
    "# I like to keep this as a backup in case I run into issues with\n",
    "# the saver files\n",
    "maybe_create_dir(\"best_params\")\n",
    "# `tf_logs/` will hold the logs that will be visable in tensorboard\n",
    "maybe_create_dir(\"tf_logs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these two functions (get_model_params and restore_model_params) are \n",
    "# ad[a|o]pted from; \n",
    "# https://github.com/ageron/handson-ml/blob/master/11_deep_learning.ipynb\n",
    "def get_model_params():\n",
    "    global_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    return {global_vars.op.name: value for global_vars, value in \n",
    "            zip(global_vars, tf.get_default_session().run(global_vars))}\n",
    "\n",
    "def restore_model_params(model_params, g, sess):\n",
    "    gvar_names = list(model_params.keys())\n",
    "    assign_ops = {gvar_name: g.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                  for gvar_name in gvar_names}\n",
    "    init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "    feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "    sess.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "# these two functions are used to manually save the best\n",
    "# model params to disk\n",
    "def save_obj(obj, name):\n",
    "    with open('best_params/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('best_params/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to add an image to the plot and choose whether\n",
    "# to include the color bar\n",
    "def implot(mp, ax, SHOW_CB=False):\n",
    "    im = ax.imshow(mp)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    if SHOW_CB:\n",
    "        cbar = plt.colorbar(im, cax=cax, format='%1.2f', boundaries=np.linspace(0,1,20))\n",
    "    else:\n",
    "        cax.set_axis_off()\n",
    "\n",
    "    ax.set_axis_off()\n",
    "\n",
    "def show_masked_result(image, mask):\n",
    "    # create combined image of (image & mask)\n",
    "    combined = np.copy(img)\n",
    "    combined[mask == 0] = [0, 0, 0]\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(12,4))\n",
    "\n",
    "    implot(image, ax1)\n",
    "    implot(mask, ax2, True)\n",
    "    implot(combined, ax3)\n",
    "\n",
    "    plt.grid('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image dimensions (GLOBAL) - [MG_WIDTH x IMG_HEIGHT, CHANNELS]\n",
    "SQUARE_DIM = 224\n",
    "if SQUARE_DIM:\n",
    "    IMG_WIDTH = SQUARE_DIM\n",
    "    IMG_HEIGHT = SQUARE_DIM\n",
    "CHANNELS = 3\n",
    "    \n",
    "ROOT_DATA = \"../ROOT_DATA/lesions/segmentation/\"\n",
    "    \n",
    "# load entire dataset into memory\n",
    "# NOTE: depending on your machine, this may not be practical, you may have to\n",
    "# create a smaller dataset (although this should have already been noticed when\n",
    "# making this dataset)\n",
    "X_dev = np.load(ROOT_DATA + str(SQUARE_DIM) + \"_\" + str(SQUARE_DIM) +  '/images.npy')\n",
    "y_dev = np.load(ROOT_DATA + str(SQUARE_DIM) + \"_\" + str(SQUARE_DIM) + '/masks.npy')\n",
    "\n",
    "# test set supplied (doesn't need to come from training data)\n",
    "X_test = np.load(ROOT_DATA + str(SQUARE_DIM) + \"_\" + str(SQUARE_DIM) +  '/test_images.npy')\n",
    "y_test = np.load(ROOT_DATA + str(SQUARE_DIM) + \"_\" + str(SQUARE_DIM) + '/test_masks.npy')\n",
    "\n",
    "# shuffle set -- not necessary, but I like to do this in case there\n",
    "# is some order to the data we don't know about.  Not that important\n",
    "# for this particular problem (segmentation), but I still feel better\n",
    "# calling a shuffle before splitting the data\n",
    "X_dev, y_dev = shuffle(X_dev, y_dev, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "[preprocess_mask]: ./misc/segmentation_clipping_y.png\n",
    "[preprocess_image]: ./misc/segmentation_preprocessing_x.png\n",
    "\n",
    "### Masks\n",
    "Our mask is currently in non-binary form.  The resize that was used created intermediate values {shown below}.  To address this, we need to clip the values to binary form.\n",
    "\n",
    "![mask preprocessing][preprocess_mask]\n",
    "\n",
    "\n",
    "### Input image\n",
    "The image is currently in [0, 255], this is ok, but we're going to scale the input image to [0,1].  This will be done for each image.  This will be a simple/basic scaling, meaning we'll divide each image pixel (minus the image min) value by the (individual image's) max - min*.\n",
    "TODO: explain and reference this process\n",
    "\n",
    "![image preprocessing][preprocess_image]\n",
    "\n",
    "*NOTE: this methodology may not be best for this and/or your independent problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess mask (example shown above)\n",
    "# threshold (clip) to [0, 1] where 0:no mask, 1:mask\n",
    "# convert to datatype int\n",
    "y_dev = [np.clip(img, 0, 1).astype(np.int32) for img in y_dev]\n",
    "y_dev = np.asarray(y_dev)\n",
    "\n",
    "y_test = [np.clip(img, 0, 1).astype(np.int32) for img in y_test]\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess image (example shown above)\n",
    "X_dev = [(image-np.amin(image))/(np.amax(image)-np.amin(image)) for image in X_dev]\n",
    "X_dev = np.asarray(X_dev)\n",
    "\n",
    "X_test = [(image-np.amin(image))/(np.amax(image)-np.amin(image)) for image in X_test]\n",
    "X_test = np.asarray(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view dataset information\n",
    "print(\"Training:  X:{} , y:{}\".format(X_dev.shape, y_dev.shape))\n",
    "print(\"Test:      X:{} , y:{}\".format(X_test.shape, y_test.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
